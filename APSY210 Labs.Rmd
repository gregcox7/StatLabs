--- 
title: "Labs"
author: "Greg Cox"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
# biblio-style: apalike
csl: apa.csl
link-citations: yes
description: Laboratory exercises for APSY210.
---
```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```

# Overview {-}

This is a collection of laboratory exercises as a part of APSY210.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Exploring Data with R {#lab1}

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')

titanic <- read.csv('data/titanic.csv')
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/titanic.png")
```

In this session, we will learn a bit about data and how to look at it in R.  This will largely involve copying and pasting code from this document into your own RStudio window and interpreting the results, though you will also have to make some tweaks to that code on your own.  The big point is to get a feel for the power of the tools we will be learning to use in the rest of the semester.

For most of what we will be doing, this document shows both the R code and, below it, the result that code is expected to produce.  The questions you will need to answer for the accompanying assignment are presented as numbered footnotes.

Another big point is that even though statistics is about dealing with data, those data are *meaningful*.  They are not just numbers or names, they are a peek into the world.  They offer glimpses of someone's life, of the workings of some natural process, of some social structure, etc.  Any dataset will be limited in how wide of a glimpse it gives us, and the point of statistics is how to learn and make decisions based on that glimpse.

The primary skills developed in this activity are:

1. Viewing data in R
2. Making frequency tables
3. Making bar charts
4. Making histograms

## R and RStudio

All of our labs will make use of RStudio, a graphical interface to the statistical computing language R.  The R language represents the current state of the art in both academic and industrial research.  It is likely to remain relevant for many years to come because it is free and open-source, meaning both that it is widely accessible and that improvements and extensions are relatively easy to make.  In fact, many of the best features of R that we will be using are extensions made by people outside the "core" development team for R.  These extensions are called "packages", and they represent bundles of code that are useful for doing statistics.

RStudio makes it easier to work with the R language, and it is also free and can be installed on computers running any modern operating system (Windows, Mac, Linux, etc.).  RStudio is already installed on the computers in the Technology-Enhanced Classrooms and the Library Public Computing Sites on campus.  If you are working on your own computer, you will have an easier time of it if you install RStudio on it.  Installing RStudio requires installing R, but you only need to do this once.  Follow the installation instructions for **RStudio Desktop** here: <https://rstudio.com/products/rstudio/download/>

You can also use RStudio in a browser!  This way, even if you don't have access to a computer with RStudio installed locally, you can use it if you have access to the internet.  You can run RStudio online here: <https://rstudio.cloud/>.  The downside with this is that there is a cap to the amount of time you can spend using the online version, so you are better off using a local installation whenever possible.

This first lab will use the cloud version so we can jump right in.  Access the first Lab here:

<https://rstudio.cloud/project/2118415>

Note that you may need to create an account, but it is free to do so.

<!-- ## Tidyverse -->

<!-- Once you have RStudio up and running, there's one more thing you need to do to make sure you can do the labs in the course.  Open up RStudio, paste the line below into your console, and hit enter: -->

<!-- ```{r eval = FALSE} -->
<!-- install.packages("tidyverse") -->
<!-- ``` -->

<!-- This will install a package called "tidyverse" that we will be using in class that will make R much more useful for us.  You only need to do this once on any particular computer, and it may already be installed.  If so, running this won't hurt, it'll just make sure you've got everything we need. -->

## Meet your data

The data we will be looking at are passenger records from the RMS *Titanic*, an oceanliner which famously sank on April 15, 1912.  Though the liner was not filled to capacity, lax safety precautions---including a failure to carry enough lifeboats---meant that many of her passengers died because they were unable to evacuate when the ship struck an iceberg.

<!-- ### Load the data -->

<!-- Open up a browser and head to the course Blackboard page.  Under the "Datasets" folder, you will find a file called "titanic.csv".  Download the file and make sure you take note of where you saved it. -->

<!-- Make sure RStudio is running.  You can load the data into RStudio by going to **File** > **Import Dataset** > **From text (readr)...** and finding the "titanic.csv" file wherever you put it.  If you are doing this using the online version of RStudio, you will first have to **Upload** the file to your workspace using the "Files" panel on the right. -->

### Check out the variables

This is what RStudio looks like (with some helpful colored labels):

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/rstudio_window.png")
```

In the upper left of the RStudio screen, you'll see a bunch of columns.  These are our data in "raw" form.  Each row represents a specific passenger and each column represents a different *variable*.  Based on the names of each variable and the types of *values* it seems to take, can you figure out what each variable is?  In other words, what does each column tell us about a person?^[For each variable in the dataset, say whether it is a) Discrete or Continuous; b) what scale it is measured on (nominal, ordinal, interval, or ratio); and c) what you think it means based on the name of the variable and/or what values it takes.]

## Answering questions with data

Now that we've gotten acquainted with the kind of data we have, we can begin using it to answer some questions.  This will involve simplifying the data, turning it into a summary form that makes it easier to understand.  These summaries fall under the heading of "descriptive statistics", because they are meant to *describe* important aspects of the data.  The three types of summaries we will explore today are **frequency tables**, **bar charts**, and **histograms**.

### Frequency tables

The first question is, who was actually aboard the *Titanic*?  One way we could answer this question is to read the names of all 1300 or so people in our dataset, but this would not be particularly efficient or informative.  What we are going to do instead is *simplify*, and focus on specific aspects of each person.

Let's first ask **how many passengers were male or female**.  One way to answer this question is by constructing a **frequency table**.  In the RStudio window, see the big open space just below our data?  This is called the "console" and is where we will do most of our work.  Copy and paste the code below into the "Console".  The code should appear right after the ">".  Once it is there, hit **enter** to run it and see the results.

```{r}
titanic %>%
  group_by(sex) %>%
  summarize(n = n())
```

We got a table that counted the frequency of males and females on the passenger list.  We went from 1300 or so rows with multiple variables each to just two numbers.  A pretty concise summary!  But how did we do it?  Let's break down that bit of code:

* `titanic` is our original dataset.
* `group_by(sex)` tells R to group that dataset by sex.
* `summarize(n=n())` tells R to take our grouped dataset and *summarize* it by counting the `n`umber of people in each group and labeling the resulting number "`n`".
* The funky symbol `%>%` connects the three steps above and makes sure R does them in the order we want.

Let's try a few things to get a sense of why that code did what it did.  What happens if we change `n = n()` in the last line to `Number = n()`?

```{r error=TRUE}
titanic %>%
  group_by(sex) %>%
  summarize(Number = n())
```

Everything looks the same except that instead of the column being labeled "n", it is labeled "Number".  So the bit before the equals sign is how the frequency table will be labeled.

Now let's try something that seems like a small change:  Instead of `n = n()` in the last line, let's write `n = m()`.  Only one letter different, surely it can't be that big of a difference?

```{r error=TRUE}
titanic %>%
  group_by(sex) %>%
  summarize(n = m())
```

R doesn't like it!  It reports an error because it doesn't know what to do with `m()`.  That's because `n()` is a **function**, it is an instruction that tells R to count the `n`umber of something.  On the other hand, `m()` doesn't mean anything to R so it throws up its hands.

We saw that we know not just the sex of each passenger, the "residence" variable tells us whether each person is American, British, or something else.  Let's modify our code to get a frequency table for country of residence instead of sex:

```{r}
titanic %>%
  group_by(residence) %>%
  summarize(n = n())
```

Easy!  So all we need to do to get a frequency table for a particular variable is to put the name of that variable in the parentheses in the `group_by` line.  Since we're on a roll, let's see if we can count the number of passengers with or without college degrees:

```{r error=TRUE}
titanic %>%
  group_by(degree) %>%
  summarize(n = n())
```

No dice!  R tells us that it can't find a column labeled "degree", and indeed, there is no such variable in our data since it was not recorded.  This illustrates that the variable in the parens in the `group_by` line can't be just anything, it has to be the name of a variable (spelled exactly the same!) that exists in our data.

Finally, let's construct a frequency table using multiple variables at once.  This lets us answer questions like, **how many British women were aboard the _Titanic_**?  We can put multiple variables in the `group_by` line:

```{r error=TRUE}
titanic %>%
  group_by(residence, sex) %>%
  summarize(n = n())
```

Now we can begin to address a few more questions.  This time, you will have to figure out how to fiddle with our code for making frequency tables.  Write code that will produce the frequency table below:^[What code did you write to make a frequency table for number of people of each class who did or did not survive?]

```{r echo=FALSE}
titanic %>%
  group_by(class, survived) %>%
  summarize(n = n())
```

This table breaks down the number of people who did or did not survive the sinking of the *Titanic* by their "class", with first-class being the most expensive with the most amenities and third-class being the least expensive with the least amenities.  Do you notice any patterns?^[Were there more people in third class than first class?  For each of the three classes, is the number of survivors more than the number who died?]

### Bar charts

Trying to find patterns among six numbers in a frequency table is not impossible, but it's also not easy.  Bar charts make numerical relationships easy to see visually, so we don't need to compare a bunch of numbers.

Let's see how even a simple comparison is easier with a bar chart than a frequency table.  First, let's make a frequency table for the number of passengers in each class:^[What R code would produce this frequency table?]

```{r echo=FALSE}
titanic %>%
  group_by(class) %>%
  summarize(n = n())
```

Now, use the following code to instead construct a **bar chart** that displays the same information as the table, but in a visual form:

```{r}
titanic %>%
  ggplot(aes(x = class)) +
  geom_bar()
```

Pretty neat!  It is now easy to see how much more 3rd class passengers there are than 1st or 2nd, and that interestingly, there are fewer 2nd class than 1st class passengers^[What about the bar chart conveys the same information as the frequency table?  In other words, what is the visual aspect of the bar chart that represents the numbers in the table?].  Notice that the code we used is similar to what we've been using, but differs in some important ways:

* The first line is the same, telling R what dataset we are using (`titanic`).
* The second line tells R that we want to make a `plot` and that we want to put the variable `class` along the horizontal axis of that plot (the `x` axis).  The "gg" in front of "plot" refers to the "**g**rammar of **g**raphics", which is language R uses to describe plots.  In this language, different parts of a plot are called "**aes**thetics", which is why `x = class` falls inside a parenthetical labeled `aes`(thetic).
* The final line just tells R that we want to make a `bar` chart.  In the grammar of graphics, different types of charts are called `geom`s.
* Notice that the second 2 lines are connected by a `+` rather than our `%>%` symbol.  This is a historical accident, but the meaning of the two symbols is basically the same: they are telling R the order in which it should follow our instructions.

### Histograms

So far, we have been summarizing *discrete* variables.  There are also some continuous variables in our data, for example the age of each passenger as well as how much they paid for their tickets.  Let's try making a frequency table to figure out how many people of different ages sailed on the *Titanic*:

```{r}
titanic %>%
  group_by(age) %>%
  summarize(n = n())
```

Well that's not very helpful!  R didn't even bother to show us the whole table.  Though we can see something interesting:  Age is measured in years, and for passengers at least one year old, their age is a whole number.  But there are fractions of years for passengers less than a year old---these ages were *measured* in months rather than years.

The main point is that even though age can be measured in a more or less fine-grained manner, it is effectively *continuous*.  We don't want to know how many passengers were exactly 31.3491 years old, we want to get a sense of whether there are more younger than older passengers, whether there might be different clusters or age groups, that sort of thing.  In other words, we want to know something about how people are *distributed* across different age ranges.

We can construct a summary that conveys this information using a **histogram**.  This is very similar to a bar chart; the difference is that bar charts are for discrete variables while histograms are for continuous variables.  The code below constructs a histogram to summarize passenger age:

```{r}
titanic %>%
  ggplot(aes(x = age)) +
  geom_histogram()
```

The resulting histogram shows a bunch of bars, the height of which indicate the number of passengers within a particular age range.^[Compare the code we used to make a histogram to what we used to make a bar chart above.  What is different between them?]

Notice that we got a couple messages from R in addition to our plot, one about "non-finite values" and another about "picking a better value".  When R says, "non-finite values", it is talking about people for whom their age was not recorded.  This is an unfortunate thing about real data: sometimes it has missing pieces.  This didn't stop R from making the plot we wanted using the non-missing data, but R wanted to warn us just in case.

The message about "picking a better value" is important:  When you make a histogram, you are looking at how many things fall within a particular range of values, say, between ages 4 and 8.  How do you decide those ranges?  If you don't tell R how to do that, it will decide on its own to divide up the range of values into 30 "bins", each of which corresponds to a range of values that is the same width.  This is usually not what we want.

Instead, we should decide how big or small we want those ranges to be.  The following code tells R to make our histogram using "bins" that are 5 years "wide" (0-4, 5-9, 10-14, etc.):

```{r}
titanic %>%
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 5)
```

Here's what it looks like when we set the width of each bin to be just 1 year^[What code would produce this plot?]:

```{r, echo = FALSE}
titanic %>%
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 1)
```

And here's what it looks like when we set the width of each bin to be 10 years^[What code would produce this plot?]:

```{r, echo = FALSE}
titanic %>%
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

Choosing different bin widths makes the resulting histogram look very different.  There is not necessarily one "right" answer, so being able to quickly see the differences between different histograms makes R very handy.^[What bin width do you think makes the best histogram for passenger ages?  Why?  Do you think different bin widths might be better in different circumstances?]

## Wrap-up

Today we began our adventure by using RStudio to explore some data.  We saw how to look at data and how to summarize it in various helpful ways.  These were frequency tables, bar charts, and histograms.

* Frequency tables count the number of times a particular value of a particular variable (or combination of values across multiple variables) occurs in our dataset.
* Bar charts display those counts in a visual form that makes it easier to compare frequencies.
* Histograms let us visually summarize counts of continuous variables by putting them in "bins", the width of which we need to decide.

<!--chapter:end:01-exploring.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Central tendency and variability {#lab2}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/rats.jpg")
```

In this session, we will learn how to use R to calculate descriptive statistics.  Specifically, these are statistics describing *central tendency* and *variability*.  We saw how complicated it can be to calculate some of these things by hand.  While it is good to understand what is going on "behind the scenes", doing such calculations by hand is not only time consuming but prone to error.  If you make a typo at one step, that little error gets magnified as you keep going.

Besides, as we saw last week, most of the time, dataset are large and it is simply unreasonable to calculate a mean of 1300 numbers by hand.  Luckily, computers are great at doing the kinds of mechanical, repetitive, mindless steps involved.

But if the computer is great at mind*less* stuff, that means us people have to be better at mind*ful* stuff.  We need to think about what these numbers mean and why they may be important for understanding the world.

Another part of computers being mindless is that they will always do exactly what you tell them to do, unless they can't.  So we have to be extremely careful when we write code because, just like making a mistake doing calculations by hand, a mistake in code can propagate into a mistake in the result.  Think about your neighbor's obnoxious dog---it is not the dog's fault it is obnoxious, it is the fault of the neighbor for doing a poor job training her.

Why is doing statistics with a computer any better, if we still have to worry about making a mistake?  Because the computer is fast and can deal with a lot more data than we can.

The primary skills developed in this activity are:

1. Using R to calculate central tendency
    1. Median
    2. Mean
2. Using R to calculate variability
    1. Variance
    2. Standard deviation

## Simple cases

### Central tendency

Imagine that we are thinking about selling a new product.  We demonstrate the product to five people in a focus group, each of whom gives us a rating of the product on a scale from 1 (very dissatisfied) to 9 (very satisfied)^[Is this measure continuous or discrete?  What is the scale of measurement?].  Their ratings, in no particular order, are 6, 9, 5, 5, and 7.

Recall that the mean is defined by the formula $\bar{X} = \frac{\sum_{i = 1}^N X_i}{N}$, where each $X_i$ is a measurement and $N$ is the number of measurements.  We can expand the summation $\sum_{i = 1}^N X_i$ into $X_1 + X_2 + X_3 + \cdots + X_N$, where the subscripts (1, 2, 3, etc.) indicate which of the measurements is being added to the total sum.

We can use R to do this calculation by basically writing out in R how we would write it out if we were finding the mean by hand.  Then we let R do the actual arithmetic.

```{r}
(6 + 9 + 5 + 5 + 7) / 5
```

We have basically turned R into a calculator.  R can do all the typical things a calculator can do, like addition (`+`) and division (`/`), as well as subtraction (`-`) and multiplication (`*`).  We used the parentheses to tell R to add up all the numbers *before* dividing them by 5.

But that's still pretty clumsy.  R gives us a more elegant way to find the mean of a set of measurements:

```{r}
mean(c(6, 9, 5, 5, 7))
```

Nice!  How did that work?  If you tell R `mean()`, it will give you the **mean** of whatever is inside those parentheses.  But what is that `c()` thing?  The `c()` tells R to **collect** the numbers together.  It tells R to treat them collectively; in our mathematical notation, the `c()` tells R that all the things inside those *inner* parentheses are to be treated like $X_i$'s, a set of measurements all on the same variable.

Can we keep using this trick?  Let's use R to find the median:

```{r}
median(c(6, 9, 5, 5, 7))
```

Worked like a charm!  Again, we had to tell R that our numbers were to be treated like a *collection*, and we told R to find the **median** of that collection of numbers.

Now imagine we get a sixth person to rate our product and they give it an 8.  Now our data consist of a set of six ratings (6, 9, 5, 5, 7, 8).  We can again use R to find the new mean^[What is the code that calculates the new mean?  *Hint: Try modifying the collection of observed values within the parentheses in the code above.*]

```{r, echo=FALSE}
mean(c(6, 9, 5, 5, 7, 8))
```

and the median^[What is the code that calculates the new median?  *Hint: Try modifying the collection of observed values within the parentheses in the code above.*]

```{r, echo=FALSE}
mean(c(6, 9, 5, 5, 7, 8))
```

Sadly, R does not give us a straightforward way to get the *mode*, so that's something we still have to figure out on our own.^[Just by looking at the observed values we have, what is the mode of the focus group's ratings?]

### Variability

R helpfully provides similar methods for calculating variability.  For example, we can calculate the *variance* of the focus group ratings like this

```{r}
var(c(6, 9, 5, 5, 7, 8))
```

Notice that R abbreviates *var*iance to just `var`, but otherwise the code looks basically the same as we used for the mean and median.  When you ask R for the variance, you get the so-called "sample" variance, given by $\frac{\sum_{i = 1}^N \left( X_i - \bar{X} \right)^2}{N - 1}$.

Similarly, we can get the *standard deviation* by swapping out `var` for `sd` (short for *s*tandard *d*eviation)

```{r}
sd(c(6, 9, 5, 5, 7, 8))
```

We can verify that this is the square root of the variance.  R has another "function" called `sqrt` that gives you the square root of whatever is in the parentheses that follow it.  Check it out:

```{r}
sqrt(var(c(6, 9, 5, 5, 7, 8)))
```

Notice that you can "wrap" things in parentheses and put function names in front, like we put the `sqrt` in front of the `var`.

## Dealing with real data

The examples above illustrated one way we can use R to calculate descriptions of central tendency and variability.  These are useful when we have a small amount of data that we can type in without risk of error, but that's almost never the situation we have to deal with in applying statistics to real data.

### Setting the stage

Before we can get to work with real data, we have to open up the right toolbox.  In R, this is done by retrieving a "package" from R's "library".  Remember that one of the nice things about R is that there are so many packages available that each contain helpful functions.  Of course, the downside is that we need to tell R to open up those packages, but this isn't too hard.

Specifically, we'll need to open the "tidyverse" package:

```{r}
library(tidyverse)
```

R will tell you a bunch of stuff, including something about "conflicts".  This isn't a big deal for us, R is just saying that we have some tools with the same name, and it calls this a "conflict".  R is just trying to keep you informed.

The tidyverse package is one that we will be using a lot.  It contains a lot of statistical tools and tools for making graphics that will be very useful for us throughout the course.

### Getting the data

The data we will be looking at come from a study by @YuEtAl1982.  They studied the lifespan of rats with two different diets:  One group of rats was allowed to eat freely, however they wanted; another group was fed on a restricted diet with only about 60% of the calories that the free-eating rats had.

We need to get this data into R.  The following code will import the data into your current R session:

```{r}
rats <- read_csv('https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv')
```

In your "Environment" panel in the upper right, you'll see a new entry called "rats".  This is the data we just imported.  Click on it to have a look at the data, which will appear in the upper left.  There are just two variables in this dataset, **Diet** and **Lifespan** (measured in days)^[For each of these variables, are they a) continuous or discrete? b) which scale are they measured on (nominal, ordinal, interval, or ratio)?].

### Visualizing the data

Numerical techniques for describing data, like means and variances, are useful compact descriptions of potentially complex distributions.  But while they can be quick and easy ways to communicate (even if they are not always quick and easy to *calculate*), it is still important to get a sense of what the data really look like.  As a result, we will never abandon our friends from last week, the bar chart and histogram.  It is essential to visualize your data prior to doing any kind of quantitative analyses, since visualizing the data helps us understand what those quantities *mean*.

Just like last week, we can construct a histogram to see how long different rats lived depending on their diet.

```{r}
rats %>%
    ggplot(aes(x=Lifespan)) +
    geom_histogram(binwidth=60)
```

So far, so good.  The code above takes our data, puts the "Lifespan" variable along the $x$ axis, and makes a histogram by putting the observed rat lifespans into "bins" that are 60 days wide.^[Compare the code we just used with the code we used to make a histogram of the ages of *Titanic* passengers last week.  What is different and what is similar?]

The histogram illustrates some interesting features of the data, notably that it appears to be *bimodal*^[Based on the histogram, roughly where do the two modes seem to be?].  Is this surprising?  Remember that our dataset actually consists of *two* groups of rats with different diets, so maybe that is why the histogram looks the way it does.

We can use a new "aesthetic" to split the histogram up so we can see the two different groups:

```{r}
rats %>%
    ggplot(aes(x=Lifespan, fill=Diet)) +
    geom_histogram(binwidth=60, position = "identity", alpha=0.5)
```

There are a couple things that got added there, especially in that last line, that probably aren't that clear.  First, the new "aesthetic":  In the second line, we tell R to put `Lifespan` along the `x` axis like before, but now we are also telling R to `fill` the histograms with a different color depending on the `Diet` variable.  Now, what about that other stuff in the third line?  Now, in addition to telling R how wide the bins of our histogram should be (`binwidth = 60`), `position = "identity"` tells R that it should *overlay* the two histograms.  If you don't tell R to do that, it will instead "stack" them, like this:

```{r}
rats %>%
    ggplot(aes(x=Lifespan, fill=Diet)) +
    geom_histogram(binwidth=60, alpha=0.5)
```

Which is kind of confusing.  What about `alpha = 0.5`?  This gets a bit into computer graphics, but `alpha` represents the *opacity* of something.  If `alpha = 1`, this means completely opaque, you can't see anything through it.  If `alpha = 0`, this means completely transparent (invisible).  So `alpha = 0.5` is halfway in between.  Because we are putting the two histograms on top of one another, we want them to be a little bit see-through so one doesn't obscure the other.  See what happens when we leave out that line (and `alpha = 1` by default):

```{r}
rats %>%
    ggplot(aes(x=Lifespan, fill=Diet)) +
    geom_histogram(binwidth=60, position = "identity")
```

Again, harder to see what is going on.

### Central tendency

By plotting the lifespans of the two groups of rats on separate but overlapping histograms, we can see there are some important differences between them.^[Based on the histogram, does it seem like one group of rats tends to live longer the the other?  Do the groups seem to have similar amounts of variability?]  Now we can describe those differences numerically.

We saw last week how to summarize data in terms of counting the frequency with which different values were observed.  The code to summarize the central tendency of a dataset is similar:

```{r}
rats %>%
    group_by(Diet) %>%
    summarize(M = mean(Lifespan))
```

And just like that, R has calculated the mean lifespans of the rats in each group.^[Which group has the higher average lifespan?  Does this make sense based on what you saw in the histogram?]  Looking at the code, the first two lines are similar to how we got R to make frequency tables last time:  We tell R what data we are working with (`rats`), tell R to group the data by a particular variable (`Diet`), and then tell R to make us a summary.  The last line is again a `summarize` instruction, but it looks a bit different.  Last time, we put `N = n()` inside the parentheses, which told R to count the number of individuals in each group and put those counts in a column labeled `N`.  By saying `M = mean(Lifespan)`, we are telling R to find the mean lifespan of the rats in each group and put the result in a column labeled `M`.  We replaced `N` with `M` and `n()` with `mean(Lifespan)`.  In so doing, we changed the label (`N` to `M`) *and* changed the way we summarized the data (from `n()`, which just counted, to `mean(Lifespan)` which finds the mean Lifespan).

Just like we did with the simple data above, we can swap out `mean` in the last line for different summaries.  For example, we can create a table that gives us the **median** lifespan of the rats in each group:^[What code would produce this table?  *Hint: it requires modifying the name of the central tendency measure in the last line of the previous bit of code.*]

```{r echo = FALSE}
rats %>%
    group_by(Diet) %>%
    summarize(M = median(Lifespan))
```

Comparing the mean to the median gives us a sense of the extent to which the distributions of lifespans of the two groups of rats are skewed.^[Compare the median lifespan to the mean lifespan in each group.  Based on this comparison, would you say that lifespans in either group are symmetric, positively skewed, or negatively skewed?]

### Variability

We can also make a summary table with multiple descriptive statistics, including both measures of central tendency and of variability.  The following code gives us a table that reports not just the mean lifespans of rats in each group, but the standard deviation of those lifespans as well:

```{r}
rats %>%
    group_by(Diet) %>%
    summarize(M = mean(Lifespan), SD = sd(Lifespan))
```

And, of course, we can create a table that summarizes the mean, median, standard deviation, and variance all at once!^[What code would produce the table below?  *Hint: See how we added a column for the standard deviation?  And remember how we told R to find different descriptive statistics above?*]

```{r echo=FALSE}
rats %>%
    group_by(Diet) %>%
    summarize(Mean = mean(Lifespan), Median = median(Lifespan), Variance = var(Lifespan), SD = sd(Lifespan))
```

These numerical summaries of central tendency and variability provide a concise description of the lifespans of rats under the two different diets.  This description helps us see whether there is any benefit to a restricted diet on lifespan.^[What would you conclude?  Is there a benefit or harm for the restricted diet over normal free diet?  Is this true for every rat?  Is the size of any benefit or harm big or small relative to how much rat lifespans tend to vary?]

## Wrap-up

Today we saw how to use R to find various numerical descriptive statistics, both in simple cases and with real data.  We focused on two measures of central tendency, the **mean** and the **median**, as well as two measures of variability, the **variance** and the **standard deviation**.  We saw how these quantities are useful in summarizing complex data, making it easier to draw conclusions about differences between groups.  We also learned a new trick about how to visualize histograms from multiple groups.

<!--chapter:end:02-basic_descriptives.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Scatterplots and correlation {#lab3}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/toys.jpg")
```

So far, most of what we've done has been about describing measurements on one variable at a time.  Today's session will look at how we can explore relationships *between* variables.  If there is a systematic relationship between two variables, they are said to be **correlated**.

The primary skills developed in this activity are a visual and a numerical way to explore relationships between variables:

1. The visual way is called a **scatterplot**.
2. The numerical way is the **Pearson correlation coefficient**.

Before we begin, make sure you have opened the tidyverse package from the library:

```{r}
library(tidyverse)
```

## Calculating the Pearson correlation coefficient

We saw last time that one way R is useful is as a powerful calculator.  Calculating things like standard deviations by hand is hard enough, but calculating the Pearson correlation coefficient requires doing that twice (once for each variable), in addition to a lot of other steps.  R can help ease that burden.

### From $X$ to $z$

Recall that the Pearson correlation coefficient can be found using $z$ scores.  Specifically, if we have measurements on one variable---let's call them $X$---and measurements on another variable---let's call them $Y$---we can find the correlation by first finding the $z$ scores for these two sets of measurements.

#### R can remember things

Let's try doing that in R.  Remember that R uses `c()` to say that everything within the parentheses is supposed to be treated as a **collection** of measurements on a single variable.  Rather than having to write out that collection every time we want to use it, we can tell R to remember it.  Let's say we have a collection of measurements and we want to tell R to remember them with the label "X".  We can do that this way:

```{r}
X <- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
```

The important thing to note in the line above is the arrow `<-`.  The arrow tells R to take the stuff on the right and put it under the label on the left.  You can think of the arrow as saying that the collection `c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)` is now "inside" a box labeled `X`.

Now if we write "`X`" at the console and hit enter, R will remind us what we put inside the "`X`" box:

```{r}
X
```

We can also find a specific value using square brackets.  For example, we can find the 3rd measurement this way:

```{r}
X[3]
```

Nice!^[How would you get R to tell you what the first measurement was?]  This is just like how we used $X_3$ as a shorthand for the 3rd measurement when we were doing things by hand.^[What do you think would happen if we asked for the 12th measurement?  Try it and see!  *Hint: how many measurements do we have?*]

#### Using labeled collections

Now that R can remember that whole collection of measurements, we can use it to do calculations.  For example, this will find the mean of all the numbers we put under the label "X":

```{r}
mean(X)
```

And this will find the standard deviation:

```{r}
sd(X)
```

Finally, we can combine different operations into a single line of code.  The following will convert each of the measurements in `X` into $z$ scores:

```{r}
(X - mean(X)) / sd(X)
```

We can even tell R to remember those $z$ scores like so:

```{r}
z_X <- (X - mean(X)) / sd(X)
```

Note that we gave these $z$ scores a special label, "`z_X`", so that we know they originally came from our collection labeled `X`.  Now if we ask R to tell us what we put under the "`z_X`" label, we'll get our $z$ scores back

```{r}
z_X
```

### Finding the correlation coefficient

#### Introducing a new set of measurements

Now that we've seen how to tell R to remember different collections of values, let's add another variable to the mix and find its correlation with $X$.  We will call this variable $Y$ to keep it distinct.

```{r}
Y <- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68)
```

Let's see what the mean of $Y$ is^[What code will find the mean of $Y$? *Hint: how did we find the mean of $X$?*]

```{r echo=FALSE}
mean(Y)
```

as well as its standard deviation^[What code will find the standard deviation of $Y$? *Hint: how did we find the standard deviation of $X$?*]

```{r echo=FALSE}
sd(Y)
```

Finally, let's find the $z$ scores for each of our $Y$ values and tell R to remember them with the label `z_Y`^[What code would you write to do this? *Hint: what code did we use to calculate and remember the $z$ scores for $X$?*].

```{r echo=FALSE}
z_Y <- (Y - mean(Y)) / sd(Y)
```

If we've done our job right, we should get the following output when we ask R to remember what we put into `z_Y`:

```{r}
z_Y
```

#### Calculating correlation from $z$ scores

To get the correlation coefficient between $X$ and $Y$ from our $z$ scores, we need to multiply the $z$ scores together, add them up, and divide by the number of values minus one.  We can tell R to multiply together all our pairs of $z$ scores like this

```{r}
z_X * z_Y
```

We can add them up like this, by telling R to find the `sum`

```{r}
sum(z_X * z_Y)
```

Finally, we need to divide by $N - 1$.  $N$ in this case is 11, so $N - 1 = 11 - 1 = 10$.

```{r}
sum(z_X * z_Y) / 10
```

And that's the Pearson correlation coefficient!^[Is the correlation positive or negative?  Does it seem strong or weak?]

#### Calculating correlation directly

This is what you might call the "shaggy-dog" way of calculating the correlation, in that it takes a long time to get to the punchline.  Fortunately, R gives us an even easier way!  The correlation between $X$ and $Y$ can be found like this

```{r}
cor(X, Y)
```

`cor` tells R to find the `cor`relation between the two sets of measurements labeled `X` and `Y`.

Remember that R knows what we mean by `X` and `Y` *only because we already told R what they stood for*.  To get a sense of how that works, let's replace our old `Y` with a new one:

```{r}
Y <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)
```

Now if we ask for the correlation again, it will be different^[Why do we get a different result from `cor(X, Y)` even though the code itself is the same?]

```{r}
cor(X, Y)
```

### Dealing with real data

The steps above are useful when you've got relatively few measurements that you can copy-and-paste into the R console.  Real data is often more complicated, so let's see how we can deal with it.

#### Get some data

First, we need to import some data into R using the code below.

```{r}
anscombe <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv")
```

These data are artificial and were devised by @Anscombe1973 to illustrate the importance of not interpreting a correlation coefficient without visualizing your data.

#### Correlations for different sets of measurements

There are four groups of measurements in these data.  The groups are labeled "A", "B", "C", and "D".  Measurements are on two variables, labeled "X" and "Y".  Click on `anscombe` in the environment panel in RStudio (upper right) to take a look.

We can quickly find the correlation coefficient for each group of "X" and "Y" measurements^[Compare the code we used to find these correlations with the code we used last time to find the means of the different groups of rat lifespans.  What is similar and what is different?]

```{r}
anscombe %>%
    group_by(Group) %>%
    summarize(r = cor(X, Y))
```

Notice that the correlation coefficients are almost all the same for each group.  But now let's try making some scatterplots to see if these correlations make sense.

## Scatterplots

As we've seen, a scatterplot lets us see how measurements on two variables are related to one another.  It does this with a bunch of dots "scattered" around the plot.  Each dot represents a single individual or object.  The horizontal position of the dot represents that individual's observed value on one variable.  The vertical position of the dot represents that same individual's observed value on a different variable.

### Making a group of scatterplots

R makes it easy not just to make a single scatterplot, but an entire group of them at once.  The code below produces a set of scatterplots, one for each of the four groups, that puts the "X" measurements on the horizontal and the "Y" measurements on the vertical^[What do you think the function of the line `facet_wrap("Group")` is in the code below?]:

```{r}
anscombe %>%
    ggplot(aes(x=X, y=Y)) +
    geom_point() +
    facet_wrap("Group")
```

Even though the correlation coefficient is the same for each group, the scatterplots all look very different!^[For each group, say whether you think the correlation coefficient provides a fair description of the relationship between the two variables.  Why or why not?]

## Some bigger data

Now that we've seen how to find correlation coefficients and make scatterplots, let's apply these tools to some real data.  These data come from a study by @RyanGauthier2016, who studied differences in how well people can remember different kinds of objects.  The point of this is to understand how we develop the ability to make fine visual distinctions between some things (e.g., between different peoples' faces) but not others (e.g., leaves or snowflakes).  They reasoned that the more experience someone has in dealing with a particular category of things, the better they'll be at noticing and remembering the visual features that distinguish between members of that category.

Two of those categories were different types of toys:  Barbies and Transformers.  They measured how well their participants could tell apart pictures of Barbies that they had seen from pictures of Barbies they hadn't seen.  They did the same for pictures of Transformers.  The final memory scores are a number correct out of 48.  Finally, each participant indicated on a scale from 1 to 9 how much experience they had with each type of toy.

The question is, "is experience with a type of toy *correlated with* better memory?"

### Import the data

To import the data into R, use the line below

```{r}
toys <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/toys.csv")
```

Click on the new "toys" dataset in the Environment panel (upper right) to see what the data look like.  "Barbie_Score" and "Transf_Score" are the scores on the two memory tests, one for Barbies and one for Transformers (respectively).  "Barbie_Exp" and "Transf_Exp" are the two reports of prior experience with Barbies and Transformers, respectively.

### Experience and memory for Barbies

Let's first make a scatterplot showing Barbie experience on the horizontal axis and Barbie memory on the vertical axis.  We can do that using this code:

```{r}
toys %>%
    ggplot(aes(x=Barbie_Exp, y=Barbie_Score)) +
    geom_point()
```

This gives us a sense of what might be going on.^[Based on this scatterplot, does it seem like there might be a correlation between Barbie experience and visual memory for Barbies?  If so, is it positive or negative?  Weak or strong?]  Now let's calculate the Pearson correlation coefficient (notice that there are no groups here, so we don't need a `group_by` line)^[Do the sign and magnitude of the correlation coefficient make sense based on the scatterplot?]:

```{r}
toys %>%
    summarize(r = cor(Barbie_Exp, Barbie_Score))
```

### Experience and memory for Transformers

Now let's do the same thing for Transformers.  Here's the scatterplot:^[What code would produce this scatterplot?  *Hint: what code did we use to make the Barbie scatterplot?  What variables in the dataset refer to Transformers rather than Barbies?*]

```{r echo=FALSE}
toys %>%
    ggplot(aes(x=Transf_Exp, y=Transf_Score)) +
    geom_point()
```

And this is the correlation coefficient between Transformers Experience and Transformers memory score:^[What code would find this correlation?  *Hint: what code did we use to find the Barbie correlation?*]

```{r}
toys %>%
    summarize(r = cor(Transf_Exp, Transf_Score))
```

It seems like the relationship between experience and memory is a bit weaker for Transformers than for Barbies.  But there's another interesting correlation here.

### Experience with Barbies and memory for Transformers

Let's see if there's a relationship between experience with Barbies and memory for Transformers.

```{r}
toys %>%
    ggplot(aes(x=Barbie_Exp, y=Transf_Score)) +
    geom_point()
```

The scatterplot is shown above and this gives the Pearson correlation coefficient:

```{r}
toys %>%
    summarize(r = cor(Barbie_Exp, Transf_Score))
```

Interesting!  Even though there is at best a weak relationship between Transformers experience and Transformers memory, there is actually a stronger relationship between Barbie experience and Transformers memory, just a *negative* one!^[Can you think of any reasons why there might be a negative correlation between Barbie experience and Transformers memory?]

## Wrap-up

In today's session, we saw how to find $z$ scores and correlation coefficients in R.  We also used R to make scatterplots and saw an example of why it is critical to look at a scatterplot before interpreting the correlation coefficient.

<!--chapter:end:03-correlation.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Probability and simulation {#lab4}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/kobe.png")
```

We have seen that there are two senses in which we can think about probability.  On the one hand, we can think of the probability of an outcome as its **long-run relative frequency**.  We can therefore *estimate* the probability that an outcome will happen by seeing how often it occurred in the past, relative to all the other possible outcomes in our sample space.  But on the other hand, we can think of probability as a **degree of belief** about how likely something is to happen.  This second perspective lets us think about probabilities for events that only happen once; until the event happens, all the possible outcomes have some probability of occurring.  We can *imagine* rewinding the clock and seeing how often an event may have turned out differently.

Actually, with computers, we can do better than just imagine what might happen.  We can use the computer to **simulate** many repetitions of an event.

Before we begin, make sure you have opened the tidyverse package from the library:

```{r}
library(tidyverse)
```

## Simulation basics

To begin, let's think about the simple scenario of flipping a coin.  The **sample space** consists of two possible outcomes, "Heads" and "Tails".

### Single coin flips

We can simulate a single coin flip in R using a "function" called `sample`:

```{r}
c("Heads", "Tails") %>%
    sample(size = 1)
```

The first line in the code above tells R our sample space, which is a **collection** of two outcomes, "Heads" and "Tails".  We use the `%>%` to tell R to take that sample space and, on the next line, take a **sample** of **size** 1 from that space.  By "sample of size 1", that's just a precise way of saying "a single flip".

Try running that same bit of code a few more times, so we can see that R is really simulating coin flips---you don't know ahead of time whether it will come up heads or tails.^[How many times did you have to run the coin-flip simulation before you got at least one "Heads" and at least one "Tails"?]

```{r}
c("Heads", "Tails") %>%
    sample(size = 1)
```

### A sequence of coin flips

It is kind of fun to keep flipping our simulated coin, but it would take a long time to do this repeatedly.  As you might have guessed, we can change the `size` of our sample to quickly give us a long sequence of coin flips.  Let's try that now and see if we can get a sequence of 10 flips.

```{r, error = TRUE}
c("Heads", "Tails") %>%
    sample(size = 10)
```

We got an error!  Why is that?  It's because, by default, when you tell R to draw samples, it does so **without replacement**.  In other words, once it draws a sample from the sample space, whatever outcome it sampled doesn't get put back into the space.  This wasn't a problem when we ran the single-sample code multiple times because we told R to start with a "fresh" sample space each time---that's what the first line `c("Heads", "Tails") %>%` means.

Luckily, this is easily fixed by just telling R to sample with replacement like so:

```{r}
c("Heads", "Tails") %>%
    sample(size = 10, replace = TRUE)
```

Just like R gave us "fresh" single flips, if we run the 10-flip code again, we will probably get a different sequence of outcomes:

```{r}
c("Heads", "Tails") %>%
    sample(size = 10, replace = TRUE)
```

And for the computer, doing 50 flips is not much harder than just 10^[What code would produce a sequence of 50 flips at once? *Hint: what is the number we had to change in the `sample` line to get 10 flips?*]:

```{r echo=FALSE}
c("Heads", "Tails") %>%
    sample(size = 50, replace = TRUE)
```

### Remembering simulated results

The problem with simulating so many outcomes is that now it is hard to keep track of them!  And, as we've seen, we can't just copy-and-paste the same line of code each time we want to refer back to our simulated outcomes, because that will produce a *new* sequence of outcomes.  Instead, let's tell R to remember the outcomes we simulated.

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 50, replace = TRUE)
```

All we did was add `result <- ` to the beginning of our code.  Like we saw last time, this will let R remember the particular sequence of outcomes under the label "result".  If we type "result" at the console, we'll see that R looks back in its memory and tells us how that sequence of 50 flips turned out.

```{r}
result
```

But if we simulate a new sequence of flips and tell R to remember it under the label "result", it will forget the original sequence:

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 50, replace = TRUE)

result
```

### Turning simulated results into "data"

Now that we can simulate long sequences of outcomes and get R to remember them, we are going to get R to treat those outcomes as if they were "data", in other words, as if they were from a real coin instead of a simulated one.  R keeps data in a special form called a "tibble".  We've seen this cutesy form of the word "table" before; every time we used R to summarize a dataset, it told us the result was a "tibble".

We can create a "tibble" from our simulated flips like this

```{r}
coin_data <- tibble(result)
```

We've told R to take our flips, turn them into a `tibble`, and remember the result under the label "coin_data".  Now when we ask R to remember "coin_data", we see it looks a bit different

```{r}
coin_data
```

## Estimating probabilities

Now that R can treat our simulated flips like it would "real" data, we can summarize it just like we would "real" data.  This will allow us to estimate the probabilities of different outcomes from their relative frequency.

### Frequency table

The first thing we want to know is how many flips were heads and how many were tails.

```{r}
coin_data %>%
    group_by(result) %>%
    summarize(n=n())
```

The summary above is a frequency table, which we have used before.^[Compare the code we just used to produce a frequency table with the code we used to make frequency tables in Lab 1.  What is similar and what is different?]

### Estimating probability from frequency

Let's use this frequency table to estimate the probability of getting either a heads or a tails.  To do this, we need to add a line to the code above:

```{r}
coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

The final line is called `mutate` because we are "mutating" the frequency counts to produce an estimate of the probability.  We get that estimate (`p`) by taking the frequency of each outcome, `n`, and dividing it by the total number of outcomes (`sum(n)`).

Let's see how this works by simulating a longer sequence of coin flips (1000 flips) and replacing our old results:

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 1000, replace = TRUE)

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Now let's try it with an even longer sequence of 10000 flips.^[What would you change in the code above to make it produce 10000 flips?  Does it seem like the estimated probabilities are closer to 0.5 for both heads and tails with more flips?]

```{r echo = FALSE}
result <- c("Heads", "Tails") %>%
    sample(size = 10000, replace = TRUE)

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

## More advanced simulation tricks

So far, we have just been simulating flips of a fair coin.  Now let's see how we can simulate more complex situations.

### Making up our own probabilities

By default, when you ask R to `sample`, it will assume that all the outcomes in the sample space have equal probability.  That's why when we asked R to simulate coin flips, heads and tails came up about equally often.  Let's mess with this and make ourselves a simulated *unfair* coin.

The code below simulates 1000 flips of a coin that comes up heads with probability 0.6, rather than 0.5:

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.6, 0.4))

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Notice the main difference is that we added another part to the `sample` line.  By adding `prob = c(0.6, 0.4)`, we told R that the `prob`ability of the *corresponding* outcomes in the sample space (`c("Heads", "Tails")`) should be 0.6 and 0.4, respectively.  If we reverse the order of the probabilities, notice that "Tails" now comes out ahead:

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.4, 0.6))

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

The first rule is that the first number in the collection of probabilities (after `prob`) gives the probability of the first outcome in the sample space, the second number gives the probability of the second outcome in the sample space, and so on.  Now let's try simulating 1000 flips where the probability of heads is 0.7:^[How would you change our coin flipping code to do this?  *Hint: remember that probabilities should always add up to 1.*]

```{r echo=FALSE}
result <- c("Heads", "Tails") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.7, 0.3))

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Notice that the **relative frequency** of the different outcomes is always close to the probabilities we give to R.

### Expanding the sample space

In addition to messing with the probabilities, we can mess with the sample space.  So far, we have been assuming that the coin can only come up heads or tails.  What if the coin could land on its side?  This would introduce a new possible *outcome* to our sample space.  We need to change the first line of our simulation code to reflect this:

```{r}
result <- c("Heads", "Tails", "Side") %>%
    sample(size = 1000, replace = TRUE)

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Notice that the code above did not specify any `prob`abilities.  As a result, all three outcomes in our sample space ended up happening about equally often.  Let's specify those probabilities to try and get the coin to come up on its side with probability 0.1, but otherwise come up heads or tails with equal probability and simulated 1000 flips.  The result will look something like this:^[What would you add to the simulation code to make the probability of heads, tails, and side come out this way? *Hint: Remember how we specified the probabilities for the unfair coin, and that all three probabilities should add up to 1.*]

```{r, echo = FALSE}
result <- c("Heads", "Tails", "Side") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.45, 0.45, 0.1))

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

## Estimating probabilities in data

By now we are probably sick of coin flipping.  Let's take a break and watch some basketball.  Specifically, let's watch Kobe Bryant of the LA Lakers as they played against the Orlando Magic in the 2009 NBA finals.  Use the following line of code to download a dataset consisting of every shooting attempt Kobe made that game, including whether or not it went in (i.e., was the shot a "Hit" or a "Miss"):

```{r}
kobe <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/kobe.csv")
```

### What was Kobe's hit rate?

The first question we should ask is how often Kobe's shots go in (called his "field goal percentage").  We can modify the code we used to estimate probabilities from our simulated coin flips to do the same for Kobe's real shot attempts:^[Compare the code below to the code we used above to estimate probabilities from our simulated coin flips.  What is similar and what is different?]

```{r}
kobe %>%
    group_by(shot) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

### A simulated Kobe

Using the estimates above, let's see if we can construct a simulated Kobe Bryant that, like our simulated coin, will have roughly the same probability of making a basket as Kobe did.  To do this, we need to specify our **sample space** (`c("H", "M")`, where "H" is for "hit" and "M" is for "miss") and our **probabilities** (e.g., `prob = c(0.5, 0.5)` if we think hits and misses are equally likely).  Since this is a simulated Kobe, we can make him attempt 1000 shots (or more):

```{r}
result <- c("H", "M") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.5, 0.5))

simulated_kobe_data <- tibble(result)

simulated_kobe_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Does this simulated Kobe look much like the real one?  What should we change to make it look more like the real Kobe?^[Play around with the `prob`abilities in the code for simulating shots from Kobe Bryant until you find some probabilities that make his hit rate look similar to what we actually observed.  What probabilities did you end up with?]

### Hot- or cold-handed

Commentators at this particular game remarked that Kobe seemed to have a "hot hand".  In other words, they seemed to believe that once Kobe made a basket ("H"), he was *more likely* to make a basket on his next shot.  This is a probability question!  Specifically, it is a question about **conditional probability**.  *Conditional on* whether Kobe's previous shot was a hit or a miss, what is the probability that his next shot is a hit or a miss?

We can address this question by modifying the code we used above to estimate probabilities:

```{r}
kobe %>%
    group_by(prev_shot, shot) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Here, `prev_shot` refers to whether Kobe's `prev`ious shot was a hit ("H") or a miss ("M").  By adding `prev_shot` to the grouping in our second line, what we have done is tell R to count the number of times each *combination* of hit/miss on the previous shot and hit/miss on the current shot occurred.^[Compare the code we just used with the code we used to get the number of passengers who did or did not survive on the *Titanic* in each class.  What is similar and what is different?]  As a result, when we ask R in the final line to give us probabilities, it gives us the probability of a hit or miss on the current shot *conditional on* whether the previous shot was a hit or a miss.

So, were the commentators right?^[Did Kobe have a "hot hand"?  Did he have a "cold hand", where missing a shot meant it was more likely he would miss his next shot?  Was there something else going on?  Or was there no particular difference depending on whether he made or missed his previous shot?]

## Wrap-up

In this activity, we saw how we could use R to **simulate** outcomes of many repeated events.  We could specify the **sample space** as well as the **probability** with which different outcomes could occur.  We saw how the **long-run relative frequency** got close to these probabilities.  Finally, we saw how we could use R to **estimate** probabilities from frequencies of different outcomes, including **conditional probabilities**.

<!--chapter:end:04-probability.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Models: The Binomial and the Normal {#lab5}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/allison_kristen.png")
```

One of the ways we use statistics to learn about the world beyond our data is by *testing hypotheses*.  A statistical hypothesis takes the form of a **model**.  The model allows us to figure out the probability of observing particular types of outcomes.  We can then take the types of outcomes we actually observed in our data and see whether they have high or low probability according to our model.  If they have a high probability, our hypothesis might be reasonable.  But if they have a low probability, there might be something wrong about our hypothesis.

In this session, we will explore two types of model that we have been discussing in class, the **binomial distribution** and the **normal distribution**.  Similar to our last session, we will see how we can *simulate* data according to these models and show that, as we simulate more data ("in the long run..."), the relative frequency of different outcomes converges on the probabilities we get from the model.

As we've seen, we can use these distributions to model different scenarios by adjusting their **parameters**.  These are the numbers that we feed into the distribution to get probabilities.

For the binomial distribution, its parameters are

* the probability of an outcome, which we called $\theta$ in math language, but which we will call `prob` in R; and
* the number of repetitions, which we labeled $N$ in math language, but which we will call `size` in R.

For the normal distribution, its parameters are

* the mean, which we called $\mu$ in math language, but which we will call `mean` in R; and
* the standard deviation, which we called $\sigma$ in math language, but which we will call `sd` in R.

Before we begin, let's make sure we have the `tidyverse` package loaded from our library

```{r}
library(tidyverse)
```

## The binomial distribution in R

Last time, we used R to *simulate* repeated outcomes from various kinds of events.  Some of these, like flipping a coin or Kobe making a shot, had two possible outcomes.  As a result, we can think of the number of times a particular outcome occurred as coming from a binomial distribution.

### Translating from `sample` to `rbinom`

Let's see how we can do some of the stuff we did last time using the binomial distribution instead of directly simulating the outcome of each event.

This is the code we used last time to simulate 1000 flips from a coin that had a probability of 0.6 of coming up heads:

```{r}
result <- c("Heads", "Tails") %>%
    sample(size = 1000, replace = TRUE, prob = c(0.6, 0.4))

coin_data <- tibble(result)

coin_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

We can use the number of flips (1000) and the probability of heads (0.6) as the *parameters* of the binomial distribution.  R has several important distributions "built in", including the binomial and normal.  R provides functions that allow us to directly sample from those distributions without having to use the `sample` function explicitly.

This is how we can simulate the number of heads out of 1000 flips, where the probability of "heads" is 0.6, using R's built-in binomial distribution:

```{r}
rbinom(n = 1, prob = 0.6, size = 1000)
```

It is definitely more concise than our old code!^[Notice that the `rbinom` function also requires us to give it things named `prob` and `size`, like our old simulation code.  How is the way we use `prob` and `size` similar between the two pieces of code?  How are they different?]  Notice that we had to add a thing `n = 1`, what's up with that?  Let's try changing it to see what it does and maybe we'll understand what `n` means here:

```{r}
rbinom(n = 2, prob = 0.6, size = 1000)
```

Okay, so `n` is actually the `n`umber of *sequences of events* to simulate.  In other words, we have two "numbers of things" here:  `size` is the number of repeated events in a single sequence; `n` is the number of sequences of events.  Why is it important to distinguish between these two numbers?  Let's try an example to see.

### Outcomes from different sequences of events

The Mets and the Yankees are well known rival baseball teams from New York City.  On a semi-regular basis, these two teams play so-called "subway series", which are sequences of six games played against each other.  Imagine that the probability that the Yankees win any single game against the Mets is 0.57.  There have been 8 "subway series" played so far as part of the regular season.

So in this scenario, the number of repeated event in a single sequence is 6, corresponding to the 6 games played in each subway series.  The number of sequences is 8, corresponding to the 8 subway series that have been played.  And the probability of a Yankees win in any single game is 0.57.  We can use this to simulate the number of games won by the Yankees in each of the 8 subway series:

```{r}
rbinom(n = 8, prob = 0.57, size = 6)
```

Each number that R gave us is a number of games won by the Yankees in a sequence of 6 games against the Mets.  So the first number is the number of Yankees wins in the first subway series, the second number is the number of Yankees wins in the second subway series, and so on.

Of course, if we run the simulation again, we will probably get a different set of outcomes for each subway series:

```{r}
rbinom(n = 8, prob = 0.57, size = 6)
```

### Simulated relative frequencies and probabilities

Now let's imagine playing a large number of subway series over the next several decades or centuries, say, 1000 of them.  Like last time with 1000 coin flips, this is too many to be able to see all at once.  And because this is a simulation, we can't just run the same line of code again and expect it to give us the same result.  So let's get R to remember the outcomes of 1000 subway series.

```{r}
result <- rbinom(n = 1000, prob = 0.57, size = 6)
```

Now let's do the same trick we did last time so we can get R to treat our simulated results as data.  We turn our simulation into a "tibble":

```{r}
subway_series <- tibble(result)
```

Finally, let's modify our code for making a frequency/probability table above to take a look at the distribution of outcomes for the 1000 subway series we simulated^[Describe what the 3 columns in the table ("result", "n", and "p") each represent.]:

```{r}
subway_series %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Sometimes the outcome is a tie (3 games out of 6), sometimes the Yankees win the majority of games, and sometimes the Mets win the majority (when the Yankees win fewer than 3 games).  The important thing to note is that *these are probabilities of a number of wins out of a sequence of games, not the probability of winning any single game*.  In other words, the **sample space** is not just win/lose, it is the *number* of wins out of 6 games^[What parameter would you change in order to get the binomial distribution to describe probabilities of winning or losing a single game?].

### Calculating binomial probabilities directly

When we were simulating outcomes last time, we directly specified the probability of each outcome in the sample space.  Now, what we specify are **parameters**, namely, the probability of an outcome in a single event and the number of repetitions of the event.  So if we want to know the probability of a specific outcome, like 3 wins out of 6, we need to use the binomial distribution to figure it out.

R lets us do this for single outcomes with the `dbinom` function.  For example, let's find the probability of a tie^[Compare the code for the `dbinom` and `rbinom` functions.  What is similar and what is different?]

```{r}
dbinom(x = 3, prob = 0.57, size = 6)
```

As we would expect, this is close to the relative frequency of a tie when we simulated 1000 subway series.  The meaning of probability is the same---it is the *long-run relative frequency*---it's just that we are asking about the probability of a different type of outcome.

We can also ask what is the probability that the Yankees win all six games^[What code would give us this probability?]

```{r echo=FALSE}
dbinom(x = 6, prob = 0.57, size = 6)
```

### Cumulative probabilities with the binomial

Another part of R's built-in binomial distribution is the ability to calculate **cumulative probabilities**.  For example, we might be interested in the probability that the Mets win a majority of the games in a subway series.  This corresponds to the Yankees winning either 0, 1, or 2 games.

One way to calculate this would be to add up each of those individual probabilities:

```{r}
dbinom(x = 0, prob = 0.57, size = 6) + dbinom(x = 1, prob = 0.57, size = 6) + dbinom(x = 2, prob = 0.57, size = 6)
```

But a more efficient way to do this is to use **cumulative probabilities**.  As we've seen, a cumulative probability is just the sum of the probabilities for all outcomes up to and including a given value.  We can get the same result that we got above using this more compact bit of code:

```{r}
pbinom(q = 2, prob = 0.57, size = 6)
```

This makes it easier to find the probabilities for *ranges* of different outcomes.  For example, we could get the probability that the Yankees either lose the series *or* tie it up (so 0, 1, 2, or 3 wins out of 6)^[How would we change the `pbinom` code to get this answer?]:

```{r echo = FALSE}
pbinom(q = 3, prob = 0.57, size = 6)
```

But what if the opposite happens?  What if the Yankees neither lose *nor* tie?  That means they must have won the majority of games in the series (i.e., 4, 5, or 6 games).  We could figure out the probability that the Yankees win the majority of games by adding up those individual probabilities like so:

```{r}
dbinom(x = 4, prob = 0.57, size = 6) + dbinom(x = 5, prob = 0.57, size = 6) + dbinom(x = 6, prob = 0.57, size = 6)
```

But using *cumulative* probability, we can reverse that question:  What is the probability that the Yankees *don't* win 3 or fewer games?  In other words, how much probability is "left over" after asking whether the Yankees win 3 or fewer games?  Since the probability across all outcomes has to add up to one, we get

```{r}
1 - pbinom(q = 3, prob = 0.57, size = 6)
```

Using the same logic, we can ask what is the probability that the Yankees win 5 or more games^[What code would tell us the probability of the Yankees winning 5 or more games in a series?]:

```{r echo=FALSE}
1 - pbinom(q = 4, prob = 0.57, size = 6)
```

### Summary of R's built-in binomial distribution

R makes it easy to use the binomial distribution by giving us several useful functions.  We can summarize these below:

 Code | Meaning
------|--------
`rbinom(n = ?, prob = ?, size = ?)` | Simulate `n` sequences of `size` events, where the probability of an outcome of interest is `prob`.  For each of the `n` sequences, return the number of times out of `size` that the outcome of interest occurred.
`dbinom(x = ?, prob = ?, size = ?)` | What is the probability that an outcome of interest occurs exactly `x` times out of a sequence of `size` repeated events, if the probability that the outcome occurs in a *single* event is `prob`?
`pbinom(q = ?, prob = ?, size = ?)` | What is the probability that an outcome of interest occurs `q` *or fewer* times out of a sequence of `size` repeated events, if the probability that the outcome occurs in a *single* event is `prob`?

## The normal distribution in R

Just like the binomial distribution lets us model situations where we have a sequence of repeated events, each of which has two possible outcomes, the **normal distribution** lets us model situations where we have a continuous outcome that has a certain mean and standard deviation.

### Simulating outcomes from a normal distribution

Just like we can simulate values from a binomial distribution using `rbinom`, we can simulate values from a normal distribution using `rnorm`.

Adult women in the US have an average height of about 64 inches, with a standard deviation of 6 inches.  These are the two **parameters** that we need for our normal distribution.  Using these parameters, let's imagine that we pick a random adult American women.  How tall is she?^[Compare the code for `rnorm` to the code for `rbinom`.  What is similar and what is different?]

```{r}
rnorm(n = 1, mean = 64, sd = 6)
```

Now let's pick 10 adult women at random and see how tall they are^[What code would use the normal distribution to simulate the heights of 10 adult women?]

```{r echo=FALSE}
rnorm(n = 10, mean = 64, sd = 6)
```

Each of those numbers represents the height of a simulated adult American woman.  We can also get the heights for simulated adult American men by remembering that their *mean* height is 69 inches with a *standard deviation* again of 6 inches^[What parameter of the normal distribution do we need to change in order to simulate adult male heights, rather than adult female heights?]

```{r echo=FALSE}
rnorm(n = 10, mean = 69, sd = 6)
```

Eyeballing it, it does seem like those simulated men tend to be taller than our simulated women.^[When you simulated male and female heights, were there any women who were taller than at least one of the men?  Why do you think this might be, even though men are on average taller than women?]

Now let's try simulating the heights of 10,000 women using our normal distribution model.  To do this, let's first try doing what we did above to simulate outcomes of subway series, just swapping out the very first line of code:

```{r}
result <- rnorm(n = 10000, mean = 64, sd = 6)

height_data <- tibble(result)

height_data %>%
    group_by(result) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Well that didn't work very well at all!  This illustrates the challenge of working with a *continuous* variable as opposed to a discrete one like number of games won.  Every observed value is unique, so it occurs with a frequency of one.  Luckily, we've already seen that **histograms** make it easy to visualize the relative frequencies of outcomes on continuous variables because they group together the outcomes into *bins*.

```{r}
height_data %>%
    ggplot(aes(x=result)) +
    geom_histogram(binwidth = 1)
```

Now we are beginning to see that classic bell curve shape:  There are more values around the mean (64 inches) than farther from the mean, but it is still possible to get a few folks who are particularly tall (like Allison Janney) or short (like Kristen Chenoweth).

### The mean and standard deviation

The parameters of the normal distribution are its mean and standard deviation, which determine its center and spread, respectively.  These parameters are *also* what you will tend to get if you *summarize* the central tendency and variability of samples from a normal distribution.  This is one of the awesome things about the normal distribution---its parameters tell you exactly what you should expect your data to look like.

Let's see that by summarizing the mean and standard deviation of the heights of our 1000 simulated women:

```{r}
height_data %>%
    summarize(M = mean(result), S = sd(result))
```

They may not be *exactly* 64 and 6, but they are pretty close.  In fact, let's simulate another group of 10,000 women and summarize their heights.

```{r}
result <- rnorm(n = 10000, mean = 64, sd = 6)

height_data <- tibble(result)

height_data %>%
    summarize(M = mean(result), S = sd(result))
```

Again, pretty close to the parameters we used!

### Calculating probabilities with the normal distribution

Remember that we used the `dbinom` function in R to find the probability of a specific outcome in a sequence of repeated binary events.  There is an analogous function in R for the normal distribution called `dnorm`, but we won't really ever use it.  The reason is the same as why our original attempt to make a frequency table didn't really work:  With a continuous variable, the probability of observing any single value is basically zero.

Instead, we will follow the same line of reasoning that makes the histogram useful:  For continuous variables, we aren't interested in whether they take some specific value, but whether they fall into a particular "bin".  We don't care if someone is exactly 71.05976... inches tall.  We care whether they are, for example, between 71 and 72 inches tall.  Or whether they are more than 64 inches tall.  Or whether they are less than 80 inches tall.

The probabilities of all of those kinds of things can be found by doing some clever tricks with **cumulative probability**.  Remember that this is the probability of observing a value that is less than or equal to some given cutoff.  This was how we figured out the probabilities of different numbers of wins.  Now we can do the same for different ranges of height.

Just like we had `pbinom` for cumulative probability, we also have `pnorm`.  This code gives us the probability that a woman is less than or equal to 60 inches tall:

```{r}
pnorm(q = 60, mean = 64, sd = 6)
```

This is awesome!  But it doesn't quite make clear where this probability is coming from.  Let's look back at our histogram of simulated heights, but now make it so that bars corresponding to heights less than 60 are `fill`ed with a different color:

```{r}
height_data %>%
    ggplot(aes(x = result, fill = (result <= 60))) +
    geom_histogram(binwidth = 1)
```

What `pnorm` is doing is telling us is the *area* of the histogram that is colored teal (for being less than or equal to 60).  But this immediately suggests that we can use `pnorm` to find the area colored red (for being more than 60).  We know that probabilities have to add up to 1, so the area across all the bars of the histogram equals 1.  So if we know the area for one color, we can figure out the area for the other because we know that, together, they have to add up to 1.  That gives us all we need to figure out the probability that a woman is *more* than 60 inches tall^[What code did you use to find the probability that a woman is more than 60 inches tall?]:

```{r echo=FALSE}
1 - pnorm(q = 60, mean = 64, sd = 6)
```

Cumulative probability also lets us figure out the probability that a woman is, say, between 60 and 70 inches tall.  The insight is that the probability that a woman is less than or equal to 70 inches "contains" the probability that a woman is less than or equal to 60 inches.  So to find the probability we need to subtract out that overlap.

```{r}
pnorm(q = 70, mean = 64, sd = 6) - pnorm(q = 60, mean = 64, sd = 6)
```

Let's make a colored histogram again to get a better sense of what is going on here

```{r}
height_data %>%
    ggplot(aes(x = result, fill = (result > 60 & result <= 70))) +
    geom_histogram(binwidth = 1)
```

Again, we are finding the area that is in the teal part of the histogram, corresponding to women between 60 and 70 inches tall.

Using that same logic, we can figure out the probability that a woman is between 71 and 72 inches tall^[What code did you use to find the probability that a woman is between 71 and 72 inches tall?]:

```{r echo=FALSE}
pnorm(q = 72, mean = 64, sd = 6) - pnorm(q = 71, mean = 64, sd = 6)
```

### Finding "quantiles" with the normal distribution

There's one final trick we will explore about the normal distribution.  Above, we saw how we could find the probability of observing a value that was less than or equal to a given cutoff.  The cutoff value is typically called a **"quantile"**, hence why `pbinom` and `pnorm` label it `q`.  But we can also turn that question around:  What is the quantile for which there is a specific probability of being less than that value?  In other words, if we know the cumulative probability is $p$, what must $q$ be?

We can answer that kind of question in R using the `qnorm` function for normal distributions (there is also a `qbinom` function for binomial distributions, but we won't typically use it).  What is the height for which a woman has a 0.6 probability of being *shorter* than that height?

```{r}
qnorm(p = 0.6, mean = 64, sd = 6)
```

Again, let's try making ourselves a colored histogram to understand what this answer means.

```{r}
height_data %>%
    ggplot(aes(x = result, fill = (result <= qnorm(p = 0.6, mean = 64, sd = 6)))) +
    geom_histogram(binwidth = 1)
```

Notice that for the `fill` aesthetic, we've just swapped out a number we picked ahead of time (like 60) for the `qnorm` line from above.  It looks like a bit more than half of the histogram is colored teal.  This corresponds to the 0.6 probability associated with the quantile we found.

Things will look quite a bit different if we ask what is the height for which a woman has a probability 0.99 of being *shorter* than that height?^[What code would answer this question?]

```{r echo=FALSE}
qnorm(p = 0.99, mean = 64, sd = 6)
```

Now if we make a colored histogram like we just did, it should look almost all teal, since there is such a large probability that a woman is shorter than the quantile we just found.

```{r echo=FALSE}
height_data %>%
    ggplot(aes(x = result, fill = (result <= qnorm(p = 0.99, mean = 64, sd = 6)))) +
    geom_histogram(binwidth = 1)
```

Recall that male heights have a mean of 69 inches.  Do you think the quantile corresponding to a probability of 0.99 will be higher for men than women?  Try it and see!^[What code did you use to find the 0.99 quantile for male heights?  *Hint: which parameter of the normal distribution is different for heights of men and women?*]

```{r echo=FALSE}
qnorm(p = 0.99, mean = 69, sd = 6)
```

### Summary of R's built-in normal distribution

 Code | Meaning
------|--------
`rnorm(n = ?, mean = ?, sd = ?)` | Simulate `n` samples from a normal distribution with a given `mean` and standard deviation (`sd`).
`pnorm(q = ?, mean = ?, sd = ?)` | What is the probability of observing a value less than or equal to `q` if values come from a normal distribution with the given `mean` and standard deviation (`sd`)?
`qnorm(p = ?, mean = ?, sd = ?)` | What is the value ("quantile") for which there is a probability `p` of observing a value that is less than or equal to that value, if the value comes from a normal distribution with the given `mean` and standard deviation (`sd`)?

## Wrap-up

In this session we explored the functions built into R that allow us to use the binomial and normal distributions as "models" for various situations.  We saw how we could describe a situation in terms of **parameters** and use R to simulate different outcomes, figure out probabilities of different types of outcomes, and for the normal distribution, how to find "quantiles" when we know the corresponding cumulative probability.

<!--chapter:end:05-binomial_normal.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Sampling and the Central Limit Theorem: The Wisdom of Large Samples {#lab6}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/crowd.png")
```

We have just encountered a remarkable phenomenon which goes by the weighty but unclear name "central limit theorem".  This theorem says that if we draw a large enough sample from our population of interest, the *mean* of our sample will have a much better chance of being a good *estimate* of the mean of the population.

Specifically, what happens is that there is a distribution of sample means which is approximately normal.  The mean of this distribution is the population mean (which we've labeled $\mu$) and the standard deviation of this distribution is called the "**standard error of the mean**" and is equal to $\frac{\sigma}{\sqrt{n}}$, where $\sigma$ is the population standard deviation and $n$ is our sample size.  So this distribution of sample means is centered on the "correct answer", and gets narrower and narrower the larger our sample.  This means that, the larger the sample, the better the chance that its mean is close to the population mean.

But for this to happen, our sample has to be *representative* of the population.  If the sample is *biased* in any way, then our estimate of the population mean will also be biased.  One way a sample can be biased is if it is not a *simple random sample* from the population.

In this session, we will see the "central limit theorem" in action to better understand what it means.  We will also see how sampling bias hampers our ability to estimate population means.  Finally, we will see that it is not just a statistical curiosity, but something that is surprisingly pervasive.

## Before we begin...

For this session, we will need the tidyverse package like usual.  We will *also* need another package from R's library called "infer".  Make sure you have it installed (if you already have it installed, you can skip the next line, but try running this line if something doesn't work):

```{r eval=FALSE}
install.packages("infer")
```

Now, make sure you have loaded *both* the tidyverse package *and* the new "infer" package from R's library:

```{r}
library(tidyverse)
library(infer)
```

One last thing before we begin:  Make your best guess about the answer to this question: **What percent of the world's airports are in the United States?**

## Heights

Last time, we saw how when we used a *model* to simulate lots of individual observed values, the descriptive statistics for our simulated data were very close to the model *parameters* we used.  We focused on the distribution of these individual values.  Now, we will focus on the **distribution of sample means**.  This is the distribution that represents our uncertainty about the true value of the population mean.

### A simulated population

Lets use the normal distribution to simulate the heights of an entire population of women, let's say 50,000.  Remember that the mean height is 64 inches and the standard deviation is 6 inches:

```{r}
height <- rnorm(n = 50000, mean = 64, sd = 6)

population <- tibble(height)
```

In the first line, we used the `rnorm` function to simulate the heights of `n = 50000` women, where the average height is `mean = 64` inches and the standard deviation of heights is `sd = 6` inches.  We told R to remember this collection of values under the name "height".  In the second line, we told R to treat "height" as if it were data (a "tibble") and told R to remember our data under the name "population".

### A sample from our simulated population

Now that we have a whole population at our disposal---which we can only do because we are *simulating them*!---we can experiment and see what the effect is of getting different samples from this population.

The code below selects a simple random sample of 10 women from our population:

```{r}
population %>%
    slice_sample(n = 10)
```

This is pretty similar to how we simulated things like coin flips earlier.^[Compare sampling from a population to simulating coin flips.  When we select an individual from a population, what is the *sample space*?  If we are doing simple random sampling, what is the probability that any particular individual is selected into our sample?]  The `slice_sample` function is just another way we can simulate drawing a "sample" of size `n` from a set of possibilities.

Let's try it again and see what a different sample of 10 women from this population might be:

```{r}
population %>%
    slice_sample(n = 10)
```

Now let's tell R to remember one of these samples under the name "sample_heights", so we can work with it later.

```{r}
sample_heights <- population %>%
    slice_sample(n = 10)
```

Now we can do our usual thing and find the mean and standard deviation of the heights *in this one sample*:

```{r}
sample_heights %>%
    summarize(M = mean(height), S = sd(height))
```

As we might expect, they are somewhat close to the parameters we used to generate the population, but they are unlikely to match exactly.

### Many samples from our simulated population

If we kept getting samples of size 10 from our population, how often would their sample means be close to the true population mean?  While we could just keep running that same code from above over and over again, R helps us automate this process:

```{r}
many_samples_size10 <- population %>%
    rep_slice_sample(n = 10, reps = 1000)
```

What we just did was use the `rep_slice_sample` function to `rep`eatedly sample groups of 10 women from our population^[Compare this line of code with the line we used to get a single sample of size 10, above.  What is similar and what is different?].  Specifically, we got `reps = 1000` samples of size 10, and we told R to remember these samples under the name "many_samples_size10".  Let's have a look at what the resulting simulated data look like:

```{r}
many_samples_size10
```

So there are two columns, one column is "height", representing the height in inches of each simulated woman in our samples.  The other column is "replicate" which tells us *which sample* (of the 1000 samples we made) the woman is in.  This column will let us group the women into their respective samples.

Now we can get the mean height for each sample:

```{r}
many_samples_size10 %>%
    group_by(replicate) %>%
    summarize(M = mean(height))
```

So we begin to see how the sample mean changes depending on who happened to have been selected into the sample.  But this is still too many numbers, so let's go to our old standby, the histogram.

First, let's tell R to remember those sample means we just found:

```{r}
sample_means_size10 <- many_samples_size10 %>%
    group_by(replicate) %>%
    summarize(M = mean(height))
```

Now, we are in shape to make a histogram representing the *frequency* with which different samples have different means:

```{r}
sample_means_size10 %>%
    ggplot(aes(x=M)) +
    geom_histogram(binwidth=1)
```

Certainly looks like a normal distribution!  Now let's see what the mean and standard deviation of the sample means are:

```{r}
sample_means_size10 %>%
    summarize(MM = mean(M), SEM = sd(M))
```

Remember that the standard deviation of sample means goes by the special name **standard error of the mean**, hence why we labeled it `SEM`.^[Based on the "central limit theorem", and the fact that the population standard deviation is $\sigma = 6$ and the samples are of size $n = 10$, how does the value of `SEM` above compare with what the central limit theorem would predict?]

### Many larger samples

Let's follow the same steps above, but instead of collecting many samples of size 10, let's get many larger samples of size 100.

First, let's simulate 1000 samples, all of size 100, and tell R to remember them under the name "many_samples_size100".^[What code would do this?  *Hint: what would you need to change in the code we used to produce many samples of size 10?*]

```{r echo=FALSE}
many_samples_size100 <- population %>%
    rep_slice_sample(n = 100, reps = 1000)
```

Next, let's find the means of each of those samples like we did above:

```{r}
sample_means_size100 <- many_samples_size100 %>%
    group_by(replicate) %>%
    summarize(M = mean(height))
```

And have a look at a histogram:

```{r}
sample_means_size100 %>%
    ggplot(aes(x=M)) +
    geom_histogram(binwidth=1)
```

And the mean of sample means and standard error of the mean^[Compare `SEM` we found for our samples of size 100 with what the central limit theorem says the standard error of the mean should be---is it close?  Compare `SEM` for samples of size 10 to `SEM` for samples of size 100, which is smaller?]:

```{r}
sample_means_size100 %>%
    summarize(MM = mean(M), SEM = sd(M))
```

### The probability of being wrong

Remember that, just like we never really get to see the whole population in any real-world settings, we also never really get to see the distribution of sample means in real life.  In real life, we will probably only have one sample and, therefore, one sample mean.

But because our sample mean comes from a distribution of sample means that is *normal*, we can use the normal distribution to figure out the *probability* that whatever sample mean we have is wrong, and by how much.

#### Using simulation

First, let's use our many simulated samples to see how many are wrong by different amounts.

How many of our samples of size 10 had sample means that under-shot or over-shot the true population mean of 64 inches?

```{r}
sample_means_size10 %>%
    group_by(M > 64) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

In the second line above, we grouped our samples by whether or not their sample means were greater than 64 (`M > 64`).  Looks like it is about even.^[By counting the number of samples that over-shot, we also end up knowing how many samples under-shot.  Why is this?]  This suggests that the sample means are not *biased* either high or low.

Now, how many of our samples of size 10 had sample means that were within 1 inch of the true population mean of 64 inches?  In other words, how many had sample means that were between 63 and 65 inches?

```{r}
sample_means_size10 %>%
    group_by(63 < M & M < 65) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

By comparison, how many of our samples of size 100 had sample means that were within 1 inch of the true population mean of 64 inches?^[What code produces the results below?  *Hint: what should we change in the code immediately above?*]

```{r echo=FALSE}
sample_means_size100 %>%
    group_by(63 < M & M < 65) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Wow, that's quite an improvement!  The probability that our estimate of the population mean---the sample mean---is within 1 inch of the correct answer is much higher if our sample has 100 women than 10 women.

#### Using the central limit theorem

Remember that we were only able to count those results above *because we have many many samples of the same size*.  In real life, we almost never have this.  That's why the central limit theorem is useful---we can use what we know about the normal distribution to calculate the probability of being wrong and by how much.

Let's first think about our samples of size 10.  The central limit theorem says that the means of these samples have a normal distribution with a mean of $\mu = 64$ inches and a standard deviation of $\frac{\sigma}{\sqrt{n}} = \frac{6}{\sqrt{10}} \left( \approx 1.897 \right)$ inches.  In R, we can write that standard deviation like this: `6 / sqrt(10)`, where `sqrt` stands for "square root".

Now, using the normal distribution, what is the probability that the mean we get from a sample of size 10 is within 1 inch of the true population mean?^[How does the probability we get from the normal distribution compare with the one we found by counting our samples, above?]

```{r}
pnorm(q=65, mean = 64, sd = 6 / sqrt(10)) - pnorm(q = 63, mean = 64, sd = 6 / sqrt(10))
```

And what is the probability that the mean we get from a sample of size 100 is within 1 inch of the true population mean?^[What code produces this result?  *Hint: what do we need to change in the code above?*]

```{r echo=FALSE}
pnorm(q=65, mean = 64, sd = 6 / sqrt(100)) - pnorm(q = 63, mean = 64, sd = 6 / sqrt(100))
```

### Biased samples

So far, we assumed that our samples were all simple random samples.  What happens if this is no longer the case?  What if instead our samples are **biased**?

Imagine that instead of selecting women at random, we got a bunch of women in a large room and selected them based on how easy they were to see.  Obviously this would lead us to select more tall than short women, because it is easier to see a tall person above a crowd (and a tall person could hide a shorter person behind them).  We can simulate this kind of biased sampling by telling R to give more "weight" to a woman who is taller when selecting samples.  This doesn't mean that a short person will *never* get selected (maybe they are standing in front so no one is in the way), but it does make it less likely.

Here's an example of a single sample of size 10, where more "weight" is given to women who are taller:

```{r}
population %>%
    slice_sample(n = 10, weight_by = height)
```

Now let's repeat the same steps from above to get 1000 *biased* samples each of size 10 and size 100 (note that this may take longer than simulating unbiased samples):

```{r}
many_biased_samples_size10 <- population %>%
    rep_slice_sample(n=10, reps=1000, weight_by = height)

many_biased_samples_size100 <- population %>%
    rep_slice_sample(n=100, reps=1000, weight_by = height)
```

Now let's get the means of each of these biased samples:

```{r}
biased_sample_means_size10 <- many_biased_samples_size10 %>%
    group_by(replicate) %>%
    summarize(M = mean(height))

biased_sample_means_size100 <- many_biased_samples_size100 %>%
    group_by(replicate) %>%
    summarize(M = mean(height))
```

Let's look at some histograms to get a sense of their distribution shape.  Here it is for biased samples of size 10:

```{r}
biased_sample_means_size10 %>%
    ggplot(aes(x=M)) +
    geom_histogram(binwidth=1)
```

And for biased samples of size 100^[Do the shapes of the distributions of biased means still look roughly normal, or do they look a bit skewed?  Is there a difference between biased samples of size 10 vs. 100?]:

```{r}
biased_sample_means_size100 %>%
    ggplot(aes(x=M)) +
    geom_histogram(binwidth=1)
```

And finally, let's have a look at the mean of the sample means and their standard errors for biased samples of size 10:

```{r}
biased_sample_means_size10 %>%
    summarize(MM = mean(M), SEM = sd(M))
```

And for biased samples of size 100^[What is different between the mean of sample means (`MM`) and the standard error of the mean (`SEM`) for biased samples versus unbiased samples?  Does this difference make sense given the way our samples are biased?]:

```{r}
biased_sample_means_size100 %>%
    summarize(MM = mean(M), SEM = sd(M))
```

## The Wisdom of the Crowd

We've seen how, at least when we have simple random samples, the larger a sample, the more likely it is that our sample mean is close to the actual population mean.  But this isn't just a statistical curiosity about estimating properties of populations, like their heights.  It turns out that if you want to know anything about the world, you're better off asking more people.  This concept was originally identified by @Galton1907 and is now called the "wisdom of the crowd" [@Surowiecki2004].

The wisdom of crowds was studied recently by @SteegenEtAl2014 who replicated a study by @VulPashler2008.  They asked people questions about world facts like the one you answered at the beginning of the session.  These people weren't experts and weren't allowed to look up the answer.  But as we will see, their *mean* answer is actually pretty close to the truth.

### Grab the data

First, let's download the guesses that the participants in @SteegenEtAl2014 made to the question you answered at the beginning:

```{r}
wisdom_data <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/wisdom.csv")
```

Taking a look at the data, each row represents a different person's answer to the question.  The answer they gave is in the "guess" column.  There is also some demographic information about each person (age, sex, nationality).

### Make a histogram

Now let's make a histogram of people's guesses^[Why do you think the histogram looks kind of "lumpy"? *Hint: how much detail do you think people can give when making a guess like this?*]:

```{r}
wisdom_data %>%
    ggplot(aes(x=guess)) +
    geom_histogram(binwidth = 5)
```

Doesn't look especially normal, but it is also pretty spread out.  This makes sense given that very few if any people in this sample would be expected to be experts in airport distribution.

### What is the right answer?

At the time these data were collected, the correct answer to the question, "what percent of the world's airports are in the United States?" was **32.3%** (how close were you?).

How many people over- or under-shot the answer?

```{r}
wisdom_data %>%
    group_by(guess < 32.3) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

### What about the average?

How close was the *mean* guess to the right answer?

```{r}
wisdom_data %>%
    summarize(M = mean(guess))
```

Huh, not too bad!  The difference between the mean guess and the right answer is about $32.3 - 27 = 5.3$.  How many people were at least as close to the correct answer as the mean guess, in other words, how many people guessed a value between 27 and $32.3 + 5.3 = 37.6$?^[What code would give us this result?  *Hint: remember how we found the number of sample mean heights that were within an inch of the true population average above.*]

```{r echo=FALSE}
wisdom_data %>%
    group_by(27 < guess & guess < 37.6) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

So the mean guess is doing better than the vast majority of individual guesses.  This is pretty remarkable!  The "wisdom of the crowd" is another case in which large samples get us closer to the truth.^[Why do you think that the *mean* of a set of guesses does so much better than most of the individual guesses?]

## Wrap-up

We've seen the "central limit theorem" in action:  The larger our sample, the more likely it is that the sample mean is close to the population mean.  This is valuable because, in reality, we almost never know the true population mean, so we have to *estimate* it using a sample.  But we also saw that a *biased* sample can give us the wrong answer.  Finally, we saw that the power of large samples extends to making guesses about the world at large---maybe instead of finding an expert, we just need a lot of guesses!

<!--chapter:end:06-central_limit.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Confidence intervals and hypothesis testing {#lab7}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/heart_clock.jpg")
```

As we've seen, one of the remarkable fruits of the "central limit theorem" is the **confidence interval**.  This interval tells us where sample means are likely to be, assuming

1. Samples are selected from the population using simple random sampling; and
2. That we either know or hypothesize the parameters of the population we are sampling from.

We explored the consequences of sampling last time.  Today, we will see more about how to find confidence intervals in R and use them to test **hypotheses**.  Specifically, we will use confidence intervals to test "null" hypotheses.  As we've seen, there are four basic steps to testing a hypothesis using a confidence interval.

1. Figure out what parameters would describe the population *if* the hypothesis were true.
2. Decide how wide to make our confidence interval.
3. Figure out the upper and lower boundaries of the confidence interval.
4. Compare our sample to the confidence interval and, if it is outside the interval, *reject* our original hypothesis.

In the first part of the session, we will do hypothesis tests with simulated data where we know whether the null hypothesis is true or not.  In the second part of the session, we will do a hypothesis test on real data and see what we can learn about the population from which it was sampled.

Before we begin, make sure you've got the "tidyverse" package loaded from R's library.

```{r}
library(tidyverse)
```

## Be still, my (artificial) heart

The resting heart rate within the typical US adult population has a mean of $\mu = 71$ beats-per-minute with a standard deviation of $\sigma = 9$ beats-per-minute.  We all know that sudden excitement or shock tends to increase our heart rate, but interestingly, the effect of chronic stress on heart rate is not well established.  There is evidence for both a *higher* resting heart rate due to sustained minor stress, as well as a *lower* resting heart rate due to frequent major stressful life events.  In any case, it is clear that detecting abnormal heart rate can be important not just for physiological health, but psychological health as well.

In this part of the session, we will simulate different samples of individuals with different resting heart rates and test the hypothesis that these individuals come from a population with the same heart rate parameters as the typical US adult population.  We will follow the steps above to construct a confidence interval (steps 1--3), then compare our simulated sample against it (step 4).

### Constructing confidence intervals

Before we can even get to step 1, we need to be clear what our hypothesis is that we are testing.  Our *research* hypothesis is that the sample of individuals comes from a population with a *different* mean heart rate than the typical US adult.  The **statistical hypothesis** that we are testing is the **null** hypothesis.

> The **null hypothesis** is that our sample comes from a population with the **same** mean heart rate as the typical US adult population, with $\mu = 71$.

If we can *reject* this null hypothesis, then we have reason to believe that our sample is meaningfully different from the typical population and we say it is **statistically significant**.

#### Figure out what parameters would describe the population *if* the hypothesis were true

From the description above, we know the population mean ($\mu = 71$) and standard deviation ($\sigma = 9$).  Let us assume that our sample size is $n = 40$, which is typical for this kind of research.  The central limit theorem tells us that our sample means will, then, have a normal distribution with mean $\mu = 71$ and **standard error of the mean** $\frac{\sigma}{\sqrt{n}} = \frac{9}{\sqrt{40}}$.  We can use R to calculate what the standard error is.

```{exercise}
What R code would give us the standard error of the mean?

```

```{r echo=FALSE}
9 / sqrt(40)
```

#### Decide how wide to make our confidence interval

Our confidence interval is defined by how unlikely a sample has to be before we are willing to reject the null hypothesis.  A standard rule is to say that if a result has probability $0.05$ or less if the null hypothesis were true, then we can *reject* the null hypothesis.  This standard rule corresponds to a "95% confidence interval", where the "95%" refers to $1 - 0.05 = 0.95$, which is the probability that we will *fail to reject* the null hypothesis when it is true.

Let us adopt the standard rule for now and use the 95% confidence interval.

#### Figure out the upper and lower boundaries of the confidence interval

To find the upper and lower boundaries of the confidence interval, we have to divide the $0.05$ probability of being outside the interval between the upper and lower ends of the scale.  This means that there is a probability of $\frac{0.05}{2} = 0.025$ of being *less than* the lower boundary and a probability of $0.025 + 0.95 = 0.975$ of being *less than* the upper boundary.

The **lower** boundary of the 95% confidence interval is then

```{r}
qnorm(p = 0.025, mean = 71, sd = 9 / sqrt(40))
```

while the **upper** boundary is

```{r}
qnorm(p = 0.975, mean = 71, sd = 9 / sqrt(40))
```

```{exercise}
Based on the upper and lower boundaries we just found, what is the *margin of error*?

```

#### Visualization

The logic of hypothesis testing can be easier to understand if we visualize the distribution of sample means and the 95% confidence interval.  To do that, we will first use the normal distribution to simulate different sample means, then make a colored histogram of those sample means that distinguishes between ones that are inside the confidence interval and ones that are outside the confidence interval.

First, let's simulate a large number of means of possible samples of size $n = 40$ that we might observe if the null hypothesis were true.

```{r}
M <- rnorm(n = 10000, mean = 71, sd = 9 / sqrt(40))
```

Now let's turn those into "data" so we can use R to make a histogram of these simulated sample means.

```{r}
sample_means_size40 <- tibble(M)
```

And finally make the histogram

```{r}
sample_means_size40 %>%
    ggplot(aes(x = M)) +
    geom_histogram(binwidth=0.5)
```

Now we need to color in this histogram depending on whether the sample mean is inside or outside the confidence interval.  To do that, let's tell R to remember the upper and lower boundaries we found earlier

```{r}
ci_lower <- qnorm(p = 0.025, mean = 71, sd = 9 / sqrt(40))
ci_upper <- qnorm(p = 0.975, mean = 71, sd = 9 / sqrt(40))
```

That makes it easier to fill the bars with different colors:

```{r}
sample_means_size40 %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth=0.5)
```

### Compare our sample to the confidence interval

In this part of our session, we will *simulate* different samples and compare them to the confidence interval we just found.  We will see how different our sample needs to be from the typical population before we consistently reject the null hypothesis.

#### When the null hypothesis is *true*

First, let's imagine that the sample of individuals we get actually does come from a population with the same mean heart rate as typical US adults, i.e. with $\mu = 71$.  We will tell R to remember our sample under the label "heart_rate_sample".

```{r}
heart_rate_sample <- rnorm(n = 40, mean = 71, sd = 9)
```

```{exercise}
Compare the code just we used to simulate a sample of heart rates with the code we used to simulate sample means.  Why is the mean the same?  Why is the standard deviation different?

```

```{r}
mean(heart_rate_sample)
```

Is the sample mean outside the confidence interval, which would lead us to *reject* the null hypothesis, even though it is actually true?

```{exercise}
Try running the previous two lines of code a few more times (simulate a new sample and find its mean).  How many times did it take before you simulated a sample with a mean that fell *outside* the 95% confidence interval?

```

#### When the null hypothesis is *false*

Now let's imagine that the sample we get comes from a population that has a higher resting heart rate, on average, than the typical US adult population.  Will we correctly reject the null hypothesis?

Let's say that the mean heart rate in this *different* population is 74.

```{r}
heart_rate_sample <- rnorm(n = 40, mean = 74, sd = 9)
```

How does the mean of this sample compare to the confidence interval we found above?

```{r}
mean(heart_rate_sample)
```

```{exercise}
Run the previous two lines of code 10 times.  Out of those ten times, how often did the sample mean fall outside the 95% confidence interval?

```

```{exercise}
In the code we just used, we assumed that our samples came from a population with a mean of 74.  How much bigger do you think this mean would need to be before almost all samples would fall outside the 95% confidence interval?  75?  76?  100?  Why?

```

## Lay your weary head to rest

Having tested hypotheses using simulated resting heart rate data, let's see about testing a hypothesis using some real data.  These data have to do with a different kind of rest, sleep.  These are the number of hours slept in a particular night for a simple random sample of $n = 110$ US college students.  We will use hypothesis testing with confidence intervals to *infer* whether college students tend to sleep the recommended amount per night.

First, let's make sure to download the data to get it into R

```{r}
student_sleep <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/student_sleep.csv")
```

If you click on the "student_sleep" data in R's environment pane in the upper right, you'll see that it is just a single column called "hours", where each row in the number of hours slept by a different student.

Let's also make a histogram to get a sense of what the data look like.

```{r}
student_sleep %>%
    ggplot(aes(x = hours)) +
    geom_histogram(binwidth = 1)
```

```{exercise}
Describe the shape of the distribution of number of hours slept.  Does it seem symmetrical, skewed?  How many modes does the distribution seem to have?

```

### Figure out what parameters would describe the population *if* the hypothesis were true

Our *research* hypothesis is that college students do *not* sleep the recommended amount per night.  According to the National Sleep Foundation, the recommended distribution of sleep for young adults has a mean of $\mu = 8$ hours and a standard deviation of $\sigma = 1$ hour.  As a result, the **null hypothesis** that we are going to test is that the mean number of hours slept by college students is *equal to* the recommended population mean of $\mu = 8$.

Notice that we are again using a "hypothetical" population to specify our null hypothesis.  This hypothetical population is college students who sleep the recommended amount per night.  We are going to see if our sample could plausibly have come from such a population.

### Decide how wide to make our confidence interval

Again, we have a choice to make here about how willing we are to make a "Type I error", that is, to reject the null hypothesis even when it is true.  For the sake of argument, let's say that we want to be very conservative and will try to minimize our risk of making a Type I error.  So instead of a 95% confidence interval, let's make a 99% confidence interval.  This means that the probability of making a Type I error is only $0.01$.

### Figure out the upper and lower boundaries of the confidence interval

We now have all the ingredients we need to make a confidence interval.  We have the population mean ($\mu = 8$), population standard deviation ($\sigma = 1$), sample size ($n = 110$), and confidence interval width (99%).

The lower bound of our confidence interval is then

```{r echo=FALSE}
qnorm(p = 0.005, mean = 8, sd = 1 / sqrt(110))
```

while the upper boundary is

```{r echo=FALSE}
qnorm(p = 0.995, mean = 8, sd = 1 / sqrt(110))
```

```{exercise}
What is the R code for finding those lower and upper boundaries of the 99% confidence interval?  *Hint: what do you need to change in the code we used for finding the confidence interval boundaries in the heart rate example above?*

```

### Compare our sample to the confidence interval

Finally, let's find our sample mean and compare it to the confidence interval to decide whether or not to reject our null hypothesis:

```{r}
student_sleep %>%
    summarize(M = mean(hours))
```

That is definitely outside the bounds of our 99% confidence interval!  As a result, we **reject** the hypothesis that the mean number of hours slept by the population of college students is equal to the recommended amount of $\mu = 8$ hours.

```{exercise}
Although we rejected the null hypothesis in this case, take a look at the histogram we made above of the complete distribution of student sleep time.  Is it the case that every single student sleeps less than the recommended amount?  What does this mean for how we should interpret the results of our hypothesis test?

```

## Wrap-up

In this session, we saw how we can use R to find confidence intervals and how we can use those intervals to test null hypotheses.  We saw how hypothesis tests are not perfect---they can lead us to reject the null hypothesis even if it is true (Type I error) and they don't always lead us to reject the null hypothesis even when it is false (Type II error).  Even so, hypothesis tests can be a powerful way to learn about the world using data.

<!--chapter:end:07-confidence.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Statistical Power {#lab8}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/polarbear.png")
```

If we conduct a null hypothesis test and decide to *reject* the null hypothesis, there are two possibilities:  Either we *incorrectly* rejected the null hypothesis, in which case we made a **Type I error**; or we *correctly* rejected the null hypothesis (and thereby avoided a **Type II error**).  We've seen that *we* get to decide the probability of a Type I error because it is the same thing as our "alpha level".  But what is the probability that we *correctly* reject the null hypothesis?  This probability is called the **power** of our test and is often called "beta" ($\beta$).

The challenge in figuring out what "beta" is that we do not know ahead of time how far from the null hypothesis our results might be.  The difference between the true population parameters and those assumed by the null hypothesis is called **effect size**.  We already encountered this last session, in which we wondered how different the heart rates in a sample would need to be before we could always reject the null hypothesis.  What we were doing was changing the "effect size" and seeing that, with a bigger effect size, *power* also increased.

In this session, we will explore the factors that contribute to power and conduct a null hypothesis test on real data to assess the effectiveness of a computer-based learning activity.

Before we begin, let's make sure to load the `tidyverse` package from R's library.

```{r}
library(tidyverse)
```

## The Power of Positive Feedback

We will be looking at a study reported by @DayEtAl2015.  They were interested in how to convey to middle school students the concept of "positive feedback".  Positive feedback occurs whenever changing one part of a system leads to changes in another part of a system, which then causes additional change in the first part (A increases B and B increases A).  Positive feedback is common in many natural, social, and technological systems.  For example, having more wealth (A) allows you to put money into investments (B) which has the potential to give you even more wealth (A).

@DayEtAl2015 wondered whether students could learn the concept of positive feedback by playing with a computer simulation of the relationship between ice and climate.  The ice covering Earth's north and south poles helps keep the planet cool because ice reflects sunlight rather than absorbing it (think of how dark clothes get warmer in sunlight than light colored clothes).  Positive feedback occurs because if the global temperature increases (A), this melts the ice (B), which then leads to more warming (A) because there is less ice to reflect sunlight back into space (for crossword puzzle fans, the technical term for how reflective a surface is "albedo").  The simulation was meant to illustrate this relationship to students so they could identify similar kinds of positive feedback situations elsewhere.

Although their whole study looked at a number of things, we will focus on one particular **research question**: Can students learn the concept of positive feedback from a computer simulation?

### Look at the data

Let's first take a look at their data to get a sense of how they addressed these two research questions.  Load it into R:

```{r}
feedback_learning <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/learning_subset.csv")
```

Click on the new "feedback_learning" data that's now in R's environment (upper right panel).  Each row represents data from a single student and there are several columns.  For now, let's focus on the last three columns:

* Pretest: This is a score (between 8 and 40) on a test designed to measure understanding of the concept of positive feedback.  This test was given prior to doing the computer simulation activity.  The score is based on 8 questions which can give between 1 and 5 points.
* Posttest: This is another score (between 8 and 40) on a similar test, but given *after* the activity.
* **Improvement**: This is the difference between the posttest and pretest scores, and this is how we *operationalize* the construct of "learning".  If a student does better after the activity than before, they will have a positive improvement.  If they do worse, they will have a negative improvement.  And if they don't learn anything, their improvement is zero.

In the next section, we will do some computer simulations ourselves!  Specifically, we will explore various factors that affect our **power** to detect whether the activity actually led to student learning or not.

```{exercise}
We are going to use the normal distribution as our *model* of the average improvement.  Why is the normal distribution more appropriate than the binomial distribution?  (*Hint: think about what kinds of values a normal distribution can produce and what kinds of values a binomial distribution can produce.*)

```

### Specifying our null hypothesis

If the answer to the research question (above) was "no", then we should expect no difference between the pre- and posttest scores.  This tells us that our **null hypothesis** is that the population mean "Improvement" is zero.  We will do a **two-tailed test** because we want to be open to the possibility that the activity actually *harmed* students' understanding.  This gives us our null and alternative hypotheses:

**Null hypothesis ($H_0$)**: $\mu = 0$

**Alternative hypothesis ($H_1$)**: $\mu \neq 0$

We are still missing something:  Although we know what the population *mean* would be if the null hypothesis were true, what about the population *standard deviation*?  In other words, what would be the typical variability between students on these tests?  For the moment, let's speculate that the population standard deviation is $\sigma = 5$, representing the range of scores on a *single* question on the test.  What we are saying is that we think the typical variability between students is around one question's worth of points.

To make things easier for us later on, let's give these numbers special labels in R:

```{r}
null_mean <- 0
null_sd <- 5
```

Finally, we need to think about the *sample size* so that we can use the Central Limit Theorem to figure out where to put the boundaries of our confidence interval.  For now, let's assume that we will have a sample of 30 students and give that number a special label as well:

```{r}
sample_size <- 30
```

### Building our confidence interval

Let's say that we will reject the null hypothesis if the sample we get has less than a 0.05 probability of happening if the null hypothesis were true.  In other words, we are setting (for now) our *alpha level* to 0.05, and as a result we need to find a 95% confidence interval.  Let's give this a label so we can use it in R:

```{r}
alpha_level <- 0.05
```

As mentioned above, we will use the normal distribution to find the upper and lower boundaries.

```{r}
ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

Notice that, in addition to giving labels to the lower and upper ends of our confidence interval (`ci_lower` and `ci_upper`, respectively), we used the labels we made earlier to specify the `mean` and `sd` in `qnorm`.  Very handy!

### Visualizing the distribution of sample means

Finally, let's visualize the distribution of sample means that we would get if the null hypothesis were true.  Like we've done before, we will use colors to distinguish between samples that fall outside our confidence interval---and into the *critical region* leading us to reject the null hypothesis---from those that fall inside it.

First, let's simulate a large number of possible sample means assuming the null hypothesis is true:

```{r}
M <- rnorm(n=10000, mean = null_mean, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)
```

Now let's make a colored histogram so we can see how often sample means fall inside or outside our confidence interval.

```{r}
sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)
```

We can also check and see how often one of the sample means from the null hypothesis ends up outside the confidence interval, leading us to make a Type I error by incorrectly rejecting the null hypothesis:

```{r}
sample_means %>%
    group_by((M > ci_lower) & (M < ci_upper)) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

```{exercise}
In the table above, the row labeled "FALSE" corresponds to simulated sample means that fell outside the confidence interval, which would lead us to (incorrectly) reject the null hypothesis.  How close is the probability "p" in that column to the alpha level we decided on earlier?

```

## Achieving a target level of power

A typical goal in many research areas is to have at least an 0.8 probability of correctly rejecting the null hypothesis.  In this section, we will imagine different scenarios with the aim of achieving this level of power.

### Effect size

**Effect size** is the difference between the actual population parameters and the population parameters we assume in the null hypothesis.  In that sense, the null hypothesis assumes the "effect size" is zero.

In this case, "effect size" refers to how much test scores improve as a function of the learning activity.  Let's imagine what the distribution of sample means would look like if the effect size were 1, in other words, if the learning activity improved scores by, on average, a single point.

First, let's put a label on this guess to make our lives easier:

```{r}
effect_size <- 1
```

Now, let's do something similar to what we did above and simulate a whole bunch of sample means assuming this effect size:

```{r}
M <- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)
```

And we can visualize what this distribution looks like, along with how much of it falls in the critical region, leading us to *correctly* reject the null hypothesis.

```{r}
sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)
```

We can approximate the probability of correctly rejecting the null hypothesis using our simulation:

```{r}
sample_means %>%
    group_by((M > ci_lower) & (M < ci_upper)) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

But we can also find *power* exactly by using the normal distribution:

```{r}
pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

```{exercise}
Try setting `effect_size` to different values (e.g., `effect_size <- 0.5`) and then re-run the lines of code we just used (i.e., simulate new sample means, make a histogram, and calculate power using the normal distribution).  Try to find an effect size where the power ends up as close as you can get it to 0.8 without going over.  What effect size did you find?

```

### Sample size

Now let's reset our hypothetical effect size back to 1 point.

```{r}
effect_size <- 1
```

Now instead of messing around with effect size, let's see if we can change the **sample size** to try and achieve a power of 0.8.  This is a bit trickier because we also have to adjust our confidence interval limits, because they also depend on sample size.

For example, let's try increasing our sample size to 40.

```{r}
sample_size <- 40
```

Now, we have to re-calculate our confidence interval limits in addition to everything else:

```{r}
ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))

M <- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)

sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)

pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

```{exercise}
Try setting `sample_size` to different values (e.g., `sample_size <- 13`) and then re-run the lines of code we just used (i.e., find the CI limits, simulate new sample means, make a histogram, and calculate power using the normal distribution).  Try to find a sample size where the power ends up as close as you can get it to 0.8 without going over.  What sample size did you find?

```

### Alpha level

Let's put the sample size back to its original value of 30.

```{r}
sample_size <- 30
```

Now, we are going to mess around with the **alpha level** in order to achieve 0.8 power.  In other words, we are changing the width of our confidence interval so that it allows us to correctly reject the null hypothesis 80% of the time.

```{exercise}
Is there a potential problem with messing with the alpha level this way?  Are we increasing the probability of making a different kind of error?

```

For example, the code below is based on changing our alpha level from 0.05 to 0.1.  Once again, we have to re-do our confidence interval:

```{r}
alpha_level <- 0.1

ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))

M <- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)

sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)

pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

.

```{exercise}
Try changing `alpha_level` in the code above to different values and then re-run the code to make a histogram and calculate power.  Try to find an alpha level where the power ends up as close as you can get it to 0.8 without going over.  What alpha level did you find?

```

### Population standard deviation

There's one final thing that can affect power, and that is the **population standard deviation ($\sigma$)**.  Obviously, this is not something we can actually control in the real world, but it is important to consider when thinking about the power to correctly reject the null hypothesis.  If the population is more variable, it will be harder to detect any differences that are the result of our intervention.

Let's first reset our alpha level back to 0.05

```{r}
alpha_level <- 0.05
```

As above, when we mess with the population standard deviation, this trickles down into every aspect of our hypothesis test, since `null_sd` appears pretty often.  For example, let's try making the population twice as variable as it was before by setting `null_sd` to 10.

```{r}
null_sd <- 10

ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))

M <- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)

sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)

pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

```{exercise}
Try changing `null_sd` in the code above to different values and then re-run the code following it to make a histogram and calculate power.  Try to find a population SD where the power ends up as close as you can get it to 0.8 without going over.  What population SD did you find?

```

## Using data to test the null hypothesis

Now that we've explored the factors that contribute to the **power** of this---and any---null hypothesis test, let's see how the data actually played out.

First, remember that we had to make a reasonable guess about the population SD before.  Now, we are going to *estimate* it using the sample SD of our actual data.  This has some consequences we will discuss later in the course, but for now we will accept this as is:

```{r}
null_sd <- sd(feedback_learning$Improvement)
```

Above, the `$` pulls out the `Improvement` column from our `feedback_learning` dataset, so `null_sd` is now the standard deviation of the improvement values we actually observed.

Also, notice that the sample size they had was 69, so let's make sure to adjust that as well:

```{r}
sample_size <- 69
```

### Confidence interval limits

Now, we find the upper and lower limits of our confidence interval:

```{r}
ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

And we can find the sample mean (and give it a label, since we'll be using it later):

```{r}
sample_mean <- mean(feedback_learning$Improvement)
```

Because this falls outside the confidence interval, we **reject** the null hypothesis!

### The $p$ value

Let's also find the $p$ value, the probability that we would observe a result at least as extreme as our result if the null hypothesis were true.  Because the sample mean is *larger* than the mean according to the null hypothesis, we need to find the probability of being *above* the sample mean:

```{r}
1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

But remember that because this is a **two-tailed** test, we need to multiply the number above by 2, to account for *both* the upper and lower parts of the critical region.

```{r}
2 * (1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)))
```

This is the final $p$ value, and we can see that it is less than our alpha level of 0.05.

```{exercise}
By rejecting the null hypothesis, our results are statistically significant.  Do you believe they are practically significant?  In other words, do you believe that the improvement in scores resulting from the learning activity is large enough to care about?  Why or why not?

```

### Estimated power

Finally, let's try something interesting.  Assume that the sample mean reflects the actual effect size:

```{r}
effect_size <- sample_mean
```

Now we can use this to simulate plausible sample means we might have observed if the effect size were equal to the sample mean.  As a result, we can see what the power of this test **would be** if applied to similar studies in the future.  All we need to do now is run the *very same code* we used in our simulations above to visualize and calculate the *power* that this study had.

```{r}
M <- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
sample_means <- tibble(M)

sample_means %>%
    ggplot(aes(x = M, fill = (M > ci_lower) & (M < ci_upper))) +
    geom_histogram(binwidth = 0.5)

pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

```{exercise}
Based on the power we just calculated, do you feel more confident believing that we correctly rejected the null hypothesis?  Does this change your opinion about whether the results are practically significant?  Why or why not?

```

## Wrap-up

We conducted simulations to see how effect size, sample size, alpha level, and population variability all affect the **power** of a test to correctly reject the null hypothesis.  We conducted such a test on actual data studying whether a computer simulation learning activity increased student understanding of the concept of "positive feedback".  In addition, we saw how to use an observed effect size to calculate the power of a test.

<!--chapter:end:08-power.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# $t$ tests {#lab9}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/lullaby.png")
```

In this session, we will see how we can use R to do all the calculations involved in conducting the various types of $t$ test we have been learning about.  In addition, we will begin to get acquainted with some more advanced R functions that make it easier to do these tests on real data, including some helpful visualizations for understanding the relationship between $t$ values and $p$ values.

For this session, we are going to need to load both our standard `tidyverse` package as well as the `infer` package from R's library.

```{r}
library(tidyverse)
library(infer)
```

## $t$ tests on tiny data

To get acquainted with how $t$ tests work in R, let's first try them out on some simple artificial data before applying them to some real data.  Imagine that we're designing an advertisement and want to make sure it is attention-grabbing.  The ad will be embedded in a webpage.  We use an eye tracker to monitor people's eye movements over the course of 1 minute while looking at a fake webpage containing the ad.  We *operationalize* the construct of attention-grabbing by measuring the proportion of that time that a person is looking at the ad.

```{r echo=FALSE}
n <- 5
logit_mean <- qlogis(0.35)
logit_sd <- qlogis(0.6) - qlogis(0.4)
logit_effect_size <- qlogis(0.4) - qlogis(0.3)

while (TRUE) {
    X <- rnorm(n=n, mean=logit_mean, sd=logit_sd)
    effect <- rnorm(n=n, mean=logit_effect_size, sd=logit_sd)

    X_1 <- round(plogis(X - effect), 3)
    X_2 <- round(plogis(X + effect), 3)
    
    t_single <- t.test(X_1, mu = 0.5, alternative='greater')
    t_paired <- t.test(X_1, X_2, paired=TRUE)
    t_indep <- t.test(X_1, X_2, var.equal=TRUE)

    if (t_single$p.value > 0.2 & t_paired$p.value < 0.04 & t_indep$p.value > 0.2) {
        break
    }
}
```

The result is, for each person, a number between 0 (never looked at the ad) and 1 (only looked at the ad).  Here are measurements from a sample of $N = 5$ individuals:

```{r}
X_1 <- c(0.507, 0.052, 0.172, 0.066, 0.204)
```

Notice that we have labeled them `X_1` because we will soon have more than one set of measurements.

### One-sample $t$ test

If participants were looking at the ad half the time on average, their population mean would be $\mu = 0.5$.  Our first **research question** is therefore, "do people look at the ad more than half the time?"

#### State your hypotheses

Given this research question, our **null hypothesis** is that the mean ad proportion is less than or equal to 0.5 ($H_0$: $\mu \leq 0.5$) while our **alternative hypothesis** is that the mean ad proportion is more than 0.5 ($H_1$: $\mu > 0.5$).  This is, therefore, a **one-sided** test.

#### Set your alpha level

Let's adopt a relatively lenient alpha level of 0.1.

#### Find the $t$ value

To get the ingredients we need for the $t$ value, we need to know the deviation of the *sample mean* from the null hypothesis as well as the *estimated standard error* of the mean.  The deviation from the null hypothesis is the difference between the sample mean and that specified by the null hypothesis, which is 0.5.

```{r}
mean(X_1) - 0.5
```

Meanwhile, the *estimated standard error* is the sample standard deviation (`sd(X_1)`) divided by the square root of the sample size (`length(X_1)`).

```{r}
sd(X_1) / sqrt(length(X_1))
```

Taking the ratio of the above two quantities gives us our $t$ value:

```{r}
t <- (mean(X_1) - 0.5) / (sd(X_1) / sqrt(length(X_1)))
t
```

#### Find the $p$ value

Because this is a *one-sided* test and the alternative hypothesis is that the population mean is *greater* than 0.5, we need to to know the probability of observing a value that is *at least as **large*** as the one we have.  We also need to know the degrees of freedom, which in this case is $5 - 1 = 4$ (`length(X_1) - 1`).

```{r}
1 - pt(q = t, df = length(X_1) - 1)
```

#### Decide whether or not to reject the null hypothesis

Since the $p$ value is *bigger* than our alpha level, we **fail to reject** the null hypothesis.  We have no evidence that people look at the ad more than half the time.

### Paired sample $t$ test

Perhaps spurred by the disappointing results of that one-sample $t$ test, we decide to change the colors in the ad and bring the same sample of 5 people back.  Again, we record the proportion of the time that they spend looking at our new ad, which we label `X_2`:

```{r}
X_2 <- c(0.907, 0.115, 0.548, 0.202, 0.347)
```

Now we have a *pair* of measurements from each person.  So the first entry in `X_1` and the first entry in `X_2` come from the same person, same for the second entry, third entry, etc.  Our **research question** now is, "did changing the ad affect how much people looked at it at all?"

To address this question, we will do a **paired sample $t$ test**.  This involves looking at the *differences* for each person, which we can find in R like so (we will also give them a label `D` to make it easier):

```{r}
D <- X_2 - X_1
```

(*Note that you can do the subtraction in any order, the important thing is to remember which order you picked so you know what the difference means!*)

#### State your hypotheses

Notice that now our question is about any kind of effect, so this will be a **two-tailed** test.  The **null hypothesis** is that the mean difference is zero ($H_0$: $\mu_D = 0$) and the **alternative hypothesis** is that the mean difference is *not* zero ($H_1$: $\mu_D \neq 0$).

#### Set your alpha level

Again, let's keep our alpha level from before and say that our alpha level is 0.1.

#### Find the $t$ value

Remember that the paired sample $t$ test is much like the one-sample $t$ test, only instead of using raw measurements, we use the *differences* which we labeled `D`.

```{r echo=FALSE}
t_D <- mean(D) / (sd(D) / sqrt(length(D)))
t_D
```

```{exercise}
Modify the slice of code we used in the previous section to find the $t$ value given above and make sure that it is labeled `t_D` (the "D" is for "difference").  What code did you write to find this new $t$ value?  *Hint: remember what the mean from the null hypothesis is!*

```

#### Find the $p$ value

Since the $t$ value is positive, this means it is on the upper end of the $t$ distribution.  To find the probability of seeing a value that is at least as high as the one we saw, we can use

```{r}
1 - pt(q = t_D, df = length(D) - 1)
```

But remember that we have to multiply that by `2` since this is a *two-tailed test*.  So this is our $p$ value:

```{r}
2 * (1 - pt(q = t_D, df = length(D) - 1))
```

#### Decide whether or not to reject the null hypothesis

Since the $p$ value is *smaller* than our alpha level, we **reject** the null hypothesis.  We have reason to believe that changing the ad really did affect each individual's viewing patterns.

```{exercise}
Although the test we just did was only about differences in *any* direction, based on the $t$ value does it seem like changing the ad increased or decreased the proportion of time people spent looking at the ad?

```

### Independent samples $t$ test

Let's imagine that, instead of bringing back the same sample of 5 people to view our new ad, we brought in a different sample.  While we're imagining, let's say that this new sample still produce the *same* measurements.  So our "data" is the same, only now it is not "paired", it comes from two *independent samples*.

Our **research question** remains the same: "did changing the ad affect how much people looked at it at all?"  But now, to address this question, we will do an **independent samples $t$ test**.

#### State your hypotheses

Because we have two samples from (potentially) two populations, our null and alternative hypotheses are about a difference in population parameters.  This is still a **two-tailed** test.  The **null hypothesis** is that the mean difference is zero ($H_0$: $\mu_2 - \mu_1 = 0$) and the **alternative hypothesis** is that the mean difference is *not* zero ($H_1$: $\mu_2 - \mu_1 \neq 0$).

Notice that we are keeping the order of the subtraction (2 minus 1) the same as above, to make it easier to compare.

#### Set your alpha level

Again, let's keep our alpha level from before and say that our alpha level is 0.1.

#### Find the $t$ value

To find the $t$ value, remember that we first have to find the **pooled sample standard deviation**.  In mathematical terms, this is $\hat{\sigma}_P = \sqrt{\frac{df_1 \hat{\sigma}^2_1 + df_2 \hat{\sigma}^2_2}{df_1 + df_2}}$.  We can write that out in R like so:

```{r}
df_1 <- length(X_1) - 1
df_2 <- length(X_2) - 1

sd_pooled <- sqrt((df_1 * sd(X_1)^2 + df_2 * sd(X_2)^2) / (df_1 + df_2))
sd_pooled
```

With our pooled SD in hand, we can now find the $t$ value

```{r}
t_Indep <- (mean(X_2) - mean(X_1)) / (sd_pooled * sqrt(1 / length(X_1) + 1 / length(X_2)))
t_Indep
```

#### Find the $p$ value

Again, since the $t$ value is positive, this means it is on the upper end of the $t$ distribution.  To find the probability of seeing a value that is at least as high as the one we saw, we can use

```{r}
1 - pt(q = t_Indep, df = length(X_1) + length(X_2) - 2)
```

Note that we have a different number of **degrees of freedom** here because we are treating these data as being from two independent samples.  But again we have to multiply the probability above by `2` since this is a *two-tailed test*.  So this is our final $p$ value:

```{r}
2 * (1 - pt(q = t_Indep, df = length(X_1) + length(X_2) - 2))
```

#### Decide whether or not to reject the null hypothesis

This time, the $p$ value is *bigger* than our alpha level, so we **fail to reject** the null hypothesis.  Funny---the same data lead us to different conclusions depending on whether it came from just one sample versus two independent samples!

```{exercise}
Why do you think we were able to reject the null hypothesis using the paired samples $t$ test, but not with the independent samples $t$ test, even though the actual data were the same in both cases?  Think about the fact that there is variability both between different individuals and within a single individual (e.g., if you do something multiple times, chances are you won't act exactly the same each time).  Does one type of test effectively eliminate one of these kinds of variability?  How might that affect the ability to the test to detect particular kinds of differences?

```

## $t$ tests with real data

We've now seen how to do all the calculations for a $t$ test in R.  Because R was built for statistics, you won't be surprised that it can automate many of those things for us, as we'll see.  As usual, though, when a computer does the "mindless" work, we have to be even more "mindful" of how to interpret the results it gives us.

In this example, we will look at a study by Sam Mehr, Lee Ann Song (aptly named), and Liz Spelke [@MehrEtAl2016].  They wanted to see whether music is an important social cue for infants.  Specifically, they wondered whether infants tend to prefer people who sing familiar songs.  The idea is that someone singing a song you know is probably a member of the same social group as you are.

They conducted an experiment in which a child's parents were taught a new melody and were instructed to sing it to their child at home over the course of 1--2 weeks.  After this exposure period, the parents brought their infant back to the lab.  The infant was seated in front of a screen showing videos of two unfamiliar adults just smiling in silence.  They recorded the proportion of the time that the infant looked at each individual during this "before" phase.  Then, one of these unfamiliar people sang the melody that the parents had been singing for 1--2 weeks, while the other sang a totally new song.  Finally, during the "after" phase, the infant saw the same videos of each person silently smiling and the researchers recorded the proportion of the time spent looking at the person who sang the familiar song.

The overall aim of this experiment is to see whether infants will prefer to look at the singer of the familiar melody after hearing them sing it, even though they've never seen this person before.

### Check out the data

First, let's get the data into R:

```{r}
lullaby <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/lullaby_wide.csv")
```

Have a look at the data by clicking on `lullaby` in RStudio's "Environment" pane.  Each row is data from a specific infant.  There are three variables in this dataset:

* "id": Identifies each infant in the study.
* "Before": In the phase before the infants heard anyone sing, what proportion of the time did they look at the person who would eventually sing the familiar melody?
* "After": In the phase after the infants heard the two people sing, what proportion of the time did they look at the person who sang the familiar melody?

Let's summarize the looking preferences for infants before and after hearing the people sing:

```{r}
lullaby %>%
    summarize(M_before = mean(Before), S_before = sd(Before), M_after = mean(After), S_after = sd(After))
```

```{exercise}
Based on the sample means and standard deviations we just found, does it seem like there might be a preference to look more at the person who would sing the familiar melody, either before or after hearing them sing?

```

### Did infants show any prior bias?

Before we can ask whether infants prefer to look at someone who sings a familiar melody, we need to see whether they show any kind of *bias* to look at one person or another *before* hearing them sing.  If they did show a bias to look at one of the individuals before hearing them, then we wouldn't be able to attribute their behavior to the familiarity of the song.

Let's first inspect the data by making a histogram:

```{r}
lullaby %>%
    ggplot(aes(x = Before)) +
    geom_histogram(binwidth=0.1)
```

```{exercise}
Based on the histogram above, does it seem like there are any outliers or skew that are very far from a normal distribution?

```

Our **research question** is, "do infants prefer to look at one person over the other, even before hearing them sing?"  We will address this question using a **one-sample $t$ test**.

#### State your hypotheses

An unbiased infant would look equally often at both people, so the proportion of the time they looked at the singer of the familiar song would be 0.5.  This is our **null hypothesis**: $H_0$: $\mu = 0$.  So our **alternative hypothesis** is just that $H_1$: $\mu \neq 0$.

#### Set your alpha level

Let's choose an alpha level of 0.05, same as the one @MehrEtAl2016 chose.

#### Find the $t$ value

R's `infer` package makes it easy to get our $t$ value.  Let's first look at the code for doing this, then explain line-by-line what it means:

```{r}
lullaby %>%
    specify(response = Before) %>%
    hypothesize(null = 'point', mu = 0.5) %>%
    calculate(stat = 't')
```

The first line, like usual, tells R what data to work with.  In the second line, we `specify` that the variable we want to use for our test is the `Before` variable.  The variable we are interested in is called a `response` variable.  In the third line, we tell R that our null hypothesis is a "point" (that is, a single value) at `mu = 0.5`.  Finally, the fourth line tells R to `calculate` the `t` `stat`istic based on what we told it in the first 3 lines.

Let's tell R to remember that $t$ value.  We'll call it `t_before`, since it refers to the looking *before* the infants heard anything.

```{r}
t_before <- lullaby %>%
    specify(response = Before) %>%
    hypothesize(null = 'point', mu = 0.5) %>%
    calculate(stat = 't')
```

#### Find the $p$ value

We can do something fancy now and `visualize` the appropriate $t$ distribution corresponding to our null hypothesis:

```{r}
t_before %>%
    visualize(method = 'theoretical')
```

The first line tells R to use the $t$ value we just found as "data".  The second line instructs R to visualize the `theoretical` $t$ distribution if the null hypothesis were true.  This distribution is what will let us calculate the $p$ value.

In fact, we can visualize what the $p$ value will be by showing where the $t$ statistic for our sample falls on the $t$ distribution:

```{r}
t_before %>%
    visualize(method = 'theoretical') +
    shade_p_value(obs_stat = t_before, direction = 'two-sided')
```

All we did was add a third line where we told R to `shade` the area of the $t$ distribution that is *at least as extreme* as our sample (that's what `obs_stat = t_before` did).  What counts as "extreme" depends on whether our test is one- or two-sided, so we had to tell R that the `direction` of our test was `two-sided`.  The *area* of the shaded region on the plot above is what our $p$ value is.

But of course, we can't get a number just by looking at a graph.  Here's how we actually get all the numbers we need out of R:

```{r}
lullaby %>%
    t_test(response = Before,
           alternative = 'two-sided',
           mu = 0.5,
           conf_level = 0.95)
```

The first line, as usual, tells R what data to use.  The remaining lines tell R all about the `t_test` we want to perform.  We tell R that the `response` variable (the one we want to test) is `Preference`; that we are doing a test with an `alternative` hypothesis that is `two-sided`; that the mean according to the null hypothesis is `mu = 0.5`; and that our confidence level (`conf_level`) is 0.95, which is one minus our alpha level.

The output we got from R told us a lot:

* `statistic`: The value of the $t$ statistic.
* `t_df`: The number of degrees of freedom.
* `p_value`: The $p$ value.
* `alternative`: Reminding us that we were doing a two-sided test.
* `lower_ci`: The lower limit of the *updated* confidence interval (assuming the population mean is equal to the sample mean).  The width of the interval is defined by the `conf_level` we told R already.
* `upper_ci`: The upper limit of the *updated* confidence interval (assuming the population mean is equal to the sample mean).

#### Decide whether or not to reject the null hypothesis

Finally, we can use the $p$ value we just found, in combination with the alpha level from above (which we decided was 0.05) to say that we **fail to reject** the null hypothesis.  As a result, we have no reason to believe that there is any bias in who the infants look at before hearing them sing.

### Do infants prefer to look at the singer of the familiar melody?

What about where infants look *after* they hear the two people sing?  Do they show any kind of preference?  Again, we can use a **one-sample $t$ test** to find out.  In fact, all we need to do is reuse different bits of code from the previous section, just changing the variable of interest from "Before" to "After".

Let's begin by making a histogram of the `After` variable.

```{r, echo=FALSE}
lullaby %>%
    ggplot(aes(x = After)) +
    geom_histogram(binwidth=0.1)
```

```{exercise}
What code did you use to make the histogram above?  Do you see any evidence of outliers or skew that are very different from what we might see in a normal distribution?

```

Our **research question** is, "do infants prefer to look at one person over the other *after* hearing them sing?"

#### State your hypotheses

Again, an infant without any preference would look equally often at both people, so the proportion of the time they looked at the singer of the familiar song would be 0.5.  This is, again, our **null hypothesis**: $H_0$: $\mu = 0$.  Again, our **alternative hypothesis** is just that $H_1$: $\mu \neq 0$.

#### Set your alpha level

Let's keep our alpha level of 0.05.

#### Find the $t$ value

Let's tell R to remember our *new* $t$ value so that we can visualize it like we did before.  We're going to call this new $t$ value `t_after` to keep it distinct.

```{r, echo=FALSE}
t_after <- lullaby %>%
    specify(response = After) %>%
    hypothesize(null = 'point', mu = 0.5) %>%
    calculate(stat = 't')
```

```{exercise}
What code did you use to make `t_after`?  (*Hint: what do you change in the code we used above to make `t_before`?*)

```

#### Find the $p$ value

Let's visualize our $t$ statistic to get a sense of what the $p$ value will be

```{r}
t_after %>%
    visualize(method = 'theoretical') +
    shade_p_value(obs_stat = t_after, direction = 'two-sided')
```

Finally, let's get the whole shebang:

```{r echo=FALSE}
lullaby %>%
    t_test(response = After,
           alternative = 'two-sided',
           mu = 0.5,
           conf_level = 0.95)
```

```{exercise}
What code did you use to get the `t_test` output above?

```

#### Decide whether or not to reject the null hypothesis

Comparing the $p$ value we just found to our alpha level of 0.05, we **reject** the null hypothesis.  We have reason to believe that the infants have a *preference* to look at one singer over the other.

### Did their preferences change?

Based on what we've done so far, it seems like before hearing the two people sing, the infants didn't look at either one more than the other, but *after* hearing them sing, the infants did show a preference.  Now let's ask a more direct **research question**:  For each infant, did hearing someone sing a familiar song lead that infant to prefer looking at that person?  This question can be addressed using a **paired samples $t$ test**.

#### State your hypotheses

Based on the way our research question is phrased, we are looking at a **one-tailed test** because we are asking about a difference in a specific direction---an *increase* in preferential looking.  So our **null hypothesis** is that the difference in preference is less than or equal to zero ($H_0$: $\mu_D \leq 0$) while our **alternative hypothesis** is that the difference is greater than zero ($H_0$: $\mu_D > 0$).

#### Set your alpha level

Let's stay consistent and say that our **alpha level** is still 0.05.

#### Find the $t$ value

Like we did above, we need to first find the *difference* for each infant between their looking preferences before and after hearing the people sing.  We can do that in R using the `mutate` function, which we've previously used to calculate probabilities based on relative frequency.

The code below creates a new variable called `Difference` by subtracting the `Before` preference from the `After` preference:

```{r}
lullaby %>%
    mutate(Difference = After - Before)
```

We can insert that second line of code above in order to calculate the $t$ statistic corresponding to our difference scores:

```{r}
t_diff <- lullaby %>%
    mutate(Difference = After - Before) %>%
    specify(response = Difference) %>%
    hypothesize(null = 'point', mu = 0) %>%
    calculate(stat = 't')
```

```{exercise}
Compare the code used to make `t_diff` with the code we previously used to make `t_before` and `t_after`.  What is similar and what is different?

```

#### Find the $p$ value

Now that we have found `t_diff`, let's see where our sample falls on the $t$ distribution corresponding to the null hypothesis:

```{r}
t_diff %>%
    visualize(method = 'theoretical') +
    shade_p_value(obs_stat = t_diff, direction = 'greater')
```

```{exercise}
Compare the visualization of the $t$ distribution for `t_diff` to the ones we made for `t_before` and `t_after`.  Is there a difference in which sides of the distribution are shaded?  Why might this be?

```

Finally, we can obtain the $p$ value along with the rest of the results of the $t$ test:

```{r echo = FALSE}
lullaby %>%
    mutate(Difference = After - Before) %>%
    t_test(response = Difference,
           alternative = 'greater',
           mu = 0,
           conf_level = 0.95)
```

```{exercise}
How did you modify the code we used above for the previous two $t$ tests to get the result above?  Do you notice anything different about the output we got from R for this one-tailed test, relative to the two-tailed tests we did above?

```

#### Decide whether or not to reject the null hypothesis

Comparing the $p$ value we just found to our alpha level of 0.05, we **reject** the null hypothesis.  We have reason to believe that infants' preferences *changed* after hearing the two people sing, such that they look more often at the singer of the familiar melody.

## Wrap-up

In this session, we saw both how to conduct all the individual calculations in a $t$ test as well as how to use R to do a $t$ test "all at once".  We saw how to do one-sample, paired samples, and independent samples $t$ tests in step-by-step form.  We also saw how to do one-sample and paired samples $t$ tests in a fancier form with helpful visual aids that are better suited for dealing with real data.  We can also do independent samples $t$ tests in this fancier way, but we will have to wait until next week to see a demonstration.  Nonetheless, it is clear that R has powerful facilities for doing the kinds of hypothesis tests that are the bread and butter of inferential statistics.

<!--chapter:end:09-ttests.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Comparing Independent Samples {#lab10}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
# knitr::include_graphics("img/lullaby.png")
```

> Put umpteen people in two groups at random.<br>
> Social dynamics make changes in tandem:<br>
> Members within groups will quickly conform;<br>
> Difference between groups will soon be the norm.
>
> --- John Kruschke [@Kruschke2015]

In this session, we will see how to compare multiple independent samples in R.  Last time, we got a sense of how to compare two independent samples by using a $t$ test to test the null hypothesis that two samples come from populations with the same mean.  We will see a bit more of that in this session, but of course $t$ tests are limited to comparing just two samples at a time.  **Analysis of Variance (ANOVA)**, as we've seen, let's us compare multiple samples all at once.

In this session, we will cover

1. Doing independent samples $t$ tests in R.
2. Doing ANOVA in R.
3. Checking the assumptions of these tests.
4. Some neat tricks for manipulating data in R.
    + Using `mutate` to make new variables out of ones we already have.
    + Using `filter` to look at "subsets" of a whole dataset based on specific criteria.

Before we begin, let's make sure we get our hands on the `tidyverse` package, as well as the `infer` package we used last time.  We will also need the `magrittr` package.

```{r}
library(tidyverse)
library(infer)
library(magrittr)
```

## Get to know the data

The data for today's session harkens back to where many of us (at least who grew up in the United States) first encountered the idea of "statistics": baseball.  These are batting records for Major League players in the 2018 season:

```{r}
batting <- read_csv("data/batting2018.csv")
```

Click on the data in R's Environment pane to get a look.  Each row is a different player and each column tells us some useful information.  

* **Name**: The player's name.
* **Team**: The team for which the player played.
* **Group**: Depends on primary position played, groups them into infield, outfield, battery, and designated hitter.
* **Position**: The primary position played.  These are abbreviated:
    + 1B: First base
    + 2B: Second base
    + 3B: Third base
    + C: Catcher
    + CF: Center field
    + DH: Designated hitter
    + LF: Left field
    + P: Pitcher
    + RF: Right field
    + SS: Shortstop
* **Games_Played**: The number of games played in the 2018 regular seaosn.
* **AB**: Number of "at bats".
* **R**: Number of runs scored.
* **H**: Number of hits while at bat.
* etc.

To get a sense of where the different positions in baseball are, take a look at this picture:

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/baseball_positions.png")
```

### Batting average

A player's "batting average" is the *proportion* of times they made a hit while at bat.  For example, if a player had 400 at bats that season and had a hit 100 of those times, their batting average would be $100 / 400 = 0.25$.  Note that it is conventional in baseball stats to multiply that average by 1000 to get a number between 0 and 1000 instead of one between 0 and 1, but we'll just keep things simple and stick with using a standard proportion between 0 and 1.

If we were interested in studying the batting performance of just one player, we could use our old friend the *binomial* distribution.  After all, we have a number of repeated events (at-bats) each of which can have an outcome of interest (a hit).  But we are interested today in comparing different players, not studying the performance of a specific one.

In particular, we'd like to find the batting averages for every player in our dataset.  We have the relevant numbers already in our dataset in the H and AB columns.  Specifically, the batting average for each player is `H / AB`, the number of hits made divided by the number of opportunities (at-bats).  While we could do this by hand for each player, that would be pretty annoying.  Instead, we can use R to do that work using `mutate`:

```{r }
batting %>%
    mutate(AVG = H / AB)
```

As usual, the first line told R what data we were working with (`batting`).  The second line `mutate`d two of the variables `H` and `AB` into `AVG`, each player's batting average.  Let's spend a bit of time on that `mutate` line, because it is extremely useful.

#### Making new variables with `mutate`

First, note that "AVG" is just the name that we decided.  We could replace "AVG" with a more descriptive name like "Batting_Average":

```{r}
batting %>%
    mutate(Batting_Average = H / AB)
```

Now the new column is called "Batting_Average" instead of "AVG".

We can use this to make some more variables as well.  Try using `mutate` to create a new variable called `Total_Bases` which will count the total number of bases earned by a player.  This number is equal to `H + Doubles + 2 * Triples + 3 * HR`.  After filling in the blank below, the result should look like the following:

```{r eval = FALSE}
batting %>%
    mutate(Total_Bases = ___)
```

```{r echo = FALSE}
batting %>%
    mutate(Total_Bases = H + Doubles + 2 * Triples + 3 * HR)
```

```{exercise}
What code did you write to use `mutate` to create the new `Total_Bases` variable?

```

#### Updating our data with new variables

So far, we've left our original `batting` dataset untouched because we haven't told R to remember the dataset with the new variables we've created.  Since we'll be using the batting average later, let's tell R to replace our original `batting` data with the updated version including the `AVG` column:

```{r}
batting <- batting %>%
    mutate(AVG = H / AB)
```

By adding `batting <-` to the beginning, we've told R to remember our updated data (with the new `AVG` column) under the same name (`batting`) as our original data.

### Batting average by position

Now let's see whether the batting average looks much different between different positions.  For example, we'd expect that pitchers would, in general, have pretty low batting averages because that's not their job.

To get a sense of the differences, we will make a set of histograms:

```{r}
batting %>%
    ggplot(aes(x = AVG)) +
    geom_histogram(binwidth = 0.05) +
    facet_wrap("Position")
```

That's pretty hard to interpret!  Let's add an option to the last line (`facet_wrap`) to help us out.  This option is `scales = "free_y"`, meaning that the `scale` of the vertical axes (the `y` axis) on each histogram are allowed to be different (they are "`free`"):

```{r}
batting %>%
    ggplot(aes(x = AVG)) +
    geom_histogram(binwidth = 0.05) +
    facet_wrap("Position", scales = "free_y")
```

This is a bit easier to see, and we can immediately tell there are some differences between different positions.

```{exercise}
Why does the histogram for `DH` (Designated Hitter) look so different from the others? (Hint: look at the numbers on the vertical axis of the `DH` histogram.)  What seems to be different about the histogram for `P` (Pitchers)?

```

To get a sense of what might be causing the unusual shape of the histogram for pitchers, let's construct another histogram that looks at `AB`, the number of at-bats.

```{r eval = FALSE}
batting %>%
    ggplot(aes(x = ___)) +
    geom_histogram(binwidth = ___) +
    facet_wrap("Position", scales = "free_y")
```

```{r echo = FALSE}
batting %>%
    ggplot(aes(x = AB)) +
    geom_histogram(binwidth=25) +
    facet_wrap("Position", scales = "free_y")
```

```{exercise}
What code did you use to make histograms for at-bats (`AB`)?  What did you pick for the `binwidth` and why?

```

Based on these new histograms, it looks like Pitchers don't have nearly as many at-bats as players in other positions, so we don't have as many opportunities to see their batting ability.  This suggests that we can't think about pitchers the same way as we would other positions.

## Independent samples $t$ test: Infield vs. Outfield

We will first use an independent samples $t$ test to compare batting averages between infielders and outfielders.  An infielder is a player who plays a position within the bases (1B, 2B, 3B, and SS) while an outfielder is a player who plays a position further away (LF, CF, RF).  Designated hitters (DH) do not play on the field at all, and pitchers and catchers are considered part of a different category (the "battery").  These groupings are given by the **Group** variable in our dataset.

To compare just infielders to outfielders we need to **filter** our data so that it only contains players from those two groups.

### Filtering data with `filter`

R makes it easy to filter out data we don't need from a particular analysis.  The code for this is, helpfully, called `filter`.  Let's see it in operation:

```{r}
batting %>%
    filter(Group != "Battery")
```

The second line `filters` the data according to the criteria in the parentheses.  The `!=` symbol means "not equal", so `Group != "Battery"` can be read as "group is not equal to battery (pitcher or catcher)".  The result of the filtering operation is a subset of the original data that excludes players with battery positions, i.e., pitchers and catchers.

We could also filter out designated hitters using a similar chunk of code.

```{r eval=FALSE}
batting %>%
    filter(___)
```

```{r echo=FALSE}
batting %>%
    filter(Group != "DH")
```

```{exercise}
What code would filter out designated hitters?  *Hint: what is the abbreviation for designated hitters?*

```

Finally, we can combine filters just by listing them.  The following bit of code filters out both pitchers and catchers, so the resulting data includes only infield and outfield players.

```{r}
batting %>%
    filter(Group != "Battery", Group != "DH")
```

### Doing the $t$ test

Last time, we saw how to carry out the individual components of the computation involved in an independent samples $t$ test, but today we will see how to do it in a more streamlined form similar to how we did it with one-sample and paired samples $t$ tests last time.

First, let's make ourselves more histograms to get a sense of whether there is anything we should be concerned about in our data

```{r}
batting %>%
    filter(Group != "Battery", Group != "DH") %>%
    ggplot(aes(x = AVG)) +
    geom_histogram(binwidth=0.05) +
    facet_wrap("Group", scales = "free_y")
```

Nothing too wonky, so we should be good to go to conduct our $t$ test.

#### State your hypotheses

Our **research question** is, "is there a difference in batting averages between infield and outfield players?"  Because our research question is about *any* kind of difference, we will be doing a **two-sided** test.  Specifically, our **null hypothesis** is that there is no difference in the population mean batting average for infielders and outfielders ($H_0$: $\mu_1 - \mu_2 = 0$) while our **alternative hypothesis** is that there is some difference ($H_1$: $\mu_1 - \mu_2 \neq 0$).

#### Set your alpha level

Let's choose an **alpha level of 0.05**.

#### Find the $t$ value

We will use the `infer` package to visualize the $t$ test like last time.  First, we need to get our $t$ value:

```{r}
t_value <- batting %>%
    filter(Group != "Battery", Group != "DH") %>%
    specify(AVG ~ Group) %>%
    calculate(stat = 't')
```

We told R to remember the result under the label `t_value`.  Aside from that, the first two lines are the same as above and tell R what data we are working with; it takes two lines because the second line is the `filter`ing operation.  The third line looks interesting:  Remember that last time we used `specify` to tell R what our "response" variable was.  Now, we have both a response variable and an "explanatory" variable, `Group`.  By "explanatory", we mean that the `Group` variable is what divides the response variable into two independent groups, and we are seeing if that variable *explains* differences between those groups.  `AVG ~ Group` says that the variable on the left of the `~` (`AVG`) is the **response** variable (the one we want to analyze) and the variable on the right (`Group`) is what we are testing to see if it **explains** differences in our response variable.  Finally, the last line just tells R to calculate the $t$ statistic.

#### Find the $p$ value

We can now visualize where our sample falls on the appropriate $t$ distribution:

```{r}
batting %>%
    filter(Group != "Battery", Group != "DH") %>%
    specify(AVG ~ Group) %>%
    hypothesize(null = 'point', mu = 0) %>%
    visualize(method = 'theoretical') +
    shade_p_value(obs_stat = t_value, direction = 'two-sided')
```

```{exercise}
Based on the plot above and the amount of the $t$ distribution that is shaded in, do you think our $p$ value will be relatively high or relatively low?
    
```

Finally, let's get our full $t$ test results:

```{r}
batting %>%
    filter(Group != "Battery", Group != "DH") %>%
    t_test(AVG ~ Group,
           alternative = 'two-sided',
           var.equal = TRUE,
           mu = 0,
           conf_level = 0.95)
```

As above, the first two lines tell R what data to use.  The remaining lines tell R all about the `t_test` we want to perform.  First, we use `AVG ~ Group` to say that we are testing a null hypothesis about how `AVG` differs between different values of `Group`.  We are doing a test with an `alternative` hypothesis that is `two-sided`; that the mean difference according to the null hypothesis is `mu = 0`; and that our confidence level (`conf_level`) is 0.95, which is one minus our alpha level.  The `var.equal = TRUE` line tells R that we are assuming the population `var`iances are `equal` between the two groups, so we can use the pooled sample standard deviation.  If we had reason to believe the variances were not equal between groups, we could say `var.equal = FALSE` instead, like so:

```{r}
batting %>%
    filter(Group != "Battery", Group != "DH") %>%
    t_test(AVG ~ Group,
           alternative = 'two-sided',
           var.equal = FALSE,
           mu = 0,
           conf_level = 0.95)
```

We won't get into the mathematics for this, but just point out that it is an option.  For our purposes, we will always use `var.equal = TRUE`.

#### Decide whether or not to reject the null hypothesis

Based on the $p$ value and our alpha level above, we **fail to reject** the null hypothesis.  We have no reason to believe that infielders and outfielders have different batting averages.

## Analysis of Variance: Batting average by position

Maybe infielders and outfielders as two big groups don't differ in their batting average.  But what if we took a closer look and compared by specific positions?  And what about the positions that we didn't include before, pitchers, catchers, and designated hitters?  To compare multiple independent samples, we will use **Analysis of Variance (ANOVA)**.

### Doing the ANOVA

As we've seen, the steps of ANOVA are broadly similar to any other hypothesis test.

#### State your hypotheses

Our **research question** is, "is there a difference in batting average between positions?"  Our **null hypothesis** is that the population mean batting average is the same for all positions ($H_0$: $\mu_{1B} = \mu_{2B} = \cdots = \mu_{SS}$).  Our **alternative hypothesis** is that there is at least one position with a different population mean batting average from the others.

#### Set your alpha level

Again, let's keep our **alpha level of 0.05**.

#### Find the $F$ value

Similar to above, we can use R not only to find the $F$ value for our data, but to visualize where it falls on the distribution of $F$ values that we would observe if the null hypothesis were true.

```{r}
F_value <- batting %>%
    specify(AVG ~ Position) %>%
    calculate(stat = "F")
```

The block of code above calculated the $F$ value and told R to remember it under the label `F_value`.  As usual, we began by telling R what data to work with (`batting`), then `specify` that our response variable is `AVG` and our explanatory variable is `Position`.  Again, by "explanatory", we mean that we are testing to see how well differences in the response variable can be "explained by" the fact that they came from different groups defined by the explanatory variable (`Position`).  Finally, we get the $F$ value.

```{exercise}
Compare the code we just used to find `F_value` with the code we used above to find the `t_value` (comparing infielders and outfielders).  What is similar and what is different?  Is there a difference in how we told R what data to use?  Is there a difference in how we "specified" the response variable and explanatory variable?  Is there a difference in what we told R to "calculate"?
    
```

Let's check out that $F$ value now

```{r}
F_value
```

#### Find the $p$ value

The following code let's us visualize the $F$ value for our data relative to the distribution of $F$ values that we would expect to see if the null hypothesis were true:

```{r}
batting %>%
  specify(AVG ~ Position) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") +
  shade_p_value(F_value, direction = "greater")
```

It's pretty clear that our $F$ value is way out there!

But let's finally get our ANOVA table so we can see the $p$ value:

```{r}
batting %$%
    lm(AVG ~ Position) %>%
    anova()
```

The block of code above is how to do ANOVA in R.  As usual, we first tell R what data to work with (`batting`).  In the second line, we again define our response variable (`AVG`) and explanatory variable (`Position`) using the `~`.  Finally, the last line tells R to do an `anova`.

There are two things to note:  First, the connection between the first and second lines uses the `$%$` symbol.  The reason for this is not so critical in this class, but this is something to check if you ever encounter a problem.  Second, the second line begins with `lm`, rather than `specify`.  As we will see, `lm` is an important function in R that is used for other things too.

Anyway, the output we get from R is a familiar ANOVA table and we can read from it that our $p$ value is extremely low, essentially equal to 0.  In other words, if the null hypothesis were true, a result like ours would be exceedingly unlikely.

#### Decide whether or not to reject the null hypothesis

Based on the fact that the $p$ value is much less than our alpha level, we **reject** the null hypothesis.  We have reason to believe that batting averages do differ between different positions.

### Post hoc pairwise tests

Since we rejected the null hypothesis at the end of our ANOVA, let's use post hoc pairwise $t$ tests to see which specific pairs of positions are likely to differ from one another.  We can get some sense of this by looking at the sample mean batting averages for each position:

```{r}
batting %>%
    group_by(Position) %>%
    summarize(M = mean(AVG))
```

Hmm, it certainly looks like there's one particularly low mean relative to the others.  But to know whether we have statistical evidence for these differences, we use the following code in R to conduct post-hoc pairwise $t$ tests:

```{r}
batting %$%
    pairwise.t.test(
        x = AVG,
        g = Position,
        p.adjust.method = 'bonferroni'
    )
```

Notice that, again, we connected the first and second lines with `%$%`.  By telling R `x = AVG`, we were saying that our response variable is `AVG`.  By telling R `g = Position`, we were saying that the explanatory variable is `Position`.  Finally, we told R to adjust the $p$ values for each of our pairwise $t$ tests using the `bonferroni` correction we discussed in class.

The result is a big matrix of $p$ values.  To see which pairs of groups are statistically significantly different from one another, we compare each $p$ value against our alpha level (0.05); as usual, if the $p$ value in the matrix is less than our alpha level, we can reject the null hypothesis that there is no difference on average.

```{exercise}
Based on the $p$ values in the table above, for which pairs of positions would we reject the null hypothesis?  Does this outcome make sense based on the sample means we found above, and the histograms we made at the beginning?

```

### No Pitchers Allowed?

At the beginning of the session, we saw that the batting averages for pitchers seemed odd relative to other positions, largely because they are not at bat as often.  And we saw with our post hoc pairwise tests that all the statistically significant pairwise differences between positions involved pitchers.  If we excluded pitchers from the set of groups being compared, would we still have evidence that positions differ in batting average?

Let's see!  You will need to add a line to our ANOVA code that filters out pitchers.  You'll need to fill in the blank part of the following slice of code:

```{r eval = FALSE}
batting %>%
    filter(___) %$%
    lm(AVG ~ Position) %>%
    anova()
```

The result of running your new ANOVA will be the following:

```{r echo = FALSE}
batting %>%
    filter(Position != 'P') %$%
    lm(AVG ~ Position) %>%
    anova()
```

```{exercise}
What code did you write to do an ANOVA with `AVG` as the response variable and `Position` as the explanatory variable, but first using a `filter` to exclude pitchers?  *Hint: Look at how we filtered out specific groups of positions earlier in the session, and remember the abbreviation for "Pitcher".*
    
```

### Slugging Average

It looks like, if pitchers are excluded, we no longer have any evidence that positions differ in their batting averages, since we would fail to reject the null hypothesis at our alpha level of 0.05.

But maybe we would find evidence for a difference in some other measure of batting performance?  Let's use `mutate` to once again create a new variable for us to analyze, this time the "slugging average".  The slugging average is the average number of bases made on each at-bat.  The slugging average is therefore a more sensitive measure of batting ability than just the batting average, because it takes into account how *good* a hit was, not just whether it was made.

The slugging average is the total number of bases won (which we found above) divided by the number of at-bats.

```{r}
batting <- batting %>%
    mutate(SLG = (H + Doubles + 2 * Triples + 3 * HR) / AB)
```

#### Inspecting the data

As before, let's construct a set of histograms so we can get a sense of whether there are any outliers or if any of the groups look like they might be very different from the others.  We will modify the code we used above for `AVG`, so you'll need to fill in the blanks to get something like the result below (play around with different `binwidth`s until you find one that seems good):

```{r eval = FALSE}
batting %>%
    ggplot(aes(x = ___)) +
    geom_histogram(binwidth = ___) +
    facet_wrap("Position", scales = "free_y")
```

```{r echo = FALSE}
batting %>%
    ggplot(aes(x = SLG)) +
    geom_histogram(binwidth=0.1) +
    facet_wrap("Position", scales = "free_y")
```

```{exercise}
What code did you write to make something like the set of histograms above?  What did you choose for your `binwidth`?

```

Again, there might be something weird about pitchers, so we'll try the ANOVA both with and without including them.

#### ANOVA including pitchers

First, let's run the ANOVA including all positions, including pitchers.  Remember that we are now using slugging average (`SLG`) as the response variable.  Fill in the blank to get the result below.

```{r eval = FALSE}
batting %$%
    lm(___ ~ Position) %>%
    anova()
```

```{r echo = FALSE}
batting %$%
    lm(SLG ~ Position) %>%
    anova()
```

Again, an extremely unlikely result if the null hypothesis were true!  As a result, we reject the null hypothesis and can move on to conducting post hoc pairwise tests to see which groups differ from one another.  Again, fill in the blank to get the result below:

```{r eval = FALSE}
batting %$%
    pairwise.t.test(
        x = ___,
        g = Position,
        p.adjust.method = 'bonferroni'
    )
```

```{r echo = FALSE}
batting %$%
    pairwise.t.test(
        x = SLG,
        g = Position,
        p.adjust.method = 'bonferroni'
    )
```

```{exercise}
What did you put in the blank to get the correct pairwise $t$ test result?  This time, do you see any $p$ values less than our alpha level (0.05) that do *not* involve pitchers?  What pairs of positions have differences in slugging average that are statistically significant?

```

#### ANOVA excluding pitchers

Just to be on the safe side, let's see how that ANOVA would turn out if we had excluded pitchers.  Now we need to insert a `filter` line.  Fill in the blanks below to conduct the ANOVA:

```{r eval = FALSE}
batting %>%
    filter(___) %$%
    lm(___ ~ Position) %>%
    anova()
```

```{r echo = FALSE}
batting %>%
    filter(Position != 'P') %$%
    lm(SLG ~ Position) %>%
    anova()
```

```{exercise}
What code did you write to do an ANOVA using `SLG` as the response variable instead of `AVG`?

```

Interesting!  Again, a result that would be very unlikely if the null hypothesis were true, even though we have excluded pitchers from the set of groups being compared.  Time for some post hoc pairwise tests...

```{r eval = FALSE}
batting %>%
    filter(___)%$%
    pairwise.t.test(
        x = ___,
        g = Position,
        p.adjust.method = 'bonferroni'
    )
```

```{r echo = FALSE}
batting %>%
    filter(Position != 'P')%$%
    pairwise.t.test(
        x = SLG,
        g = Position,
        p.adjust.method = 'bonferroni'
    )
```

```{exercise}
What code did you use to do the post hoc pairwise tests above?  Which pairs of positions have different slugging averages using an alpha level of 0.05?  Why are we able to detect differences that we couldn't before when we had included pitchers?  (*Hint: remember that the Bonferroni correction depends on the number of possible pairs.*)

```

## Wrap-up

In this session, we have seen how to compare independent samples in R.  We can compare two independent samples using a **$t$ test**.  We can compare multiple independent samples using **Analysis of Variance (ANOVA)**.  If the result of our ANOVA leads us to reject the null hypothesis, then we can proceed to conduct **post hoc pairwise $t$ tests** using the **Bonferroni correction** to avoid inflating our probability of making a Type I error.

<!--chapter:end:10-indep_groups.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# (APPENDIX) Appendix {-}

# Distributions {#coderef}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

## Binomial distribution

For the following examples, I am using the following labeled values so you can see the kind of result R will give you.

```{r}
this_prob <- 0.3
this_size <- 10

given_value <- 5

upper_value <- 9
lower_value <- 3

given_probability <- 0.2
```

To use these bits of code yourself, just replace the label with the appropriate number or assign a new value under the same label (e.g., `this_prob <- 0.941`).

### Probability of observing an outcome exactly equal to a given value

```{r}
dbinom(x = given_value, prob = this_prob, size = this_size)
```

### Probability of observing a value less than or equal to a given value

```{r}
pbinom(q = given_value, prob = this_prob, size = this_size)
```

### Probability of observing a value greater than a given value

```{r}
1 - pbinom(q = given_value, prob = this_prob, size = this_size)
```

### Probability of observing a value inside an interval between an upper and lower value

```{r}
pbinom(q = upper_value, prob = this_prob, size = this_size) - pbinom(q = lower_value, prob = this_prob, size = this_size)
```

### Probability of observing a value outside an interval between an upper and lower value

```{r}
1 - (pbinom(q = upper_value, prob = this_prob, size = this_size) - pbinom(q = lower_value, prob = this_prob, size = this_size))
```

or

```{r}
1 - pbinom(q = upper_value, prob = this_prob, size = this_size) + pbinom(q = lower_value, prob = this_prob, size = this_size)
```

### The value for which there is a given probability of observing an outcome less than or equal to that value

```{r}
qbinom(p = given_probability, prob = this_prob, size = this_size)
```

### Testing a null hypothesis

The following are important bits of code for conducting null hypothesis tests with the binomial distribution.  For the following examples, I have told R to remember the following labeled values, in addition to the ones above.

```{r}
null_prob <- 0.5

sample_value <- 6

alpha_level <- 0.1
```

#### One-tailed test (alternative is that probability is *less* than null probability)

##### Boundary of confidence interval

```{r}
ci_boundary <- qbinom(p = alpha_level, prob = null_prob, size = this_size)
```

##### $p$ value

```{r}
pbinom(q = sample_value, prob = null_prob, size = this_size)
```

#### One-tailed test (alternative is that probability is *greater* than null probability)

##### Boundary of confidence interval

```{r}
ci_boundary <- qbinom(p = 1 - alpha_level, prob = null_prob, size = this_size)
```

##### $p$ value

```{r}
1 - pbinom(q = sample_value - 1, prob = null_prob, size = this_size)
```

## Normal distribution

For the following examples, I am using the following labeled values so you can see the kind of result R will give you.

```{r}
this_mean <- 0
this_sd <- 1

given_value <- 1.5

upper_value <- 2
lower_value <- -1

given_probability <- 0.2
```

To use these bits of code yourself, just replace the label with the appropriate number or assign a new value under the same label (e.g., `this_mean <- -4`).

### Probability of observing a value less than or equal to a given value

```{r}
pnorm(q = given_value, mean = this_mean, sd = this_sd)
```

### Probability of observing a value greater than a given value

```{r}
1 - pnorm(q = given_value, mean = this_mean, sd = this_sd)
```

### Probability of observing a value inside an interval between an upper and lower value

```{r}
pnorm(q = upper_value, mean = this_mean, sd = this_sd) - pnorm(q = lower_value, mean = this_mean, sd = this_sd)
```

### Probability of observing a value outside an interval between an upper and lower value

```{r}
1 - (pnorm(q = upper_value, mean = this_mean, sd = this_sd) - pnorm(q = lower_value, mean = this_mean, sd = this_sd))
```

or

```{r}
1 - pnorm(q = upper_value, mean = this_mean, sd = this_sd) + pnorm(q = lower_value, mean = this_mean, sd = this_sd)
```

### The value for which there is a given probability of observing an outcome less than or equal to that value

```{r}
qnorm(p = given_probability, mean = this_mean, sd = this_sd)
```

### Testing a null hypothesis

The following are important bits of code for conducting null hypothesis tests with the normal distribution (see [this lab](#lab8) for additional examples).  For the following examples, I have told R to remember the following labeled values.

```{r}
null_mean <- 0
null_sd <- 1

sample_size <- 30

sample_mean <- 0.3

effect_size <- 0.5

alpha_level <- 0.1
```

#### Standard error of the mean

```{r}
null_sd / sqrt(sample_size)
```

#### One-tailed test (alternative is that mean is *less* than null mean)

##### Boundary of confidence interval

```{r}
ci_boundary <- qnorm(p = alpha_level, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

##### $p$ value

```{r}
pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

##### Power

```{r}
pnorm(q = ci_boundary, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

#### One-tailed test (alternative is that mean is *greater* than null mean)

##### Boundary of confidence interval

```{r}
ci_boundary <- qnorm(p = 1 - alpha_level, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

##### $p$ value

```{r}
1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

##### Power

```{r}
1 - pnorm(q = ci_boundary, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

#### Two-tailed test

##### Boundaries of confidence interval

```{r}
ci_lower <- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
ci_upper <- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

##### $p$ value

If `sample_mean > null_mean`:

```{r}
2 * (1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)))
```

If `sample_mean <= null_mean`:

```{r echo=FALSE}
sample_mean <- -abs(sample_mean)
```

```{r}
2 * pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))
```

```{r echo=FALSE}
sample_mean <- abs(sample_mean)
```

##### Power

```{r}
pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) +
    1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size))
```

<!--chapter:end:97-formulas.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
# Hypothesis Tests {#hyptests}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

## $t$ tests

### Doing them step-by-step

#### One-sample $t$ test

For the following example, I've labeled the collection of observed data as `X`.  If you put your own collection of values under the same label, the following code will give you what you need to do a $t$ test on your own data.

```{r}
X <- c(-0.58, 1.02, 1.25, -0.02, 1.15)
```

1. Translate your **research question** into a **null** and **alternative** hypothesis.

I will assume in this example that the population mean if the null hypothesis is true is $\mu = 0$, but you can replace this with your own `null_mean` as you need.

```{r}
null_mean <- 0
```

2. Decide on an **alpha level**.

3. Find the $t$ value ($t = \frac{\bar{X} - \mu}{\hat{\sigma} / \sqrt{N}}$):

```{r}
t <- (mean(X) - null_mean) / (sd(X) / sqrt(length(X)))
```

4. Find the $p$ value.

Direction of test    | Is $t$ value positive or negative? | Code
---------------------|------------------------------------|------------------------------------
One-tailed (less)    | NA                                 | `pt(q = t, df = length(X) - 1)`
One-tailed (greater) | NA                                 | `1 - pt(q = t, df = length(X) - 1)`
Two-tailed           | Positive                           | `2 * (1 - pt(q = t, df = length(X) - 1))`
Two-tailed           | Negative                           | `2 * pt(q = t, df = length(X) - 1)`

5. Decide whether or not to reject the null hypothesis.

If the $p$ value is less than the alpha level, you *reject* the null hypothesis, otherwise you *fail to reject* it.

#### Paired samples $t$ test

For the following example, I've labeled the pairs of observed values as `X_1` and `X_2`.  If you put your own collection of values under the same labels, the following code will give you what you need to do a $t$ test on your own data.

```{r}
X_1 <- c(-0.58, 1.02, 1.25, -0.02, 1.15)
X_2 <- c(-2.05, -0.92, -1.18, -0.16, 0.73)
```

We then need to get the *difference* between each pair of values, like so:

```{r}
D <- X_1 - X_2
```

Remember to keep track of the order in which you did the subtraction!

1. Translate your **research question** into a **null** and **alternative** hypothesis.

I will assume in this example that the population mean difference if the null hypothesis is true is $\mu_D = 0$, since this is the most common use for a paired-sample $t$ test, but you can replace this with your own `null_mean` as you need.

```{r}
null_mean <- 0
```

2. Decide on an **alpha level**.

3. Find the $t$ value ($t = \frac{\bar{D} - \mu_D}{\hat{\sigma}_D / \sqrt{N}}$).

```{r}
t <- (mean(D) - null_mean) / (sd(D) / sqrt(length(D)))
```

4. Find the $p$ value.

Direction of test    | Is $t$ value positive or negative? | Code
---------------------|------------------------------------|------------------------------------
One-tailed (less)    | NA                                 | `pt(q = t, df = length(D) - 1)`
One-tailed (greater) | NA                                 | `1 - pt(q = t, df = length(D) - 1)`
Two-tailed           | Positive                           | `2 * (1 - pt(q = t, df = length(D) - 1))`
Two-tailed           | Negative                           | `2 * pt(q = t, df = length(D) - 1)`

5. Decide whether or not to reject the null hypothesis.

If the $p$ value is less than the alpha level, you *reject* the null hypothesis, otherwise you *fail to reject* it.

#### Independent samples $t$ test

For the following example, I've labeled the pairs of observed values as `X_1` and `X_2`.  Note that they are different from above and that they are not the same size.  If you put your own collection of values under the same labels, the following code will give you what you need to do a $t$ test on your own data.

```{r}
X_1 <- c(-0.87, -0.13, 0.13, -0.14, -0.45, -1)
X_2 <- c(0.06, -0.99, 0.47, 0.86, 0.06)
```

1. Translate your **research question** into a **null** and **alternative** hypothesis.

I will assume in this example that the difference in population means if the null hypothesis is true is $\mu_1 - \mu_2 = 0$, since this is the most common use for a independent samples $t$ test, but you can replace this with your own `null_mean` as you need.

```{r}
null_mean <- 0
```

2. Decide on an **alpha level**.

3. Find the $t$ value.

First, we need to find the pooled sample standard deviation ($\hat{\sigma}_P = \sqrt{\frac{df_1 \hat{\sigma}_1^2 + df_2 \hat{\sigma}_2^2}{df_1 + df_2}}$)

```{r}
df_1 <- length(X_1) - 1
df_2 <- length(X_2) - 1

sd_pooled <- sqrt((df_1 * sd(X_1)^2 + df_2 * sd(X_2)^2) / (df_1 + df_2))
```

Now we can get the $t$ value ($t = \frac{\bar{X}_1 - \bar{X}_2 - (\mu_1 - \mu_2)}{\hat{\sigma}_P \sqrt{\frac{1}{N_1} + \frac{1}{N_2}}}$):

```{r}
t <- (mean(X_1) - mean(X_2) - null_mean) / (sd_pooled * sqrt(1 / length(X_1) + 1 / length(X_2)))
```

4. Find the $p$ value.

Direction of test    | Is $t$ value positive or negative? | Code
---------------------|------------------------------------|------------------------------------
One-tailed (less)    | NA                                 | `pt(q = t, df = df_1 + df_2)`
One-tailed (greater) | NA                                 | `1 - pt(q = t, df = df_1 + df_2)`
Two-tailed           | Positive                           | `2 * (1 - pt(q = t, df = df_1 + df_2))`
Two-tailed           | Negative                           | `2 * pt(q = t, df = df_1 + df_2)`

5. Decide whether or not to reject the null hypothesis.

If the $p$ value is less than the alpha level, you *reject* the null hypothesis, otherwise you *fail to reject* it.

<!-- ### Doing them all at once -->

<!-- If we're going to do $t$ tests all at once, we need to make sure we have loaded the `tidyverse` and `infer` packages: -->

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(infer) -->
<!-- ``` -->

<!-- For the following examples,  -->

<!-- #### One-sample $t$ test -->

<!-- 1. Translate your **research question** into a **null** and **alternative** hypothesis. -->

<!-- I will assume in this example that the population mean according to our null hypothesis is $\mu = 0$, but you can change this as you need: -->

<!-- ```{r} -->
<!-- null_mean <- 0 -->
<!-- ``` -->

<!-- 2. Decide on an **alpha level**. -->

<!-- 3. Find the $t$ value. -->

<!-- First, let's calculate the $t$ value and get R to remember it under the label `t`: -->

<!-- ```{r} -->
<!-- t <- my_data %>% -->
<!--     specify(response = X) %>% -->
<!--     hypothesize(null = 'point', mu = null_mean) %>% -->
<!--     calculate(stat = 't') -->
<!-- ``` -->

<!-- 4. Find the $p$ value. -->

<!-- Direction of test    | Is $t$ value positive or negative? | Code -->
<!-- ---------------------|------------------------------------|------------------------------------ -->
<!-- One-tailed (less)    | NA                                 | `pt(q = t, df = df_1 + df_2)` -->
<!-- One-tailed (greater) | NA                                 | `1 - pt(q = t, df = df_1 + df_2)` -->
<!-- Two-tailed           | Positive                           | `2 * (1 - pt(q = t, df = df_1 + df_2))` -->
<!-- Two-tailed           | Negative                           | `2 * pt(q = t, df = df_1 + df_2)` -->

<!-- 5. Decide whether or not to reject the null hypothesis. -->

<!-- If the $p$ value is less than the alpha level, you *reject* the null hypothesis, otherwise you *fail to reject* it. -->

<!--chapter:end:98-hyp_tests.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list=ls(all=TRUE))
```
`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

