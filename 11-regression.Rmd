# Linear Regression {#lab11}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = 'hold')
# knitr::opts_chunk$set(class.source = 'code-style')
# knitr::opts_chunk$set(class.output = 'out-style')
set.seed(12222)
```

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("img/colin_jackson.jpg")
```

In this session, we will get acquainted with **linear regression**.  This is an important technique in statistics because it allows us both to make *inferences* about relationships between variables, and because it allows us to make *predictions* that help us make decisions.  We will work through how to do linear regression in R to do both inference and prediction.

Before we begin, make sure to grab our usual `tidyverse` package from R's library, as well as the `magrittr` package:

```{r}
library(tidyverse)
library(magrittr)
```

## Inference: The Answer is Blowing in the Wind

We will get ourselves acquainted with linear regression in R using a small dataset.  This dataset records the finishing times for [Colin Jackson](https://en.wikipedia.org/wiki/Colin_Jackson) in the 110 meter hurdles that he competed in during the year 1990.

Load the data into R:

```{r}
wind_run_data <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/wind_run.csv")
```

There are 21 total races, for which we have two variables observed:

* **Time**: The amount of time Colin took to complete the race (in seconds).
* **Wind**: The speed of the wind (in meters per second) during the race.  Negative values are head winds, so they were blowing in Colin's face.  Positive values are tail winds, so they were blowing on his back.

Let's first make a **scatterplot** to see whether there might be a relationship between wind speed and finishing time:

```{r}
wind_run_data %>%
    ggplot(aes(x = Wind, y = Time)) +
    geom_point()
```

```{exercise, ex1101}
Based on the scatterplot, how would you describe the relationship between wind speed and finishing time?  Might they be correlated?  If so, is the correlation positive or negative?  Strong or weak?

```

### What a line is made of: Slopes and Intercepts

Let's see how well we can describe the relationship between wind speed and finishing time using a line.  As we've seen, the equation for a line looks like this:

$$
\hat{Y} = b_1 X + b_0
$$

where $b_1$ is the *slope* and $b_0$ is the *intercept*.  The slope describes how the response variable is predicted to change when the explanatory variable is increased by one.  The intercept describes what the predicted response is when the explanatory variable equals zero.

### Adding our own line

The following chunk of code adds a line to our scatterplot using `geom_abline`.  Notice that we need to specify the `slope` and `intercept` of the line that we want to add.

```{r}
wind_run_data %>%
    ggplot(aes(x = Wind, y = Time)) +
    geom_point() +
    geom_abline(slope = 0.1, intercept = 13.3)
```

```{exercise, ex1102}
Play around with the code above by changing the values for `slope` and `intercept` in the last line.  See if you can find values for the slope and intercept that make the line run close to the points in the plot.  What values did you find?

```

### Adding the best-fitting line

Now that we've got a reasonable guess about the best-fitting line, let's see how close we were.  We can add the best fitting line to our plot by adding another line to our code.  This line is `stat_smooth(method = "lm")`.  `stat_smooth` can draw various kinds of lines on top of our data that are meant to "smooth" the raw data and make it easier to see any important relationships there might be.  In the parentheses, we tell R that the `method` it should use is `lm`, where `lm` stands for "linear model" (we've already seen `lm` and we'll see it again shortly).

Note that you'll need to fill in the blanks in the code below with the slope and intercept you found in the last section.

```{r eval = FALSE}
wind_run_data %>%
    ggplot(aes(x = Wind, y = Time)) +
    geom_point() +
    geom_abline(slope = ___, intercept = ___) +
    stat_smooth(method = 'lm')
```

```{r echo = FALSE}
wind_run_data %>%
    ggplot(aes(x = Wind, y = Time)) +
    geom_point() +
    geom_abline(slope = 0.1, intercept = 13.3) +
    stat_smooth(method = 'lm')
```

```{exercise, ex1103}
How close were you to the best-fitting line in blue?  Notice that in addition to the line, there is a shaded region around the line.  This shaded region is a visual representation of our **uncertainty** regarding the slope and intercept of the best-fitting line.  Why do you think this region is narrower near the middle and gets wider toward the edges?

```

### Getting the slope and intercept

Being able to see the best-fitting line is great, but it doesn't let us *do* anything with it.  For example, if we want to predict Colin's time under a stronger headwind (e.g., -4 m/s), we'll need to actually know what $b_1$ and $b_0$ are.

In class, we saw how to calculate the coefficients of the best-fitting line using the descriptive statistics for our data.  Now, we will use R to do this.  The code is shockingly similar to what we used for ANOVA last time:

```{r}
wind_run_data %$%
    lm(Time ~ Wind)
```

Once again, the first line tells R what data we are working with.  The second line tells R that we want to find the best-fitting "linear model" (`lm`) that describes the linear relationship between our response variable (`Time`) and our explanatory variable (`Wind`).  R then told us the coefficients of that line.  R labels the slope with the same name as the explanatory variable (`Wind` in this case).

### How much variance does wind speed explain?

As we've seen, we can use $r^2$ to describe how much of the variability in Colin's finishing time can be explained by wind speed.  Because $r^2$ is the square of the Pearson correlation coefficient, we can find it by first calculating the correlation between finishing time and wind speed:

```{r}
wind_run_data %>%
    summarize(r = cor(Time, Wind))
```

If we take the square of that result, we'll get the value of $r^2$:

```{r}
(-0.635)^2
```

```{exercise, ex1104}
Based on the value for $r^2$, do you think that wind speed has a big effect on Colin's finishing times?  A small effect?  A moderate effect?  Remember that $r^2$ is always between 0 (no effect) and 1 (explains everything).

```

### Making Predictions

We can now use the linear model to make predictions about how fast Colin will be expected to finish based on different wind speeds.  First, let's see what the model predicts for each of the observed wind speeds:

```{r}
wind_run_data %$%
    lm(Time ~ Wind) %>%
    predict()
```

The result is what the model *predicts* Colin's finishing time would have been for each race in our dataset.  In other words, these are all the $\hat{Y}_i$'s for each $X_i$ in our data.  We can also get the **residuals** for each prediction, to see how far off it was:

```{r}
wind_run_data %$%
    lm(Time ~ Wind) %>%
    residuals()
```

To make a prediction about a *new* race, we need to add something to our "predict" line.  Specifically, we need to give it "new data" about the Wind speed for each prediction we want to make.  This is how we can do this:

```{r}
future_wind <- tibble(Wind = c(-1, 0, 1))
```

The previous line of code created a new dataset called `future_wind` that contains a single variable called `Wind` which has 3 values, -1, 0, and 1.  We can get Colin's predicted time for each of these three wind speeds like so:

```{r}
wind_run_data %$%
    lm(Time ~ Wind) %>%
    predict(newdata = future_wind)
```

```{exercise, ex1105}
Create a new "future_wind" dataset that contains the values -30 and 30, then use the preceding chunk of code to get predictions for how long Colin will take to finish under those two conditions.  What were the predictions?  Do they seem reasonable?  Does it seem reasonable that someone would be running when the wind is blowing 30 meters per second (67 miles per hour)?

```

### Hypothesis Testing

Based on the plots we made earlier, it certainly looks like there is an approximately linear relationship between Colin's finishing times and wind speed.  But the slope of the best-fitting line is only about -0.08, which seems kind of small.  So it is worth asking the **research question**, "is there a relationship between finishing time and wind speed?"

Let's use our standard 5-step hypothesis testing procedure to address this question.

1. Translate your research question into a null and alternative hypothesis

Our **null hypothesis** is that there is no such relationship, meaning that the true value of the slope is 0 ($H_0$: $b_1 = 0$).  Our **alternative hypothesis** is that there is a relationship, meaning that the true value of the slope is not zero ($H_1$: $b_1 \neq 0$).

2. Select an alpha level

We will select a reasonable **alpha level** of 0.05.

3. Find the $F$ value

4. Find the $p$ value

These two steps can be done using...the very same code we used for ANOVA!  As we saw in class, linear regression and ANOVA can be thought of as addressing the same question, it is just that the explanatory variable in ANOVA is nominal while the explanatory variable in linear regression is interval or ratio.  The following code gives us both the $F$ value and the $p$ value:

```{r}
wind_run_data %$%
    lm(Time ~ Wind) %>%
    anova()
```

As you can see, the basic structure of that code and what we used for ANOVA is the same, and the resulting output is nearly identical as well.  The difference is in the type of explanatory variable:  It is a nominal variable in ANOVA, whereas it is an interval or ratio scale variable in linear regression.  The shared basic outline is this:

```{r eval=FALSE}
Name_of_Data %$%
    lm(Name_of_Response_Variable ~ Name_of_Explanatory_Variable) %>%
    anova()
```

5. Decide whether or not to reject the null hypothesis

```{exercise, ex1106}
Based on the $p$ value reported in the table above, and based on the alpha level we chose in step 2, do we reject the null hypothesis?  What does this tell us about the relationship between Colin's performance and wind speed?

```

## Making Decisions: Housing Prices

Linear regression is often helpful when we have to make a decision.  Specifically, if we want to make a decision about a response variable---and all we know is an explanatory variable---linear regression allows us to use the relationship between the response and explanatory variables to make a reasonable guess.

One setting in which these decisions are important is *purchasing*.  The seller needs to decide how much to price what they are selling.  The potential buyer needs to know whether they are paying a fair price.

To see this in action, we will look at housing sale data from the lovely college town of Ames, Iowa.

```{r}
ames <- read_csv("https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ames.csv")
```

Although there are a lot of variables in this dataset, there are two that we will focus on:

* **area**: The total livable area of the house, in square feet.
* **price**: The final price for which the house sold.

### Visualize the data

First, let's make a scatterplot to visualize the relationship between these variables.  Fill in the blanks below.

```{r, eval=FALSE}
ames %>%
    ggplot(aes(x = ___, y = ___)) +
    ___()
```

```{r, echo=FALSE}
ames %>%
    ggplot(aes(x = area, y = price)) +
    geom_point()
```


```{exercise, ex1107}
What did you put in the blanks above?  How would you describe the relationship between area and price?  Are they positively or negatively correlated?  Is the correlation strong or weak?  Do you notice any "outliers", points that seem really odd relative to the rest of the data?

```

### Find the best-fitting line

Now let's add the best-fitting line to our scatterplot (again, be sure to fill in the blanks; see the Colin Jackson example above for examples):

```{r, eval=FALSE}
ames %>%
    ggplot(aes(x = ___, y = ___)) +
    ___() +
    stat_smooth(method = ___)
```

```{r, echo=FALSE}
ames %>%
    ggplot(aes(x = area, y = price)) +
    geom_point() +
    stat_smooth(method = 'lm')
```

And let's get the coefficients (slope and intercept) of the best-fitting line:

```{r}
ames %$%
    lm(price ~ area)
```

```{exercise, ex1108}
What does the intercept tell us about house prices and area?  What does the slope tell us about house prices and area?  Based on the scatterplot, do you think the linear model does a better job predicting prices for different sized houses?

```

### Make Predictions

Now let's put ourselves in different scenarios and see how our linear model can help us make decisions.

#### How to set the price?

Let's imagine that there are two houses on the market, one with a 1000 square foot area, one with 2000 square foot area.  Let's first use our linear model to *predict* the price for these three houses:

```{r}
new_price <- tibble(area = c(1000, 2000))

ames %$%
    lm(price ~ area) %>%
    predict(newdata = new_price)
```

As above, the first result is the predicted price for the 1000 square foot house and the second result is the predicted price for the 2000 square foot house.

```{exercise, ex1109}
Imagine you are a seller.  Look at the scatterplot to get a sense of where these predicted prices fall relative to all the houses around the two sizes.  Do you think the prices predicted from the linear model are too low, too high, or about average for these two houses?  Do you think you might have more flexibility in setting the price of the smaller house (1000 sq. ft.) or the larger house (2000 sq. ft.)?

```

#### Am I getting a fair price?

Now let's imagine that we are looking to buy a house.  We have three different options which are 1200, 2400, and 4500 square feet in size, respectively.  The listed prices for each house are

House Area ($X$)   | 1200 sq. ft | 2400 sq. ft | 4500 sq. ft
-------------------|-------------|-------------|------------
Actual Price ($Y$) | \$150000.00 | \$400000.00 | \$250000.00

Let's use our linear model to predict what the prices for these houses *would* be.  By comparing these predictions to the actual prices, we can see which prices seem fair, which seem excessively high, and which might be a discount.

First, fill in the blanks to get the predicted price for each of these houses:

```{r, eval=FALSE}
new_price <- tibble(___ = c(___))

ames %$%
    lm(price ~ area) %>%
    predict(newdata = new_price)
```

```{r, echo=FALSE}
new_price <- tibble(area = c(1200, 2400, 4500))

ames %$%
    lm(price ~ area) %>%
    predict(newdata = new_price)
```

```{exercise, ex1110}
By comparing the actual prices to the predicted prices, which of the three houses seems like a good deal?  Which seem to be overpriced?

```

## Wrap-up

Today, we saw how to do linear regression in R, including

1. How to visualize the best-fitting line that relates an explanatory variable and a response variable.
2. How to get the coefficients of the best-fitting line.
3. How to find the proportion of variance explained by the linear relationship ($r^2$).
4. How to make predictions using a linear model.
5. How to test the null hypothesis that the slope of the line is zero, to draw inferences about whether there is a meaningful relationship between the explanatory and response variables.
6. How to use predictions from the linear model to help make decisions.