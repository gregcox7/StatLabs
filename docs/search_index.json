[["index.html", "Labs Overview", " Labs Greg Cox 2021-02-19 Overview This is a collection of laboratory exercises as a part of APSY210. "],["lab1.html", "Lab 1 Exploring Data with R 1.1 R and RStudio 1.2 Meet your data 1.3 Answering questions with data 1.4 Wrap-up", " Lab 1 Exploring Data with R In this session, we will learn a bit about data and how to look at it in R. This will largely involve copying and pasting code from this document into your own RStudio window and interpreting the results, though you will also have to make some tweaks to that code on your own. The big point is to get a feel for the power of the tools we will be learning to use in the rest of the semester. For most of what we will be doing, this document shows both the R code and, below it, the result that code is expected to produce. The questions you will need to answer for the accompanying assignment are presented as numbered footnotes. Another big point is that even though statistics is about dealing with data, those data are meaningful. They are not just numbers or names, they are a peek into the world. They offer glimpses of someone’s life, of the workings of some natural process, of some social structure, etc. Any dataset will be limited in how wide of a glimpse it gives us, and the point of statistics is how to learn and make decisions based on that glimpse. The primary skills developed in this activity are: Viewing data in R Making frequency tables Making bar charts Making histograms 1.1 R and RStudio All of our labs will make use of RStudio, a graphical interface to the statistical computing language R. The R language represents the current state of the art in both academic and industrial research. It is likely to remain relevant for many years to come because it is free and open-source, meaning both that it is widely accessible and that improvements and extensions are relatively easy to make. In fact, many of the best features of R that we will be using are extensions made by people outside the “core” development team for R. These extensions are called “packages,” and they represent bundles of code that are useful for doing statistics. RStudio makes it easier to work with the R language, and it is also free and can be installed on computers running any modern operating system (Windows, Mac, Linux, etc.). RStudio is already installed on the computers in the Technology-Enhanced Classrooms and the Library Public Computing Sites on campus. If you are working on your own computer, you will have an easier time of it if you install RStudio on it. Installing RStudio requires installing R, but you only need to do this once. Follow the installation instructions for RStudio Desktop here: https://rstudio.com/products/rstudio/download/ You can also use RStudio in a browser! This way, even if you don’t have access to a computer with RStudio installed locally, you can use it if you have access to the internet. You can run RStudio online here: https://rstudio.cloud/. The downside with this is that there is a cap to the amount of time you can spend using the online version, so you are better off using a local installation whenever possible. This first lab will use the cloud version so we can jump right in. Access the first Lab here: https://rstudio.cloud/project/2118415 Note that you may need to create an account, but it is free to do so. 1.2 Meet your data The data we will be looking at are passenger records from the RMS Titanic, an oceanliner which famously sank on April 15, 1912. Though the liner was not filled to capacity, lax safety precautions—including a failure to carry enough lifeboats—meant that many of her passengers died because they were unable to evacuate when the ship struck an iceberg. 1.2.1 Check out the variables This is what RStudio looks like (with some helpful colored labels): In the upper left of the RStudio screen, you’ll see a bunch of columns. These are our data in “raw” form. Each row represents a specific passenger and each column represents a different variable. Based on the names of each variable and the types of values it seems to take, can you figure out what each variable is? In other words, what does each column tell us about a person?1 1.3 Answering questions with data Now that we’ve gotten acquainted with the kind of data we have, we can begin using it to answer some questions. This will involve simplifying the data, turning it into a summary form that makes it easier to understand. These summaries fall under the heading of “descriptive statistics,” because they are meant to describe important aspects of the data. The three types of summaries we will explore today are frequency tables, bar charts, and histograms. 1.3.1 Frequency tables The first question is, who was actually aboard the Titanic? One way we could answer this question is to read the names of all 1300 or so people in our dataset, but this would not be particularly efficient or informative. What we are going to do instead is simplify, and focus on specific aspects of each person. Let’s first ask how many passengers were male or female. One way to answer this question is by constructing a frequency table. In the RStudio window, see the big open space just below our data? This is called the “console” and is where we will do most of our work. Copy and paste the code below into the “Console.” The code should appear right after the “&gt;.” Once it is there, hit enter to run it and see the results. titanic %&gt;% group_by(sex) %&gt;% summarize(n = n()) ## # A tibble: 2 x 2 ## sex n ## * &lt;chr&gt; &lt;int&gt; ## 1 Female 466 ## 2 Male 843 We got a table that counted the frequency of males and females on the passenger list. We went from 1300 or so rows with multiple variables each to just two numbers. A pretty concise summary! But how did we do it? Let’s break down that bit of code: titanic is our original dataset. group_by(sex) tells R to group that dataset by sex. summarize(n=n()) tells R to take our grouped dataset and summarize it by counting the number of people in each group and labeling the resulting number “n.” The funky symbol %&gt;% connects the three steps above and makes sure R does them in the order we want. Let’s try a few things to get a sense of why that code did what it did. What happens if we change n = n() in the last line to Number = n()? titanic %&gt;% group_by(sex) %&gt;% summarize(Number = n()) ## # A tibble: 2 x 2 ## sex Number ## * &lt;chr&gt; &lt;int&gt; ## 1 Female 466 ## 2 Male 843 Everything looks the same except that instead of the column being labeled “n,” it is labeled “Number.” So the bit before the equals sign is how the frequency table will be labeled. Now let’s try something that seems like a small change: Instead of n = n() in the last line, let’s write n = m(). Only one letter different, surely it can’t be that big of a difference? titanic %&gt;% group_by(sex) %&gt;% summarize(n = m()) ## Error: Problem with `summarise()` input `n`. ## x could not find function &quot;m&quot; ## ℹ Input `n` is `m()`. ## ℹ The error occurred in group 1: sex = &quot;Female&quot;. R doesn’t like it! It reports an error because it doesn’t know what to do with m(). That’s because n() is a function, it is an instruction that tells R to count the number of something. On the other hand, m() doesn’t mean anything to R so it throws up its hands. We saw that we know not just the sex of each passenger, the “residence” variable tells us whether each person is American, British, or something else. Let’s modify our code to get a frequency table for country of residence instead of sex: titanic %&gt;% group_by(residence) %&gt;% summarize(n = n()) ## # A tibble: 3 x 2 ## residence n ## * &lt;chr&gt; &lt;int&gt; ## 1 American 258 ## 2 British 302 ## 3 Other 749 Easy! So all we need to do to get a frequency table for a particular variable is to put the name of that variable in the parentheses in the group_by line. Since we’re on a roll, let’s see if we can count the number of passengers with or without college degrees: titanic %&gt;% group_by(degree) %&gt;% summarize(n = n()) ## Error: Must group by variables found in `.data`. ## * Column `degree` is not found. No dice! R tells us that it can’t find a column labeled “degree,” and indeed, there is no such variable in our data since it was not recorded. This illustrates that the variable in the parens in the group_by line can’t be just anything, it has to be the name of a variable (spelled exactly the same!) that exists in our data. Finally, let’s construct a frequency table using multiple variables at once. This lets us answer questions like, how many British women were aboard the Titanic? We can put multiple variables in the group_by line: titanic %&gt;% group_by(residence, sex) %&gt;% summarize(n = n()) ## `summarise()` has grouped output by &#39;residence&#39;. You can override using the `.groups` argument. ## # A tibble: 6 x 3 ## # Groups: residence [3] ## residence sex n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 American Female 108 ## 2 American Male 150 ## 3 British Female 94 ## 4 British Male 208 ## 5 Other Female 264 ## 6 Other Male 485 Now we can begin to address a few more questions. This time, you will have to figure out how to fiddle with our code for making frequency tables. Write code that will produce the frequency table below:2 ## `summarise()` has grouped output by &#39;class&#39;. You can override using the `.groups` argument. ## # A tibble: 6 x 3 ## # Groups: class [3] ## class survived n ## &lt;int&gt; &lt;lgl&gt; &lt;int&gt; ## 1 1 FALSE 123 ## 2 1 TRUE 200 ## 3 2 FALSE 158 ## 4 2 TRUE 119 ## 5 3 FALSE 528 ## 6 3 TRUE 181 This table breaks down the number of people who did or did not survive the sinking of the Titanic by their “class,” with first-class being the most expensive with the most amenities and third-class being the least expensive with the least amenities. Do you notice any patterns?3 1.3.2 Bar charts Trying to find patterns among six numbers in a frequency table is not impossible, but it’s also not easy. Bar charts make numerical relationships easy to see visually, so we don’t need to compare a bunch of numbers. Let’s see how even a simple comparison is easier with a bar chart than a frequency table. First, let’s make a frequency table for the number of passengers in each class:4 ## # A tibble: 3 x 2 ## class n ## * &lt;int&gt; &lt;int&gt; ## 1 1 323 ## 2 2 277 ## 3 3 709 Now, use the following code to instead construct a bar chart that displays the same information as the table, but in a visual form: titanic %&gt;% ggplot(aes(x = class)) + geom_bar() Pretty neat! It is now easy to see how much more 3rd class passengers there are than 1st or 2nd, and that interestingly, there are fewer 2nd class than 1st class passengers5. Notice that the code we used is similar to what we’ve been using, but differs in some important ways: The first line is the same, telling R what dataset we are using (titanic). The second line tells R that we want to make a plot and that we want to put the variable class along the horizontal axis of that plot (the x axis). The “gg” in front of “plot” refers to the “grammar of graphics,” which is language R uses to describe plots. In this language, different parts of a plot are called “aesthetics,” which is why x = class falls inside a parenthetical labeled aes(thetic). The final line just tells R that we want to make a bar chart. In the grammar of graphics, different types of charts are called geoms. Notice that the second 2 lines are connected by a + rather than our %&gt;% symbol. This is a historical accident, but the meaning of the two symbols is basically the same: they are telling R the order in which it should follow our instructions. 1.3.3 Histograms So far, we have been summarizing discrete variables. There are also some continuous variables in our data, for example the age of each passenger as well as how much they paid for their tickets. Let’s try making a frequency table to figure out how many people of different ages sailed on the Titanic: titanic %&gt;% group_by(age) %&gt;% summarize(n = n()) ## # A tibble: 99 x 2 ## age n ## * &lt;dbl&gt; &lt;int&gt; ## 1 0.167 1 ## 2 0.333 1 ## 3 0.417 1 ## 4 0.667 1 ## 5 0.75 3 ## 6 0.833 3 ## 7 0.917 2 ## 8 1 10 ## 9 2 12 ## 10 3 7 ## # … with 89 more rows Well that’s not very helpful! R didn’t even bother to show us the whole table. Though we can see something interesting: Age is measured in years, and for passengers at least one year old, their age is a whole number. But there are fractions of years for passengers less than a year old—these ages were measured in months rather than years. The main point is that even though age can be measured in a more or less fine-grained manner, it is effectively continuous. We don’t want to know how many passengers were exactly 31.3491 years old, we want to get a sense of whether there are more younger than older passengers, whether there might be different clusters or age groups, that sort of thing. In other words, we want to know something about how people are distributed across different age ranges. We can construct a summary that conveys this information using a histogram. This is very similar to a bar chart; the difference is that bar charts are for discrete variables while histograms are for continuous variables. The code below constructs a histogram to summarize passenger age: titanic %&gt;% ggplot(aes(x = age)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 263 rows containing non-finite values (stat_bin). The resulting histogram shows a bunch of bars, the height of which indicate the number of passengers within a particular age range.6 Notice that we got a couple messages from R in addition to our plot, one about “non-finite values” and another about “picking a better value.” When R says, “non-finite values,” it is talking about people for whom their age was not recorded. This is an unfortunate thing about real data: sometimes it has missing pieces. This didn’t stop R from making the plot we wanted using the non-missing data, but R wanted to warn us just in case. The message about “picking a better value” is important: When you make a histogram, you are looking at how many things fall within a particular range of values, say, between ages 4 and 8. How do you decide those ranges? If you don’t tell R how to do that, it will decide on its own to divide up the range of values into 30 “bins,” each of which corresponds to a range of values that is the same width. This is usually not what we want. Instead, we should decide how big or small we want those ranges to be. The following code tells R to make our histogram using “bins” that are 5 years “wide” (0-4, 5-9, 10-14, etc.): titanic %&gt;% ggplot(aes(x = age)) + geom_histogram(binwidth = 5) ## Warning: Removed 263 rows containing non-finite values (stat_bin). Here’s what it looks like when we set the width of each bin to be just 1 year7: ## Warning: Removed 263 rows containing non-finite values (stat_bin). And here’s what it looks like when we set the width of each bin to be 10 years8: ## Warning: Removed 263 rows containing non-finite values (stat_bin). Choosing different bin widths makes the resulting histogram look very different. There is not necessarily one “right” answer, so being able to quickly see the differences between different histograms makes R very handy.9 1.4 Wrap-up Today we began our adventure by using RStudio to explore some data. We saw how to look at data and how to summarize it in various helpful ways. These were frequency tables, bar charts, and histograms. Frequency tables count the number of times a particular value of a particular variable (or combination of values across multiple variables) occurs in our dataset. Bar charts display those counts in a visual form that makes it easier to compare frequencies. Histograms let us visually summarize counts of continuous variables by putting them in “bins,” the width of which we need to decide. For each variable in the dataset, say whether it is a) Discrete or Continuous; b) what scale it is measured on (nominal, ordinal, interval, or ratio); and c) what you think it means based on the name of the variable and/or what values it takes.↩︎ What code did you write to make a frequency table for number of people of each class who did or did not survive?↩︎ Were there more people in third class than first class? For each of the three classes, is the number of survivors more than the number who died?↩︎ What R code would produce this frequency table?↩︎ What about the bar chart conveys the same information as the frequency table? In other words, what is the visual aspect of the bar chart that represents the numbers in the table?↩︎ Compare the code we used to make a histogram to what we used to make a bar chart above. What is different between them?↩︎ What code would produce this plot?↩︎ What code would produce this plot?↩︎ What bin width do you think makes the best histogram for passenger ages? Why? Do you think different bin widths might be better in different circumstances?↩︎ "],["lab2.html", "Lab 2 Central tendency and variability 2.1 Simple cases 2.2 Dealing with real data 2.3 Wrap-up", " Lab 2 Central tendency and variability In this session, we will learn how to use R to calculate descriptive statistics. Specifically, these are statistics describing central tendency and variability. We saw how complicated it can be to calculate some of these things by hand. While it is good to understand what is going on “behind the scenes,” doing such calculations by hand is not only time consuming but prone to error. If you make a typo at one step, that little error gets magnified as you keep going. Besides, as we saw last week, most of the time, dataset are large and it is simply unreasonable to calculate a mean of 1300 numbers by hand. Luckily, computers are great at doing the kinds of mechanical, repetitive, mindless steps involved. But if the computer is great at mindless stuff, that means us people have to be better at mindful stuff. We need to think about what these numbers mean and why they may be important for understanding the world. Another part of computers being mindless is that they will always do exactly what you tell them to do, unless they can’t. So we have to be extremely careful when we write code because, just like making a mistake doing calculations by hand, a mistake in code can propagate into a mistake in the result. Think about your neighbor’s obnoxious dog—it is not the dog’s fault it is obnoxious, it is the fault of the neighbor for doing a poor job training her. Why is doing statistics with a computer any better, if we still have to worry about making a mistake? Because the computer is fast and can deal with a lot more data than we can. The primary skills developed in this activity are: Using R to calculate central tendency Median Mean Using R to calculate variability Variance Standard deviation 2.1 Simple cases 2.1.1 Central tendency Imagine that we are thinking about selling a new product. We demonstrate the product to five people in a focus group, each of whom gives us a rating of the product on a scale from 1 (very dissatisfied) to 9 (very satisfied)10. Their ratings, in no particular order, are 6, 9, 5, 5, and 7. Recall that the mean is defined by the formula \\(\\bar{X} = \\frac{\\sum_{i = 1}^N X_i}{N}\\), where each \\(X_i\\) is a measurement and \\(N\\) is the number of measurements. We can expand the summation \\(\\sum_{i = 1}^N X_i\\) into \\(X_1 + X_2 + X_3 + \\cdots + X_N\\), where the subscripts (1, 2, 3, etc.) indicate which of the measurements is being added to the total sum. We can use R to do this calculation by basically writing out in R how we would write it out if we were finding the mean by hand. Then we let R do the actual arithmetic. (6 + 9 + 5 + 5 + 7) / 5 ## [1] 6.4 We have basically turned R into a calculator. R can do all the typical things a calculator can do, like addition (+) and division (/), as well as subtraction (-) and multiplication (*). We used the parentheses to tell R to add up all the numbers before dividing them by 5. But that’s still pretty clumsy. R gives us a more elegant way to find the mean of a set of measurements: mean(c(6, 9, 5, 5, 7)) ## [1] 6.4 Nice! How did that work? If you tell R mean(), it will give you the mean of whatever is inside those parentheses. But what is that c() thing? The c() tells R to collect the numbers together. It tells R to treat them collectively; in our mathematical notation, the c() tells R that all the things inside those inner parentheses are to be treated like \\(X_i\\)’s, a set of measurements all on the same variable. Can we keep using this trick? Let’s use R to find the median: median(c(6, 9, 5, 5, 7)) ## [1] 6 Worked like a charm! Again, we had to tell R that our numbers were to be treated like a collection, and we told R to find the median of that collection of numbers. Now imagine we get a sixth person to rate our product and they give it an 8. Now our data consist of a set of six ratings (6, 9, 5, 5, 7, 8). We can again use R to find the new mean11 ## [1] 6.666667 and the median12 ## [1] 6.666667 Sadly, R does not give us a straightforward way to get the mode, so that’s something we still have to figure out on our own.13 2.1.2 Variability R helpfully provides similar methods for calculating variability. For example, we can calculate the variance of the focus group ratings like this var(c(6, 9, 5, 5, 7, 8)) ## [1] 2.666667 Notice that R abbreviates variance to just var, but otherwise the code looks basically the same as we used for the mean and median. When you ask R for the variance, you get the so-called “sample” variance, given by \\(\\frac{\\sum_{i = 1}^N \\left( X_i - \\bar{X} \\right)^2}{N - 1}\\). Similarly, we can get the standard deviation by swapping out var for sd (short for standard deviation) sd(c(6, 9, 5, 5, 7, 8)) ## [1] 1.632993 We can verify that this is the square root of the variance. R has another “function” called sqrt that gives you the square root of whatever is in the parentheses that follow it. Check it out: sqrt(var(c(6, 9, 5, 5, 7, 8))) ## [1] 1.632993 Notice that you can “wrap” things in parentheses and put function names in front, like we put the sqrt in front of the var. 2.2 Dealing with real data The examples above illustrated one way we can use R to calculate descriptions of central tendency and variability. These are useful when we have a small amount of data that we can type in without risk of error, but that’s almost never the situation we have to deal with in applying statistics to real data. 2.2.1 Setting the stage Before we can get to work with real data, we have to open up the right toolbox. In R, this is done by retrieving a “package” from R’s “library.” Remember that one of the nice things about R is that there are so many packages available that each contain helpful functions. Of course, the downside is that we need to tell R to open up those packages, but this isn’t too hard. Specifically, we’ll need to open the “tidyverse” package: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() R will tell you a bunch of stuff, including something about “conflicts.” This isn’t a big deal for us, R is just saying that we have some tools with the same name, and it calls this a “conflict.” R is just trying to keep you informed. The tidyverse package is one that we will be using a lot. It contains a lot of statistical tools and tools for making graphics that will be very useful for us throughout the course. 2.2.2 Getting the data The data we will be looking at come from a study by Yu et al. (1982). They studied the lifespan of rats with two different diets: One group of rats was allowed to eat freely, however they wanted; another group was fed on a restricted diet with only about 60% of the calories that the free-eating rats had. We need to get this data into R. The following code will import the data into your current R session: rats &lt;- read_csv(&#39;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv&#39;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Diet = col_character(), ## Lifespan = col_double() ## ) In your “Environment” panel in the upper right, you’ll see a new entry called “rats.” This is the data we just imported. Click on it to have a look at the data, which will appear in the upper left. There are just two variables in this dataset, Diet and Lifespan (measured in days)14. 2.2.3 Visualizing the data Numerical techniques for describing data, like means and variances, are useful compact descriptions of potentially complex distributions. But while they can be quick and easy ways to communicate (even if they are not always quick and easy to calculate), it is still important to get a sense of what the data really look like. As a result, we will never abandon our friends from last week, the bar chart and histogram. It is essential to visualize your data prior to doing any kind of quantitative analyses, since visualizing the data helps us understand what those quantities mean. Just like last week, we can construct a histogram to see how long different rats lived depending on their diet. rats %&gt;% ggplot(aes(x=Lifespan)) + geom_histogram(binwidth=60) So far, so good. The code above takes our data, puts the “Lifespan” variable along the \\(x\\) axis, and makes a histogram by putting the observed rat lifespans into “bins” that are 60 days wide.15 The histogram illustrates some interesting features of the data, notably that it appears to be bimodal16. Is this surprising? Remember that our dataset actually consists of two groups of rats with different diets, so maybe that is why the histogram looks the way it does. We can use a new “aesthetic” to split the histogram up so we can see the two different groups: rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, position = &quot;identity&quot;, alpha=0.5) There are a couple things that got added there, especially in that last line, that probably aren’t that clear. First, the new “aesthetic”: In the second line, we tell R to put Lifespan along the x axis like before, but now we are also telling R to fill the histograms with a different color depending on the Diet variable. Now, what about that other stuff in the third line? Now, in addition to telling R how wide the bins of our histogram should be (binwidth = 60), position = \"identity\" tells R that it should overlay the two histograms. If you don’t tell R to do that, it will instead “stack” them, like this: rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, alpha=0.5) Which is kind of confusing. What about alpha = 0.5? This gets a bit into computer graphics, but alpha represents the opacity of something. If alpha = 1, this means completely opaque, you can’t see anything through it. If alpha = 0, this means completely transparent (invisible). So alpha = 0.5 is halfway in between. Because we are putting the two histograms on top of one another, we want them to be a little bit see-through so one doesn’t obscure the other. See what happens when we leave out that line (and alpha = 1 by default): rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, position = &quot;identity&quot;) Again, harder to see what is going on. 2.2.4 Central tendency By plotting the lifespans of the two groups of rats on separate but overlapping histograms, we can see there are some important differences between them.17 Now we can describe those differences numerically. We saw last week how to summarize data in terms of counting the frequency with which different values were observed. The code to summarize the central tendency of a dataset is similar: rats %&gt;% group_by(Diet) %&gt;% summarize(M = mean(Lifespan)) ## # A tibble: 2 x 2 ## Diet M ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Free 684. ## 2 Restricted 969. And just like that, R has calculated the mean lifespans of the rats in each group.18 Looking at the code, the first two lines are similar to how we got R to make frequency tables last time: We tell R what data we are working with (rats), tell R to group the data by a particular variable (Diet), and then tell R to make us a summary. The last line is again a summarize instruction, but it looks a bit different. Last time, we put N = n() inside the parentheses, which told R to count the number of individuals in each group and put those counts in a column labeled N. By saying M = mean(Lifespan), we are telling R to find the mean lifespan of the rats in each group and put the result in a column labeled M. We replaced N with M and n() with mean(Lifespan). In so doing, we changed the label (N to M) and changed the way we summarized the data (from n(), which just counted, to mean(Lifespan) which finds the mean Lifespan). Just like we did with the simple data above, we can swap out mean in the last line for different summaries. For example, we can create a table that gives us the median lifespan of the rats in each group:19 ## # A tibble: 2 x 2 ## Diet M ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Free 710 ## 2 Restricted 1036. Comparing the mean to the median gives us a sense of the extent to which the distributions of lifespans of the two groups of rats are skewed.20 2.2.5 Variability We can also make a summary table with multiple descriptive statistics, including both measures of central tendency and of variability. The following code gives us a table that reports not just the mean lifespans of rats in each group, but the standard deviation of those lifespans as well: rats %&gt;% group_by(Diet) %&gt;% summarize(M = mean(Lifespan), SD = sd(Lifespan)) ## # A tibble: 2 x 3 ## Diet M SD ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Free 684. 134. ## 2 Restricted 969. 285. And, of course, we can create a table that summarizes the mean, median, standard deviation, and variance all at once!21 ## # A tibble: 2 x 5 ## Diet Mean Median Variance SD ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Free 684. 710 17979. 134. ## 2 Restricted 969. 1036. 80986. 285. These numerical summaries of central tendency and variability provide a concise description of the lifespans of rats under the two different diets. This description helps us see whether there is any benefit to a restricted diet on lifespan.22 2.3 Wrap-up Today we saw how to use R to find various numerical descriptive statistics, both in simple cases and with real data. We focused on two measures of central tendency, the mean and the median, as well as two measures of variability, the variance and the standard deviation. We saw how these quantities are useful in summarizing complex data, making it easier to draw conclusions about differences between groups. We also learned a new trick about how to visualize histograms from multiple groups. Is this measure continuous or discrete? What is the scale of measurement?↩︎ What is the code that calculates the new mean? Hint: Try modifying the collection of observed values within the parentheses in the code above.↩︎ What is the code that calculates the new median? Hint: Try modifying the collection of observed values within the parentheses in the code above.↩︎ Just by looking at the observed values we have, what is the mode of the focus group’s ratings?↩︎ For each of these variables, are they a) continuous or discrete? b) which scale are they measured on (nominal, ordinal, interval, or ratio)?↩︎ Compare the code we just used with the code we used to make a histogram of the ages of Titanic passengers last week. What is different and what is similar?↩︎ Based on the histogram, roughly where do the two modes seem to be?↩︎ Based on the histogram, does it seem like one group of rats tends to live longer the the other? Do the groups seem to have similar amounts of variability?↩︎ Which group has the higher average lifespan? Does this make sense based on what you saw in the histogram?↩︎ What code would produce this table? Hint: it requires modifying the name of the central tendency measure in the last line of the previous bit of code.↩︎ Compare the median lifespan to the mean lifespan in each group. Based on this comparison, would you say that lifespans in either group are symmetric, positively skewed, or negatively skewed?↩︎ What code would produce the table below? Hint: See how we added a column for the standard deviation? And remember how we told R to find different descriptive statistics above?↩︎ What would you conclude? Is there a benefit or harm for the restricted diet over normal free diet? Is this true for every rat? Is the size of any benefit or harm big or small relative to how much rat lifespans tend to vary?↩︎ "],["lab3.html", "Lab 3 Scatterplots and correlation 3.1 Calculating the Pearson correlation coefficient 3.2 Scatterplots 3.3 Some bigger data 3.4 Wrap-up", " Lab 3 Scatterplots and correlation So far, most of what we’ve done has been about describing measurements on one variable at a time. Today’s session will look at how we can explore relationships between variables. If there is a systematic relationship between two variables, they are said to be correlated. The primary skills developed in this activity are a visual and a numerical way to explore relationships between variables: The visual way is called a scatterplot. The numerical way is the Pearson correlation coefficient. Before we begin, make sure you have opened the tidyverse package from the library: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 3.1 Calculating the Pearson correlation coefficient We saw last time that one way R is useful is as a powerful calculator. Calculating things like standard deviations by hand is hard enough, but calculating the Pearson correlation coefficient requires doing that twice (once for each variable), in addition to a lot of other steps. R can help ease that burden. 3.1.1 From \\(X\\) to \\(z\\) Recall that the Pearson correlation coefficient can be found using \\(z\\) scores. Specifically, if we have measurements on one variable—let’s call them \\(X\\)—and measurements on another variable—let’s call them \\(Y\\)—we can find the correlation by first finding the \\(z\\) scores for these two sets of measurements. 3.1.1.1 R can remember things Let’s try doing that in R. Remember that R uses c() to say that everything within the parentheses is supposed to be treated as a collection of measurements on a single variable. Rather than having to write out that collection every time we want to use it, we can tell R to remember it. Let’s say we have a collection of 4 measurements and we want to tell R to remember them with the label “X.” We can do that this way: X &lt;- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5) The important thing to note in the line above is the arrow &lt;-. The arrow tells R to take the stuff on the right and put it under the label on the left. You can think of the arrow as saying that the collection c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5) is now “inside” a box labeled X. Now if we write “X” at the console and hit enter, R will remind us what we put inside the “X” box: X ## [1] 10 8 13 9 11 14 6 4 12 7 5 We can also find a specific value using square brackets. For example, we can find the 3rd measurement this way: X[3] ## [1] 13 Nice!23 This is just like how we used \\(X_3\\) as a shorthand for the 3rd measurement when we were doing things by hand.24 3.1.1.2 Using labeled collections Now that R can remember that whole collection of measurements, we can use it to do calculations. For example, this will find the mean of all the numbers we put under the label “X”: mean(X) ## [1] 9 And this will find the standard deviation: sd(X) ## [1] 3.316625 Finally, we can combine different operations into a single line of code. The following will convert each of the measurements in X into \\(z\\) scores: (X - mean(X)) / sd(X) ## [1] 0.3015113 -0.3015113 1.2060454 0.0000000 0.6030227 1.5075567 ## [7] -0.9045340 -1.5075567 0.9045340 -0.6030227 -1.2060454 We can even tell R to remember those \\(z\\) scores like so: z_X &lt;- (X - mean(X)) / sd(X) Note that we gave these \\(z\\) scores a special label, “z_X,” so that we know they originally came from our collection labeled X. Now if we ask R to tell us what we put under the “z_X” label, we’ll get our \\(z\\) scores back z_X ## [1] 0.3015113 -0.3015113 1.2060454 0.0000000 0.6030227 1.5075567 ## [7] -0.9045340 -1.5075567 0.9045340 -0.6030227 -1.2060454 3.1.2 Finding the correlation coefficient 3.1.2.1 Introducing a new set of measurements Now that we’ve seen how to tell R to remember different collections of values, let’s add another variable to the mix and find its correlation with \\(X\\). We will call this variable \\(Y\\) to keep it distinct. Y &lt;- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68) Let’s see what the mean of \\(Y\\) is25 ## [1] 7.500909 as well as its standard deviation26 ## [1] 2.031568 Finally, let’s find the \\(z\\) scores for each of our \\(Y\\) values and tell R to remember them with the label z_Y27: If we’ve done our job right, we should get the following output when we ask R to remember what we put into z_Y z_Y ## [1] 0.26535704 -0.27117431 0.03893097 0.64437460 0.40810391 1.21043979 ## [7] -0.12842744 -1.59527462 1.64360272 -1.31962549 -0.89630717 3.1.2.2 Calculating correlation from \\(z\\) scores To get the correlation coefficient between \\(X\\) and \\(Y\\) from our \\(z\\) scores, we need to multiply the \\(z\\) scores together, add them up, and divide by the number of values minus one. We can tell R to multiply together all our pairs of \\(z\\) scores like this z_X * z_Y ## [1] 0.08000816 0.08176213 0.04695251 0.00000000 0.24609592 1.82480664 ## [7] 0.11616699 2.40496698 1.48669460 0.79576411 1.08098712 We can add them up like this, by telling R to find the sum sum(z_X * z_Y) ## [1] 8.164205 Finally, we need to divide by \\(N - 1\\). \\(N\\) in this case is 11, so \\(N - 1 = 11 - 1 = 10\\). sum(z_X * z_Y) / 10 ## [1] 0.8164205 And that’s the Pearson correlation coefficient!28 3.1.2.3 Calculating correlation directly This is what you might call the “shaggy-dog” way of calculating the correlation, in that it takes a long time to get to the punchline. Fortunately, R gives us an even easier way! The correlation between \\(X\\) and \\(Y\\) can be found like this cor(X, Y) ## [1] 0.8164205 cor tells R to find the correlation between the two sets of measurements labeled X and Y. Remember that R knows what we mean by X and Y only because we already told R what they stood for. To get a sense of how that works, let’s replace our old Y with a new one: Y &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11) Now if we ask for the correlation again, it will be different29 cor(X, Y) ## [1] -0.4272727 3.1.3 Dealing with real data The steps above are useful when you’ve got relatively few measurements that you can copy-and-paste into the R console. Real data is often more complicated, so let’s see how we can deal with it. 3.1.3.1 Get some data First, we need to import some data into R using the code below. anscombe &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Group = col_character(), ## X = col_double(), ## Y = col_double() ## ) These data are artificial and were devised by Anscombe (1973) to illustrate the importance of not interpreting a correlation coefficient without visualizing your data. 3.1.3.2 Correlations for different sets of measurements There are four groups of measurements in these data. The groups are labeled “A,” “B,” “C,” and “D.” Measurements are on two variables, labeled “X” and “Y.” Click on anscombe in the environment panel in RStudio (upper right) to take a look. We can quickly find the correlation coefficient for each group of “X” and “Y” measurements30 anscombe %&gt;% group_by(Group) %&gt;% summarize(r = cor(X, Y)) ## # A tibble: 4 x 2 ## Group r ## * &lt;chr&gt; &lt;dbl&gt; ## 1 A 0.816 ## 2 B 0.816 ## 3 C 0.816 ## 4 D 0.817 Notice that the correlation coefficients are almost all the same for each group. But now let’s try making some scatterplots to see if these correlations make sense. 3.2 Scatterplots As we’ve seen, a scatterplot lets us see how measurements on two variables are related to one another. It does this with a bunch of dots “scattered” around the plot. Each dot represents a single individual or object. The horizontal position of the dot represents that individual’s observed value on one variable. The vertical position of the dot represents that same individual’s observed value on a different variable. 3.2.1 Making a group of scatterplots R makes it easy not just to make a single scatterplot, but an entire group of them at once. The code below produces a set of scatterplots, one for each of the four groups, that puts the “X” measurements on the horizontal and the “Y” measurements on the vertical31: anscombe %&gt;% ggplot(aes(x=X, y=Y)) + geom_point() + facet_wrap(&quot;Group&quot;) Even though the correlation coefficient is the same for each group, the scatterplots all look very different!32 3.3 Some bigger data Now that we’ve seen how to find correlation coefficients and make scatterplots, let’s apply these tools to some real data. These data come from a study by Ryan &amp; Gauthier (2016), who studied differences in how well people can remember different kinds of objects. The point of this is to understand how we develop the ability to make fine visual distinctions between some things (e.g., between different peoples’ faces) but not others (e.g., leaves or snowflakes). They reasoned that the more experience someone has in dealing with a particular category of things, the better they’ll be at noticing and remembering the visual features that distinguish between members of that category. Two of those categories were different types of toys: Barbies and Transformers. They measured how well their participants could tell apart pictures of Barbies that they had seen from pictures of Barbies they hadn’t seen. They did the same for pictures of Transformers. The final memory scores are a number correct out of 48. Finally, each participant indicated on a scale from 1 to 9 how much experience they had with each type of toy. The question is, “is experience with a type of toy correlated with better memory?” 3.3.1 Import the data To import the data into R, use the line below toys &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/toys.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Barbie_Score = col_double(), ## Transf_Score = col_double(), ## Barbie_Exp = col_double(), ## Transf_Exp = col_double() ## ) Click on the new “toys” dataset in the Environment panel (upper right) to see what the data look like. “Barbie_Score” and “Transf_Score” are the scores on the two memory tests, one for Barbies and one for Transformers (respectively). “Barbie_Exp” and “Transf_Exp” are the two reports of prior experience with Barbies and Transformers, respectively. 3.3.2 Experience and memory for Barbies Let’s first make a scatterplot showing Barbie experience on the horizontal axis and Barbie memory on the vertical axis. We can do that using this code: toys %&gt;% ggplot(aes(x=Barbie_Exp, y=Barbie_Score)) + geom_point() This gives us a sense of what might be going on.33 Now let’s calculate the Pearson correlation coefficient (notice that there are no groups here, so we don’t need a group_by line)34: toys %&gt;% summarize(r = cor(Barbie_Exp, Barbie_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 0.142 3.3.3 Experience and memory for Transformers Now let’s do the same thing for Transformers. Here’s the scatterplot:35 And this is the correlation coefficient between Transformers Experience and Transformers memory score:36 toys %&gt;% summarize(r = cor(Transf_Exp, Transf_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 0.0964 It seems like the relationship between experience and memory is a bit weaker for Transformers than for Barbies. But there’s another interesting correlation here. 3.3.4 Experience with Barbies and memory for Transformers Let’s see if there’s a relationship between experience with Barbies and memory for Transformers. toys %&gt;% ggplot(aes(x=Barbie_Exp, y=Transf_Score)) + geom_point() The scatterplot is shown above and this gives the Pearson correlation coefficient: toys %&gt;% summarize(r = cor(Barbie_Exp, Transf_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 -0.124 Interesting! Even though there is at best a weak relationship between Transformers experience and Transformers memory, there is actually a stronger relationship between Barbie experience and Transformers memory, just a negative one!37 3.4 Wrap-up In today’s session, we saw how to find \\(z\\) scores and correlation coefficients in R. We also used R to make scatterplots and saw an example of why it is critical to look at a scatterplot before interpreting the correlation coefficient. How would you get R to tell you what the first measurement was?↩︎ What do you think would happen if we asked for the 12th measurement? Try it and see! Hint: how many measurements do we have?↩︎ What code will find the mean of \\(Y\\)? Hint: how did we find the mean of \\(X\\)?↩︎ What code will find the standard deviation of \\(Y\\)? Hint: how did we find the standard deviation of \\(X\\)?↩︎ What code would you write to do this? Hint: what code did we use to calculate and remember the \\(z\\) scores for \\(X\\)?↩︎ Is the correlation positive or negative? Does it seem strong or weak?↩︎ Why do we get a different result from cor(X, Y) even though the code itself is the same?↩︎ Compare the code we used to find these correlations with the code we used last time to find the means of the different groups of rat lifespans. What is similar and what is different?↩︎ What do you think the function of the line facet_wrap(\"Group\") is in the code below?↩︎ For each group, say whether you think the correlation coefficient provides a fair description of the relationship between the two variables. Why or why not?↩︎ Based on this scatterplot, does it seem like there might be a correlation between Barbie experience and visual memory for Barbies? If so, is it positive or negative? Weak or strong?↩︎ Do the sign and magnitude of the correlation coefficient make sense based on the scatterplot?↩︎ What code would produce this scatterplot? Hint: what code did we use to make the Barbie scatterplot? What variables in the dataset refer to Transformers rather than Barbies?↩︎ What code would find this correlation? Hint: what code did we use to find the Barbie correlation?↩︎ Can you think of any reasons why there might be a negative correlation between Barbie experience and Transformers memory?↩︎ "],["references.html", "References", " References Anscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27(1), 17–21. Ryan, K. F., &amp; Gauthier, I. (2016). Gender differences in recognition of toy faces suggest a contribution of experience. Vision Research, 129, 69–76. Yu, B. P., Masoro, E. J., Murata, I., Bertrand, H. A., &amp; Lynd, F. T. (1982). Life span study of SPF Fischer 344 male rats fed ad libitum or restricted diets: Longevity, growth, lean body mass and disease. Journal of Gerontology, 37(2), 130–141. "]]
