[["index.html", "Labs Overview", " Labs Greg Cox 2021-04-16 Overview This is a collection of laboratory exercises as a part of APSY210. "],["lab1.html", "Lab 1 Exploring Data with R 1.1 R and RStudio 1.2 Meet your data 1.3 Answering questions with data 1.4 Wrap-up", " Lab 1 Exploring Data with R In this session, we will learn a bit about data and how to look at it in R. This will largely involve copying and pasting code from this document into your own RStudio window and interpreting the results, though you will also have to make some tweaks to that code on your own. The big point is to get a feel for the power of the tools we will be learning to use in the rest of the semester. For most of what we will be doing, this document shows both the R code and, below it, the result that code is expected to produce. The questions you will need to answer for the accompanying assignment are presented as numbered footnotes. Another big point is that even though statistics is about dealing with data, those data are meaningful. They are not just numbers or names, they are a peek into the world. They offer glimpses of someone’s life, of the workings of some natural process, of some social structure, etc. Any dataset will be limited in how wide of a glimpse it gives us, and the point of statistics is how to learn and make decisions based on that glimpse. The primary skills developed in this activity are: Viewing data in R Making frequency tables Making bar charts Making histograms 1.1 R and RStudio All of our labs will make use of RStudio, a graphical interface to the statistical computing language R. The R language represents the current state of the art in both academic and industrial research. It is likely to remain relevant for many years to come because it is free and open-source, meaning both that it is widely accessible and that improvements and extensions are relatively easy to make. In fact, many of the best features of R that we will be using are extensions made by people outside the “core” development team for R. These extensions are called “packages,” and they represent bundles of code that are useful for doing statistics. RStudio makes it easier to work with the R language, and it is also free and can be installed on computers running any modern operating system (Windows, Mac, Linux, etc.). RStudio is already installed on the computers in the Technology-Enhanced Classrooms and the Library Public Computing Sites on campus. If you are working on your own computer, you will have an easier time of it if you install RStudio on it. Installing RStudio requires installing R, but you only need to do this once. Follow the installation instructions for RStudio Desktop here: https://rstudio.com/products/rstudio/download/ You can also use RStudio in a browser! This way, even if you don’t have access to a computer with RStudio installed locally, you can use it if you have access to the internet. You can run RStudio online here: https://rstudio.cloud/. The downside with this is that there is a cap to the amount of time you can spend using the online version, so you are better off using a local installation whenever possible. This first lab will use the cloud version so we can jump right in. Access the first Lab here: https://rstudio.cloud/project/2118415 Note that you may need to create an account, but it is free to do so. 1.2 Meet your data The data we will be looking at are passenger records from the RMS Titanic, an oceanliner which famously sank on April 15, 1912. Though the liner was not filled to capacity, lax safety precautions—including a failure to carry enough lifeboats—meant that many of her passengers died because they were unable to evacuate when the ship struck an iceberg. 1.2.1 Check out the variables This is what RStudio looks like (with some helpful colored labels): In the upper left of the RStudio screen, you’ll see a bunch of columns. These are our data in “raw” form. Each row represents a specific passenger and each column represents a different variable. Based on the names of each variable and the types of values it seems to take, can you figure out what each variable is? In other words, what does each column tell us about a person?1 1.3 Answering questions with data Now that we’ve gotten acquainted with the kind of data we have, we can begin using it to answer some questions. This will involve simplifying the data, turning it into a summary form that makes it easier to understand. These summaries fall under the heading of “descriptive statistics,” because they are meant to describe important aspects of the data. The three types of summaries we will explore today are frequency tables, bar charts, and histograms. 1.3.1 Frequency tables The first question is, who was actually aboard the Titanic? One way we could answer this question is to read the names of all 1300 or so people in our dataset, but this would not be particularly efficient or informative. What we are going to do instead is simplify, and focus on specific aspects of each person. Let’s first ask how many passengers were male or female. One way to answer this question is by constructing a frequency table. In the RStudio window, see the big open space just below our data? This is called the “console” and is where we will do most of our work. Copy and paste the code below into the “Console.” The code should appear right after the “&gt;.” Once it is there, hit enter to run it and see the results. titanic %&gt;% group_by(sex) %&gt;% summarize(n = n()) ## # A tibble: 2 x 2 ## sex n ## * &lt;chr&gt; &lt;int&gt; ## 1 Female 466 ## 2 Male 843 We got a table that counted the frequency of males and females on the passenger list. We went from 1300 or so rows with multiple variables each to just two numbers. A pretty concise summary! But how did we do it? Let’s break down that bit of code: titanic is our original dataset. group_by(sex) tells R to group that dataset by sex. summarize(n=n()) tells R to take our grouped dataset and summarize it by counting the number of people in each group and labeling the resulting number “n.” The funky symbol %&gt;% connects the three steps above and makes sure R does them in the order we want. Let’s try a few things to get a sense of why that code did what it did. What happens if we change n = n() in the last line to Number = n()? titanic %&gt;% group_by(sex) %&gt;% summarize(Number = n()) ## # A tibble: 2 x 2 ## sex Number ## * &lt;chr&gt; &lt;int&gt; ## 1 Female 466 ## 2 Male 843 Everything looks the same except that instead of the column being labeled “n,” it is labeled “Number.” So the bit before the equals sign is how the frequency table will be labeled. Now let’s try something that seems like a small change: Instead of n = n() in the last line, let’s write n = m(). Only one letter different, surely it can’t be that big of a difference? titanic %&gt;% group_by(sex) %&gt;% summarize(n = m()) ## Error: Problem with `summarise()` input `n`. ## x could not find function &quot;m&quot; ## ℹ Input `n` is `m()`. ## ℹ The error occurred in group 1: sex = &quot;Female&quot;. R doesn’t like it! It reports an error because it doesn’t know what to do with m(). That’s because n() is a function, it is an instruction that tells R to count the number of something. On the other hand, m() doesn’t mean anything to R so it throws up its hands. We saw that we know not just the sex of each passenger, the “residence” variable tells us whether each person is American, British, or something else. Let’s modify our code to get a frequency table for country of residence instead of sex: titanic %&gt;% group_by(residence) %&gt;% summarize(n = n()) ## # A tibble: 3 x 2 ## residence n ## * &lt;chr&gt; &lt;int&gt; ## 1 American 258 ## 2 British 302 ## 3 Other 749 Easy! So all we need to do to get a frequency table for a particular variable is to put the name of that variable in the parentheses in the group_by line. Since we’re on a roll, let’s see if we can count the number of passengers with or without college degrees: titanic %&gt;% group_by(degree) %&gt;% summarize(n = n()) ## Error: Must group by variables found in `.data`. ## * Column `degree` is not found. No dice! R tells us that it can’t find a column labeled “degree,” and indeed, there is no such variable in our data since it was not recorded. This illustrates that the variable in the parens in the group_by line can’t be just anything, it has to be the name of a variable (spelled exactly the same!) that exists in our data. Finally, let’s construct a frequency table using multiple variables at once. This lets us answer questions like, how many British women were aboard the Titanic? We can put multiple variables in the group_by line: titanic %&gt;% group_by(residence, sex) %&gt;% summarize(n = n()) ## `summarise()` has grouped output by &#39;residence&#39;. You can override using the `.groups` argument. ## # A tibble: 6 x 3 ## # Groups: residence [3] ## residence sex n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 American Female 108 ## 2 American Male 150 ## 3 British Female 94 ## 4 British Male 208 ## 5 Other Female 264 ## 6 Other Male 485 Now we can begin to address a few more questions. This time, you will have to figure out how to fiddle with our code for making frequency tables. Write code that will produce the frequency table below:2 ## `summarise()` has grouped output by &#39;class&#39;. You can override using the `.groups` argument. ## # A tibble: 6 x 3 ## # Groups: class [3] ## class survived n ## &lt;int&gt; &lt;lgl&gt; &lt;int&gt; ## 1 1 FALSE 123 ## 2 1 TRUE 200 ## 3 2 FALSE 158 ## 4 2 TRUE 119 ## 5 3 FALSE 528 ## 6 3 TRUE 181 This table breaks down the number of people who did or did not survive the sinking of the Titanic by their “class,” with first-class being the most expensive with the most amenities and third-class being the least expensive with the least amenities. Do you notice any patterns?3 1.3.2 Bar charts Trying to find patterns among six numbers in a frequency table is not impossible, but it’s also not easy. Bar charts make numerical relationships easy to see visually, so we don’t need to compare a bunch of numbers. Let’s see how even a simple comparison is easier with a bar chart than a frequency table. First, let’s make a frequency table for the number of passengers in each class:4 ## # A tibble: 3 x 2 ## class n ## * &lt;int&gt; &lt;int&gt; ## 1 1 323 ## 2 2 277 ## 3 3 709 Now, use the following code to instead construct a bar chart that displays the same information as the table, but in a visual form: titanic %&gt;% ggplot(aes(x = class)) + geom_bar() Pretty neat! It is now easy to see how much more 3rd class passengers there are than 1st or 2nd, and that interestingly, there are fewer 2nd class than 1st class passengers5. Notice that the code we used is similar to what we’ve been using, but differs in some important ways: The first line is the same, telling R what dataset we are using (titanic). The second line tells R that we want to make a plot and that we want to put the variable class along the horizontal axis of that plot (the x axis). The “gg” in front of “plot” refers to the “grammar of graphics,” which is language R uses to describe plots. In this language, different parts of a plot are called “aesthetics,” which is why x = class falls inside a parenthetical labeled aes(thetic). The final line just tells R that we want to make a bar chart. In the grammar of graphics, different types of charts are called geoms. Notice that the second 2 lines are connected by a + rather than our %&gt;% symbol. This is a historical accident, but the meaning of the two symbols is basically the same: they are telling R the order in which it should follow our instructions. 1.3.3 Histograms So far, we have been summarizing discrete variables. There are also some continuous variables in our data, for example the age of each passenger as well as how much they paid for their tickets. Let’s try making a frequency table to figure out how many people of different ages sailed on the Titanic: titanic %&gt;% group_by(age) %&gt;% summarize(n = n()) ## # A tibble: 99 x 2 ## age n ## * &lt;dbl&gt; &lt;int&gt; ## 1 0.167 1 ## 2 0.333 1 ## 3 0.417 1 ## 4 0.667 1 ## 5 0.75 3 ## 6 0.833 3 ## 7 0.917 2 ## 8 1 10 ## 9 2 12 ## 10 3 7 ## # … with 89 more rows Well that’s not very helpful! R didn’t even bother to show us the whole table. Though we can see something interesting: Age is measured in years, and for passengers at least one year old, their age is a whole number. But there are fractions of years for passengers less than a year old—these ages were measured in months rather than years. The main point is that even though age can be measured in a more or less fine-grained manner, it is effectively continuous. We don’t want to know how many passengers were exactly 31.3491 years old, we want to get a sense of whether there are more younger than older passengers, whether there might be different clusters or age groups, that sort of thing. In other words, we want to know something about how people are distributed across different age ranges. We can construct a summary that conveys this information using a histogram. This is very similar to a bar chart; the difference is that bar charts are for discrete variables while histograms are for continuous variables. The code below constructs a histogram to summarize passenger age: titanic %&gt;% ggplot(aes(x = age)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 263 rows containing non-finite values (stat_bin). The resulting histogram shows a bunch of bars, the height of which indicate the number of passengers within a particular age range.6 Notice that we got a couple messages from R in addition to our plot, one about “non-finite values” and another about “picking a better value.” When R says, “non-finite values,” it is talking about people for whom their age was not recorded. This is an unfortunate thing about real data: sometimes it has missing pieces. This didn’t stop R from making the plot we wanted using the non-missing data, but R wanted to warn us just in case. The message about “picking a better value” is important: When you make a histogram, you are looking at how many things fall within a particular range of values, say, between ages 4 and 8. How do you decide those ranges? If you don’t tell R how to do that, it will decide on its own to divide up the range of values into 30 “bins,” each of which corresponds to a range of values that is the same width. This is usually not what we want. Instead, we should decide how big or small we want those ranges to be. The following code tells R to make our histogram using “bins” that are 5 years “wide” (0-4, 5-9, 10-14, etc.): titanic %&gt;% ggplot(aes(x = age)) + geom_histogram(binwidth = 5) ## Warning: Removed 263 rows containing non-finite values (stat_bin). Here’s what it looks like when we set the width of each bin to be just 1 year7: ## Warning: Removed 263 rows containing non-finite values (stat_bin). And here’s what it looks like when we set the width of each bin to be 10 years8: ## Warning: Removed 263 rows containing non-finite values (stat_bin). Choosing different bin widths makes the resulting histogram look very different. There is not necessarily one “right” answer, so being able to quickly see the differences between different histograms makes R very handy.9 1.4 Wrap-up Today we began our adventure by using RStudio to explore some data. We saw how to look at data and how to summarize it in various helpful ways. These were frequency tables, bar charts, and histograms. Frequency tables count the number of times a particular value of a particular variable (or combination of values across multiple variables) occurs in our dataset. Bar charts display those counts in a visual form that makes it easier to compare frequencies. Histograms let us visually summarize counts of continuous variables by putting them in “bins,” the width of which we need to decide. For each variable in the dataset, say whether it is a) Discrete or Continuous; b) what scale it is measured on (nominal, ordinal, interval, or ratio); and c) what you think it means based on the name of the variable and/or what values it takes.↩︎ What code did you write to make a frequency table for number of people of each class who did or did not survive?↩︎ Were there more people in third class than first class? For each of the three classes, is the number of survivors more than the number who died?↩︎ What R code would produce this frequency table?↩︎ What about the bar chart conveys the same information as the frequency table? In other words, what is the visual aspect of the bar chart that represents the numbers in the table?↩︎ Compare the code we used to make a histogram to what we used to make a bar chart above. What is different between them?↩︎ What code would produce this plot?↩︎ What code would produce this plot?↩︎ What bin width do you think makes the best histogram for passenger ages? Why? Do you think different bin widths might be better in different circumstances?↩︎ "],["lab2.html", "Lab 2 Central tendency and variability 2.1 Simple cases 2.2 Dealing with real data 2.3 Wrap-up", " Lab 2 Central tendency and variability In this session, we will learn how to use R to calculate descriptive statistics. Specifically, these are statistics describing central tendency and variability. We saw how complicated it can be to calculate some of these things by hand. While it is good to understand what is going on “behind the scenes,” doing such calculations by hand is not only time consuming but prone to error. If you make a typo at one step, that little error gets magnified as you keep going. Besides, as we saw last week, most of the time, dataset are large and it is simply unreasonable to calculate a mean of 1300 numbers by hand. Luckily, computers are great at doing the kinds of mechanical, repetitive, mindless steps involved. But if the computer is great at mindless stuff, that means us people have to be better at mindful stuff. We need to think about what these numbers mean and why they may be important for understanding the world. Another part of computers being mindless is that they will always do exactly what you tell them to do, unless they can’t. So we have to be extremely careful when we write code because, just like making a mistake doing calculations by hand, a mistake in code can propagate into a mistake in the result. Think about your neighbor’s obnoxious dog—it is not the dog’s fault it is obnoxious, it is the fault of the neighbor for doing a poor job training her. Why is doing statistics with a computer any better, if we still have to worry about making a mistake? Because the computer is fast and can deal with a lot more data than we can. The primary skills developed in this activity are: Using R to calculate central tendency Median Mean Using R to calculate variability Variance Standard deviation 2.1 Simple cases 2.1.1 Central tendency Imagine that we are thinking about selling a new product. We demonstrate the product to five people in a focus group, each of whom gives us a rating of the product on a scale from 1 (very dissatisfied) to 9 (very satisfied)10. Their ratings, in no particular order, are 6, 9, 5, 5, and 7. Recall that the mean is defined by the formula \\(\\bar{X} = \\frac{\\sum_{i = 1}^N X_i}{N}\\), where each \\(X_i\\) is a measurement and \\(N\\) is the number of measurements. We can expand the summation \\(\\sum_{i = 1}^N X_i\\) into \\(X_1 + X_2 + X_3 + \\cdots + X_N\\), where the subscripts (1, 2, 3, etc.) indicate which of the measurements is being added to the total sum. We can use R to do this calculation by basically writing out in R how we would write it out if we were finding the mean by hand. Then we let R do the actual arithmetic. (6 + 9 + 5 + 5 + 7) / 5 ## [1] 6.4 We have basically turned R into a calculator. R can do all the typical things a calculator can do, like addition (+) and division (/), as well as subtraction (-) and multiplication (*). We used the parentheses to tell R to add up all the numbers before dividing them by 5. But that’s still pretty clumsy. R gives us a more elegant way to find the mean of a set of measurements: mean(c(6, 9, 5, 5, 7)) ## [1] 6.4 Nice! How did that work? If you tell R mean(), it will give you the mean of whatever is inside those parentheses. But what is that c() thing? The c() tells R to collect the numbers together. It tells R to treat them collectively; in our mathematical notation, the c() tells R that all the things inside those inner parentheses are to be treated like \\(X_i\\)’s, a set of measurements all on the same variable. Can we keep using this trick? Let’s use R to find the median: median(c(6, 9, 5, 5, 7)) ## [1] 6 Worked like a charm! Again, we had to tell R that our numbers were to be treated like a collection, and we told R to find the median of that collection of numbers. Now imagine we get a sixth person to rate our product and they give it an 8. Now our data consist of a set of six ratings (6, 9, 5, 5, 7, 8). We can again use R to find the new mean11 ## [1] 6.666667 and the median12 ## [1] 6.666667 Sadly, R does not give us a straightforward way to get the mode, so that’s something we still have to figure out on our own.13 2.1.2 Variability R helpfully provides similar methods for calculating variability. For example, we can calculate the variance of the focus group ratings like this var(c(6, 9, 5, 5, 7, 8)) ## [1] 2.666667 Notice that R abbreviates variance to just var, but otherwise the code looks basically the same as we used for the mean and median. When you ask R for the variance, you get the so-called “sample” variance, given by \\(\\frac{\\sum_{i = 1}^N \\left( X_i - \\bar{X} \\right)^2}{N - 1}\\). Similarly, we can get the standard deviation by swapping out var for sd (short for standard deviation) sd(c(6, 9, 5, 5, 7, 8)) ## [1] 1.632993 We can verify that this is the square root of the variance. R has another “function” called sqrt that gives you the square root of whatever is in the parentheses that follow it. Check it out: sqrt(var(c(6, 9, 5, 5, 7, 8))) ## [1] 1.632993 Notice that you can “wrap” things in parentheses and put function names in front, like we put the sqrt in front of the var. 2.2 Dealing with real data The examples above illustrated one way we can use R to calculate descriptions of central tendency and variability. These are useful when we have a small amount of data that we can type in without risk of error, but that’s almost never the situation we have to deal with in applying statistics to real data. 2.2.1 Setting the stage Before we can get to work with real data, we have to open up the right toolbox. In R, this is done by retrieving a “package” from R’s “library.” Remember that one of the nice things about R is that there are so many packages available that each contain helpful functions. Of course, the downside is that we need to tell R to open up those packages, but this isn’t too hard. Specifically, we’ll need to open the “tidyverse” package: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() R will tell you a bunch of stuff, including something about “conflicts.” This isn’t a big deal for us, R is just saying that we have some tools with the same name, and it calls this a “conflict.” R is just trying to keep you informed. The tidyverse package is one that we will be using a lot. It contains a lot of statistical tools and tools for making graphics that will be very useful for us throughout the course. 2.2.2 Getting the data The data we will be looking at come from a study by Yu et al. (1982). They studied the lifespan of rats with two different diets: One group of rats was allowed to eat freely, however they wanted; another group was fed on a restricted diet with only about 60% of the calories that the free-eating rats had. We need to get this data into R. The following code will import the data into your current R session: rats &lt;- read_csv(&#39;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv&#39;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Diet = col_character(), ## Lifespan = col_double() ## ) In your “Environment” panel in the upper right, you’ll see a new entry called “rats.” This is the data we just imported. Click on it to have a look at the data, which will appear in the upper left. There are just two variables in this dataset, Diet and Lifespan (measured in days)14. 2.2.3 Visualizing the data Numerical techniques for describing data, like means and variances, are useful compact descriptions of potentially complex distributions. But while they can be quick and easy ways to communicate (even if they are not always quick and easy to calculate), it is still important to get a sense of what the data really look like. As a result, we will never abandon our friends from last week, the bar chart and histogram. It is essential to visualize your data prior to doing any kind of quantitative analyses, since visualizing the data helps us understand what those quantities mean. Just like last week, we can construct a histogram to see how long different rats lived depending on their diet. rats %&gt;% ggplot(aes(x=Lifespan)) + geom_histogram(binwidth=60) So far, so good. The code above takes our data, puts the “Lifespan” variable along the \\(x\\) axis, and makes a histogram by putting the observed rat lifespans into “bins” that are 60 days wide.15 The histogram illustrates some interesting features of the data, notably that it appears to be bimodal16. Is this surprising? Remember that our dataset actually consists of two groups of rats with different diets, so maybe that is why the histogram looks the way it does. We can use a new “aesthetic” to split the histogram up so we can see the two different groups: rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, position = &quot;identity&quot;, alpha=0.5) There are a couple things that got added there, especially in that last line, that probably aren’t that clear. First, the new “aesthetic”: In the second line, we tell R to put Lifespan along the x axis like before, but now we are also telling R to fill the histograms with a different color depending on the Diet variable. Now, what about that other stuff in the third line? Now, in addition to telling R how wide the bins of our histogram should be (binwidth = 60), position = \"identity\" tells R that it should overlay the two histograms. If you don’t tell R to do that, it will instead “stack” them, like this: rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, alpha=0.5) Which is kind of confusing. What about alpha = 0.5? This gets a bit into computer graphics, but alpha represents the opacity of something. If alpha = 1, this means completely opaque, you can’t see anything through it. If alpha = 0, this means completely transparent (invisible). So alpha = 0.5 is halfway in between. Because we are putting the two histograms on top of one another, we want them to be a little bit see-through so one doesn’t obscure the other. See what happens when we leave out that line (and alpha = 1 by default): rats %&gt;% ggplot(aes(x=Lifespan, fill=Diet)) + geom_histogram(binwidth=60, position = &quot;identity&quot;) Again, harder to see what is going on. 2.2.4 Central tendency By plotting the lifespans of the two groups of rats on separate but overlapping histograms, we can see there are some important differences between them.17 Now we can describe those differences numerically. We saw last week how to summarize data in terms of counting the frequency with which different values were observed. The code to summarize the central tendency of a dataset is similar: rats %&gt;% group_by(Diet) %&gt;% summarize(M = mean(Lifespan)) ## # A tibble: 2 x 2 ## Diet M ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Free 684. ## 2 Restricted 969. And just like that, R has calculated the mean lifespans of the rats in each group.18 Looking at the code, the first two lines are similar to how we got R to make frequency tables last time: We tell R what data we are working with (rats), tell R to group the data by a particular variable (Diet), and then tell R to make us a summary. The last line is again a summarize instruction, but it looks a bit different. Last time, we put N = n() inside the parentheses, which told R to count the number of individuals in each group and put those counts in a column labeled N. By saying M = mean(Lifespan), we are telling R to find the mean lifespan of the rats in each group and put the result in a column labeled M. We replaced N with M and n() with mean(Lifespan). In so doing, we changed the label (N to M) and changed the way we summarized the data (from n(), which just counted, to mean(Lifespan) which finds the mean Lifespan). Just like we did with the simple data above, we can swap out mean in the last line for different summaries. For example, we can create a table that gives us the median lifespan of the rats in each group:19 ## # A tibble: 2 x 2 ## Diet M ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Free 710 ## 2 Restricted 1036. Comparing the mean to the median gives us a sense of the extent to which the distributions of lifespans of the two groups of rats are skewed.20 2.2.5 Variability We can also make a summary table with multiple descriptive statistics, including both measures of central tendency and of variability. The following code gives us a table that reports not just the mean lifespans of rats in each group, but the standard deviation of those lifespans as well: rats %&gt;% group_by(Diet) %&gt;% summarize(M = mean(Lifespan), SD = sd(Lifespan)) ## # A tibble: 2 x 3 ## Diet M SD ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Free 684. 134. ## 2 Restricted 969. 285. And, of course, we can create a table that summarizes the mean, median, standard deviation, and variance all at once!21 ## # A tibble: 2 x 5 ## Diet Mean Median Variance SD ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Free 684. 710 17979. 134. ## 2 Restricted 969. 1036. 80986. 285. These numerical summaries of central tendency and variability provide a concise description of the lifespans of rats under the two different diets. This description helps us see whether there is any benefit to a restricted diet on lifespan.22 2.3 Wrap-up Today we saw how to use R to find various numerical descriptive statistics, both in simple cases and with real data. We focused on two measures of central tendency, the mean and the median, as well as two measures of variability, the variance and the standard deviation. We saw how these quantities are useful in summarizing complex data, making it easier to draw conclusions about differences between groups. We also learned a new trick about how to visualize histograms from multiple groups. Is this measure continuous or discrete? What is the scale of measurement?↩︎ What is the code that calculates the new mean? Hint: Try modifying the collection of observed values within the parentheses in the code above.↩︎ What is the code that calculates the new median? Hint: Try modifying the collection of observed values within the parentheses in the code above.↩︎ Just by looking at the observed values we have, what is the mode of the focus group’s ratings?↩︎ For each of these variables, are they a) continuous or discrete? b) which scale are they measured on (nominal, ordinal, interval, or ratio)?↩︎ Compare the code we just used with the code we used to make a histogram of the ages of Titanic passengers last week. What is different and what is similar?↩︎ Based on the histogram, roughly where do the two modes seem to be?↩︎ Based on the histogram, does it seem like one group of rats tends to live longer the the other? Do the groups seem to have similar amounts of variability?↩︎ Which group has the higher average lifespan? Does this make sense based on what you saw in the histogram?↩︎ What code would produce this table? Hint: it requires modifying the name of the central tendency measure in the last line of the previous bit of code.↩︎ Compare the median lifespan to the mean lifespan in each group. Based on this comparison, would you say that lifespans in either group are symmetric, positively skewed, or negatively skewed?↩︎ What code would produce the table below? Hint: See how we added a column for the standard deviation? And remember how we told R to find different descriptive statistics above?↩︎ What would you conclude? Is there a benefit or harm for the restricted diet over normal free diet? Is this true for every rat? Is the size of any benefit or harm big or small relative to how much rat lifespans tend to vary?↩︎ "],["lab3.html", "Lab 3 Scatterplots and correlation 3.1 Calculating the Pearson correlation coefficient 3.2 Scatterplots 3.3 Some bigger data 3.4 Wrap-up", " Lab 3 Scatterplots and correlation So far, most of what we’ve done has been about describing measurements on one variable at a time. Today’s session will look at how we can explore relationships between variables. If there is a systematic relationship between two variables, they are said to be correlated. The primary skills developed in this activity are a visual and a numerical way to explore relationships between variables: The visual way is called a scatterplot. The numerical way is the Pearson correlation coefficient. Before we begin, make sure you have opened the tidyverse package from the library: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 3.1 Calculating the Pearson correlation coefficient We saw last time that one way R is useful is as a powerful calculator. Calculating things like standard deviations by hand is hard enough, but calculating the Pearson correlation coefficient requires doing that twice (once for each variable), in addition to a lot of other steps. R can help ease that burden. 3.1.1 From \\(X\\) to \\(z\\) Recall that the Pearson correlation coefficient can be found using \\(z\\) scores. Specifically, if we have measurements on one variable—let’s call them \\(X\\)—and measurements on another variable—let’s call them \\(Y\\)—we can find the correlation by first finding the \\(z\\) scores for these two sets of measurements. 3.1.1.1 R can remember things Let’s try doing that in R. Remember that R uses c() to say that everything within the parentheses is supposed to be treated as a collection of measurements on a single variable. Rather than having to write out that collection every time we want to use it, we can tell R to remember it. Let’s say we have a collection of measurements and we want to tell R to remember them with the label “X.” We can do that this way: X &lt;- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5) The important thing to note in the line above is the arrow &lt;-. The arrow tells R to take the stuff on the right and put it under the label on the left. You can think of the arrow as saying that the collection c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5) is now “inside” a box labeled X. Now if we write “X” at the console and hit enter, R will remind us what we put inside the “X” box: X ## [1] 10 8 13 9 11 14 6 4 12 7 5 We can also find a specific value using square brackets. For example, we can find the 3rd measurement this way: X[3] ## [1] 13 Nice!23 This is just like how we used \\(X_3\\) as a shorthand for the 3rd measurement when we were doing things by hand.24 3.1.1.2 Using labeled collections Now that R can remember that whole collection of measurements, we can use it to do calculations. For example, this will find the mean of all the numbers we put under the label “X”: mean(X) ## [1] 9 And this will find the standard deviation: sd(X) ## [1] 3.316625 Finally, we can combine different operations into a single line of code. The following will convert each of the measurements in X into \\(z\\) scores: (X - mean(X)) / sd(X) ## [1] 0.3015113 -0.3015113 1.2060454 0.0000000 0.6030227 1.5075567 ## [7] -0.9045340 -1.5075567 0.9045340 -0.6030227 -1.2060454 We can even tell R to remember those \\(z\\) scores like so: z_X &lt;- (X - mean(X)) / sd(X) Note that we gave these \\(z\\) scores a special label, “z_X,” so that we know they originally came from our collection labeled X. Now if we ask R to tell us what we put under the “z_X” label, we’ll get our \\(z\\) scores back z_X ## [1] 0.3015113 -0.3015113 1.2060454 0.0000000 0.6030227 1.5075567 ## [7] -0.9045340 -1.5075567 0.9045340 -0.6030227 -1.2060454 3.1.2 Finding the correlation coefficient 3.1.2.1 Introducing a new set of measurements Now that we’ve seen how to tell R to remember different collections of values, let’s add another variable to the mix and find its correlation with \\(X\\). We will call this variable \\(Y\\) to keep it distinct. Y &lt;- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68) Let’s see what the mean of \\(Y\\) is25 ## [1] 7.500909 as well as its standard deviation26 ## [1] 2.031568 Finally, let’s find the \\(z\\) scores for each of our \\(Y\\) values and tell R to remember them with the label z_Y27. If we’ve done our job right, we should get the following output when we ask R to remember what we put into z_Y: z_Y ## [1] 0.26535704 -0.27117431 0.03893097 0.64437460 0.40810391 1.21043979 ## [7] -0.12842744 -1.59527462 1.64360272 -1.31962549 -0.89630717 3.1.2.2 Calculating correlation from \\(z\\) scores To get the correlation coefficient between \\(X\\) and \\(Y\\) from our \\(z\\) scores, we need to multiply the \\(z\\) scores together, add them up, and divide by the number of values minus one. We can tell R to multiply together all our pairs of \\(z\\) scores like this z_X * z_Y ## [1] 0.08000816 0.08176213 0.04695251 0.00000000 0.24609592 1.82480664 ## [7] 0.11616699 2.40496698 1.48669460 0.79576411 1.08098712 We can add them up like this, by telling R to find the sum sum(z_X * z_Y) ## [1] 8.164205 Finally, we need to divide by \\(N - 1\\). \\(N\\) in this case is 11, so \\(N - 1 = 11 - 1 = 10\\). sum(z_X * z_Y) / 10 ## [1] 0.8164205 And that’s the Pearson correlation coefficient!28 3.1.2.3 Calculating correlation directly This is what you might call the “shaggy-dog” way of calculating the correlation, in that it takes a long time to get to the punchline. Fortunately, R gives us an even easier way! The correlation between \\(X\\) and \\(Y\\) can be found like this cor(X, Y) ## [1] 0.8164205 cor tells R to find the correlation between the two sets of measurements labeled X and Y. Remember that R knows what we mean by X and Y only because we already told R what they stood for. To get a sense of how that works, let’s replace our old Y with a new one: Y &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11) Now if we ask for the correlation again, it will be different29 cor(X, Y) ## [1] -0.4272727 3.1.3 Dealing with real data The steps above are useful when you’ve got relatively few measurements that you can copy-and-paste into the R console. Real data is often more complicated, so let’s see how we can deal with it. 3.1.3.1 Get some data First, we need to import some data into R using the code below. anscombe &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Group = col_character(), ## X = col_double(), ## Y = col_double() ## ) These data are artificial and were devised by Anscombe (1973) to illustrate the importance of not interpreting a correlation coefficient without visualizing your data. 3.1.3.2 Correlations for different sets of measurements There are four groups of measurements in these data. The groups are labeled “A,” “B,” “C,” and “D.” Measurements are on two variables, labeled “X” and “Y.” Click on anscombe in the environment panel in RStudio (upper right) to take a look. We can quickly find the correlation coefficient for each group of “X” and “Y” measurements30 anscombe %&gt;% group_by(Group) %&gt;% summarize(r = cor(X, Y)) ## # A tibble: 4 x 2 ## Group r ## * &lt;chr&gt; &lt;dbl&gt; ## 1 A 0.816 ## 2 B 0.816 ## 3 C 0.816 ## 4 D 0.817 Notice that the correlation coefficients are almost all the same for each group. But now let’s try making some scatterplots to see if these correlations make sense. 3.2 Scatterplots As we’ve seen, a scatterplot lets us see how measurements on two variables are related to one another. It does this with a bunch of dots “scattered” around the plot. Each dot represents a single individual or object. The horizontal position of the dot represents that individual’s observed value on one variable. The vertical position of the dot represents that same individual’s observed value on a different variable. 3.2.1 Making a group of scatterplots R makes it easy not just to make a single scatterplot, but an entire group of them at once. The code below produces a set of scatterplots, one for each of the four groups, that puts the “X” measurements on the horizontal and the “Y” measurements on the vertical31: anscombe %&gt;% ggplot(aes(x=X, y=Y)) + geom_point() + facet_wrap(&quot;Group&quot;) Even though the correlation coefficient is the same for each group, the scatterplots all look very different!32 3.3 Some bigger data Now that we’ve seen how to find correlation coefficients and make scatterplots, let’s apply these tools to some real data. These data come from a study by Ryan &amp; Gauthier (2016), who studied differences in how well people can remember different kinds of objects. The point of this is to understand how we develop the ability to make fine visual distinctions between some things (e.g., between different peoples’ faces) but not others (e.g., leaves or snowflakes). They reasoned that the more experience someone has in dealing with a particular category of things, the better they’ll be at noticing and remembering the visual features that distinguish between members of that category. Two of those categories were different types of toys: Barbies and Transformers. They measured how well their participants could tell apart pictures of Barbies that they had seen from pictures of Barbies they hadn’t seen. They did the same for pictures of Transformers. The final memory scores are a number correct out of 48. Finally, each participant indicated on a scale from 1 to 9 how much experience they had with each type of toy. The question is, “is experience with a type of toy correlated with better memory?” 3.3.1 Import the data To import the data into R, use the line below toys &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/toys.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Barbie_Score = col_double(), ## Transf_Score = col_double(), ## Barbie_Exp = col_double(), ## Transf_Exp = col_double() ## ) Click on the new “toys” dataset in the Environment panel (upper right) to see what the data look like. “Barbie_Score” and “Transf_Score” are the scores on the two memory tests, one for Barbies and one for Transformers (respectively). “Barbie_Exp” and “Transf_Exp” are the two reports of prior experience with Barbies and Transformers, respectively. 3.3.2 Experience and memory for Barbies Let’s first make a scatterplot showing Barbie experience on the horizontal axis and Barbie memory on the vertical axis. We can do that using this code: toys %&gt;% ggplot(aes(x=Barbie_Exp, y=Barbie_Score)) + geom_point() This gives us a sense of what might be going on.33 Now let’s calculate the Pearson correlation coefficient (notice that there are no groups here, so we don’t need a group_by line)34: toys %&gt;% summarize(r = cor(Barbie_Exp, Barbie_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 0.142 3.3.3 Experience and memory for Transformers Now let’s do the same thing for Transformers. Here’s the scatterplot:35 And this is the correlation coefficient between Transformers Experience and Transformers memory score:36 toys %&gt;% summarize(r = cor(Transf_Exp, Transf_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 0.0964 It seems like the relationship between experience and memory is a bit weaker for Transformers than for Barbies. But there’s another interesting correlation here. 3.3.4 Experience with Barbies and memory for Transformers Let’s see if there’s a relationship between experience with Barbies and memory for Transformers. toys %&gt;% ggplot(aes(x=Barbie_Exp, y=Transf_Score)) + geom_point() The scatterplot is shown above and this gives the Pearson correlation coefficient: toys %&gt;% summarize(r = cor(Barbie_Exp, Transf_Score)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 -0.124 Interesting! Even though there is at best a weak relationship between Transformers experience and Transformers memory, there is actually a stronger relationship between Barbie experience and Transformers memory, just a negative one!37 3.4 Wrap-up In today’s session, we saw how to find \\(z\\) scores and correlation coefficients in R. We also used R to make scatterplots and saw an example of why it is critical to look at a scatterplot before interpreting the correlation coefficient. How would you get R to tell you what the first measurement was?↩︎ What do you think would happen if we asked for the 12th measurement? Try it and see! Hint: how many measurements do we have?↩︎ What code will find the mean of \\(Y\\)? Hint: how did we find the mean of \\(X\\)?↩︎ What code will find the standard deviation of \\(Y\\)? Hint: how did we find the standard deviation of \\(X\\)?↩︎ What code would you write to do this? Hint: what code did we use to calculate and remember the \\(z\\) scores for \\(X\\)?↩︎ Is the correlation positive or negative? Does it seem strong or weak?↩︎ Why do we get a different result from cor(X, Y) even though the code itself is the same?↩︎ Compare the code we used to find these correlations with the code we used last time to find the means of the different groups of rat lifespans. What is similar and what is different?↩︎ What do you think the function of the line facet_wrap(\"Group\") is in the code below?↩︎ For each group, say whether you think the correlation coefficient provides a fair description of the relationship between the two variables. Why or why not?↩︎ Based on this scatterplot, does it seem like there might be a correlation between Barbie experience and visual memory for Barbies? If so, is it positive or negative? Weak or strong?↩︎ Do the sign and magnitude of the correlation coefficient make sense based on the scatterplot?↩︎ What code would produce this scatterplot? Hint: what code did we use to make the Barbie scatterplot? What variables in the dataset refer to Transformers rather than Barbies?↩︎ What code would find this correlation? Hint: what code did we use to find the Barbie correlation?↩︎ Can you think of any reasons why there might be a negative correlation between Barbie experience and Transformers memory?↩︎ "],["lab4.html", "Lab 4 Probability and simulation 4.1 Simulation basics 4.2 Estimating probabilities 4.3 More advanced simulation tricks 4.4 Estimating probabilities in data 4.5 Wrap-up", " Lab 4 Probability and simulation We have seen that there are two senses in which we can think about probability. On the one hand, we can think of the probability of an outcome as its long-run relative frequency. We can therefore estimate the probability that an outcome will happen by seeing how often it occurred in the past, relative to all the other possible outcomes in our sample space. But on the other hand, we can think of probability as a degree of belief about how likely something is to happen. This second perspective lets us think about probabilities for events that only happen once; until the event happens, all the possible outcomes have some probability of occurring. We can imagine rewinding the clock and seeing how often an event may have turned out differently. Actually, with computers, we can do better than just imagine what might happen. We can use the computer to simulate many repetitions of an event. Before we begin, make sure you have opened the tidyverse package from the library: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 4.1 Simulation basics To begin, let’s think about the simple scenario of flipping a coin. The sample space consists of two possible outcomes, “Heads” and “Tails.” 4.1.1 Single coin flips We can simulate a single coin flip in R using a “function” called sample: c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1) ## [1] &quot;Tails&quot; The first line in the code above tells R our sample space, which is a collection of two outcomes, “Heads” and “Tails.” We use the %&gt;% to tell R to take that sample space and, on the next line, take a sample of size 1 from that space. By “sample of size 1,” that’s just a precise way of saying “a single flip.” Try running that same bit of code a few more times, so we can see that R is really simulating coin flips—you don’t know ahead of time whether it will come up heads or tails.38 c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1) ## [1] &quot;Heads&quot; 4.1.2 A sequence of coin flips It is kind of fun to keep flipping our simulated coin, but it would take a long time to do this repeatedly. As you might have guessed, we can change the size of our sample to quickly give us a long sequence of coin flips. Let’s try that now and see if we can get a sequence of 10 flips. c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 10) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; We got an error! Why is that? It’s because, by default, when you tell R to draw samples, it does so without replacement. In other words, once it draws a sample from the sample space, whatever outcome it sampled doesn’t get put back into the space. This wasn’t a problem when we ran the single-sample code multiple times because we told R to start with a “fresh” sample space each time—that’s what the first line c(\"Heads\", \"Tails\") %&gt;% means. Luckily, this is easily fixed by just telling R to sample with replacement like so: c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 10, replace = TRUE) ## [1] &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; ## [10] &quot;Tails&quot; Just like R gave us “fresh” single flips, if we run the 10-flip code again, we will probably get a different sequence of outcomes: c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 10, replace = TRUE) ## [1] &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; ## [10] &quot;Tails&quot; And for the computer, doing 50 flips is not much harder than just 1039: ## [1] &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; ## [10] &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; ## [19] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; ## [28] &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; ## [37] &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; ## [46] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; 4.1.3 Remembering simulated results The problem with simulating so many outcomes is that now it is hard to keep track of them! And, as we’ve seen, we can’t just copy-and-paste the same line of code each time we want to refer back to our simulated outcomes, because that will produce a new sequence of outcomes. Instead, let’s tell R to remember the outcomes we simulated. result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 50, replace = TRUE) All we did was add result &lt;- to the beginning of our code. Like we saw last time, this will let R remember the particular sequence of outcomes under the label “result.” If we type “result” at the console, we’ll see that R looks back in its memory and tells us how that sequence of 50 flips turned out. result ## [1] &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; ## [10] &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; ## [19] &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; ## [28] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; ## [37] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; ## [46] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; But if we simulate a new sequence of flips and tell R to remember it under the label “result,” it will forget the original sequence: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 50, replace = TRUE) result ## [1] &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; ## [10] &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; ## [19] &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; ## [28] &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; ## [37] &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; ## [46] &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; 4.1.4 Turning simulated results into “data” Now that we can simulate long sequences of outcomes and get R to remember them, we are going to get R to treat those outcomes as if they were “data,” in other words, as if they were from a real coin instead of a simulated one. R keeps data in a special form called a “tibble.” We’ve seen this cutesy form of the word “table” before; every time we used R to summarize a dataset, it told us the result was a “tibble.” We can create a “tibble” from our simulated flips like this coin_data &lt;- tibble(result) We’ve told R to take our flips, turn them into a tibble, and remember the result under the label “coin_data.” Now when we ask R to remember “coin_data,” we see it looks a bit different coin_data ## # A tibble: 50 x 1 ## result ## &lt;chr&gt; ## 1 Tails ## 2 Tails ## 3 Heads ## 4 Tails ## 5 Heads ## 6 Tails ## 7 Heads ## 8 Heads ## 9 Heads ## 10 Tails ## # … with 40 more rows 4.2 Estimating probabilities Now that R can treat our simulated flips like it would “real” data, we can summarize it just like we would “real” data. This will allow us to estimate the probabilities of different outcomes from their relative frequency. 4.2.1 Frequency table The first thing we want to know is how many flips were heads and how many were tails. coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) ## # A tibble: 2 x 2 ## result n ## * &lt;chr&gt; &lt;int&gt; ## 1 Heads 26 ## 2 Tails 24 The summary above is a frequency table, which we have used before.40 4.2.2 Estimating probability from frequency Let’s use this frequency table to estimate the probability of getting either a heads or a tails. To do this, we need to add a line to the code above: coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 26 0.52 ## 2 Tails 24 0.48 The final line is called mutate because we are “mutating” the frequency counts to produce an estimate of the probability. We get that estimate (p) by taking the frequency of each outcome, n, and dividing it by the total number of outcomes (sum(n)). Let’s see how this works by simulating a longer sequence of coin flips (1000 flips) and replacing our old results: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1000, replace = TRUE) coin_data &lt;- tibble(result) coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 520 0.52 ## 2 Tails 480 0.48 Now let’s try it with an even longer sequence of 10000 flips.41 ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 4999 0.500 ## 2 Tails 5001 0.500 4.3 More advanced simulation tricks So far, we have just been simulating flips of a fair coin. Now let’s see how we can simulate more complex situations. 4.3.1 Making up our own probabilities By default, when you ask R to sample, it will assume that all the outcomes in the sample space have equal probability. That’s why when we asked R to simulate coin flips, heads and tails came up about equally often. Let’s mess with this and make ourselves a simulated unfair coin. The code below simulates 1000 flips of a coin that comes up heads with probability 0.6, rather than 0.5: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1000, replace = TRUE, prob = c(0.6, 0.4)) coin_data &lt;- tibble(result) coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 609 0.609 ## 2 Tails 391 0.391 Notice the main difference is that we added another part to the sample line. By adding prob = c(0.6, 0.4), we told R that the probability of the corresponding outcomes in the sample space (c(\"Heads\", \"Tails\")) should be 0.6 and 0.4, respectively. If we reverse the order of the probabilities, notice that “Tails” now comes out ahead: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1000, replace = TRUE, prob = c(0.4, 0.6)) coin_data &lt;- tibble(result) coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 398 0.398 ## 2 Tails 602 0.602 The first rule is that the first number in the collection of probabilities (after prob) gives the probability of the first outcome in the sample space, the second number gives the probability of the second outcome in the sample space, and so on. Now let’s try simulating 1000 flips where the probability of heads is 0.7:42 ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 672 0.672 ## 2 Tails 328 0.328 Notice that the relative frequency of the different outcomes is always close to the probabilities we give to R. 4.3.2 Expanding the sample space In addition to messing with the probabilities, we can mess with the sample space. So far, we have been assuming that the coin can only come up heads or tails. What if the coin could land on its side? This would introduce a new possible outcome to our sample space. We need to change the first line of our simulation code to reflect this: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;, &quot;Side&quot;) %&gt;% sample(size = 1000, replace = TRUE) coin_data &lt;- tibble(result) coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 3 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 338 0.338 ## 2 Side 342 0.342 ## 3 Tails 320 0.32 Notice that the code above did not specify any probabilities. As a result, all three outcomes in our sample space ended up happening about equally often. Let’s specify those probabilities to try and get the coin to come up on its side with probability 0.1, but otherwise come up heads or tails with equal probability and simulated 1000 flips. The result will look something like this:43 ## # A tibble: 3 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 464 0.464 ## 2 Side 100 0.1 ## 3 Tails 436 0.436 4.4 Estimating probabilities in data By now we are probably sick of coin flipping. Let’s take a break and watch some basketball. Specifically, let’s watch Kobe Bryant of the LA Lakers as they played against the Orlando Magic in the 2009 NBA finals. Use the following line of code to download a dataset consisting of every shooting attempt Kobe made that game, including whether or not it went in (i.e., was the shot a “Hit” or a “Miss”): kobe &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/kobe.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## vs = col_character(), ## game = col_double(), ## quarter = col_character(), ## time = col_time(format = &quot;&quot;), ## description = col_character(), ## shot = col_character(), ## prev_shot = col_character() ## ) 4.4.1 What was Kobe’s hit rate? The first question we should ask is how often Kobe’s shots go in (called his “field goal percentage”). We can modify the code we used to estimate probabilities from our simulated coin flips to do the same for Kobe’s real shot attempts:44 kobe %&gt;% group_by(shot) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## shot n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 H 57 0.432 ## 2 M 75 0.568 4.4.2 A simulated Kobe Using the estimates above, let’s see if we can construct a simulated Kobe Bryant that, like our simulated coin, will have roughly the same probability of making a basket as Kobe did. To do this, we need to specify our sample space (c(\"H\", \"M\"), where “H” is for “hit” and “M” is for “miss”) and our probabilities (e.g., prob = c(0.5, 0.5) if we think hits and misses are equally likely). Since this is a simulated Kobe, we can make him attempt 1000 shots (or more): result &lt;- c(&quot;H&quot;, &quot;M&quot;) %&gt;% sample(size = 1000, replace = TRUE, prob = c(0.5, 0.5)) simulated_kobe_data &lt;- tibble(result) simulated_kobe_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 H 520 0.52 ## 2 M 480 0.48 Does this simulated Kobe look much like the real one? What should we change to make it look more like the real Kobe?45 4.4.3 Hot- or cold-handed Commentators at this particular game remarked that Kobe seemed to have a “hot hand.” In other words, they seemed to believe that once Kobe made a basket (“H”), he was more likely to make a basket on his next shot. This is a probability question! Specifically, it is a question about conditional probability. Conditional on whether Kobe’s previous shot was a hit or a miss, what is the probability that his next shot is a hit or a miss? We can address this question by modifying the code we used above to estimate probabilities: kobe %&gt;% group_by(prev_shot, shot) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## `summarise()` has grouped output by &#39;prev_shot&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 4 ## # Groups: prev_shot [2] ## prev_shot shot n p ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 H H 21 0.368 ## 2 H M 36 0.632 ## 3 M H 36 0.48 ## 4 M M 39 0.52 Here, prev_shot refers to whether Kobe’s previous shot was a hit (“H”) or a miss (“M”). By adding prev_shot to the grouping in our second line, what we have done is tell R to count the number of times each combination of hit/miss on the previous shot and hit/miss on the current shot occurred.46 As a result, when we ask R in the final line to give us probabilities, it gives us the probability of a hit or miss on the current shot conditional on whether the previous shot was a hit or a miss. So, were the commentators right?47 4.5 Wrap-up In this activity, we saw how we could use R to simulate outcomes of many repeated events. We could specify the sample space as well as the probability with which different outcomes could occur. We saw how the long-run relative frequency got close to these probabilities. Finally, we saw how we could use R to estimate probabilities from frequencies of different outcomes, including conditional probabilities. How many times did you have to run the coin-flip simulation before you got at least one “Heads” and at least one “Tails?”↩︎ What code would produce a sequence of 50 flips at once? Hint: what is the number we had to change in the sample line to get 10 flips?↩︎ Compare the code we just used to produce a frequency table with the code we used to make frequency tables in Lab 1. What is similar and what is different?↩︎ What would you change in the code above to make it produce 10000 flips? Does it seem like the estimated probabilities are closer to 0.5 for both heads and tails with more flips?↩︎ How would you change our coin flipping code to do this? Hint: remember that probabilities should always add up to 1.↩︎ What would you add to the simulation code to make the probability of heads, tails, and side come out this way? Hint: Remember how we specified the probabilities for the unfair coin, and that all three probabilities should add up to 1.↩︎ Compare the code below to the code we used above to estimate probabilities from our simulated coin flips. What is similar and what is different?↩︎ Play around with the probabilities in the code for simulating shots from Kobe Bryant until you find some probabilities that make his hit rate look similar to what we actually observed. What probabilities did you end up with?↩︎ Compare the code we just used with the code we used to get the number of passengers who did or did not survive on the Titanic in each class. What is similar and what is different?↩︎ Did Kobe have a “hot hand?” Did he have a “cold hand,” where missing a shot meant it was more likely he would miss his next shot? Was there something else going on? Or was there no particular difference depending on whether he made or missed his previous shot?↩︎ "],["lab5.html", "Lab 5 Models: The Binomial and the Normal 5.1 The binomial distribution in R 5.2 The normal distribution in R 5.3 Wrap-up", " Lab 5 Models: The Binomial and the Normal One of the ways we use statistics to learn about the world beyond our data is by testing hypotheses. A statistical hypothesis takes the form of a model. The model allows us to figure out the probability of observing particular types of outcomes. We can then take the types of outcomes we actually observed in our data and see whether they have high or low probability according to our model. If they have a high probability, our hypothesis might be reasonable. But if they have a low probability, there might be something wrong about our hypothesis. In this session, we will explore two types of model that we have been discussing in class, the binomial distribution and the normal distribution. Similar to our last session, we will see how we can simulate data according to these models and show that, as we simulate more data (“in the long run…”), the relative frequency of different outcomes converges on the probabilities we get from the model. As we’ve seen, we can use these distributions to model different scenarios by adjusting their parameters. These are the numbers that we feed into the distribution to get probabilities. For the binomial distribution, its parameters are the probability of an outcome, which we called \\(\\theta\\) in math language, but which we will call prob in R; and the number of repetitions, which we labeled \\(N\\) in math language, but which we will call size in R. For the normal distribution, its parameters are the mean, which we called \\(\\mu\\) in math language, but which we will call mean in R; and the standard deviation, which we called \\(\\sigma\\) in math language, but which we will call sd in R. Before we begin, let’s make sure we have the tidyverse package loaded from our library library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 5.1 The binomial distribution in R Last time, we used R to simulate repeated outcomes from various kinds of events. Some of these, like flipping a coin or Kobe making a shot, had two possible outcomes. As a result, we can think of the number of times a particular outcome occurred as coming from a binomial distribution. 5.1.1 Translating from sample to rbinom Let’s see how we can do some of the stuff we did last time using the binomial distribution instead of directly simulating the outcome of each event. This is the code we used last time to simulate 1000 flips from a coin that had a probability of 0.6 of coming up heads: result &lt;- c(&quot;Heads&quot;, &quot;Tails&quot;) %&gt;% sample(size = 1000, replace = TRUE, prob = c(0.6, 0.4)) coin_data &lt;- tibble(result) coin_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## result n p ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Heads 619 0.619 ## 2 Tails 381 0.381 We can use the number of flips (1000) and the probability of heads (0.6) as the parameters of the binomial distribution. R has several important distributions “built in,” including the binomial and normal. R provides functions that allow us to directly sample from those distributions without having to use the sample function explicitly. This is how we can simulate the number of heads out of 1000 flips, where the probability of “heads” is 0.6, using R’s built-in binomial distribution: rbinom(n = 1, prob = 0.6, size = 1000) ## [1] 611 It is definitely more concise than our old code!48 Notice that we had to add a thing n = 1, what’s up with that? Let’s try changing it to see what it does and maybe we’ll understand what n means here: rbinom(n = 2, prob = 0.6, size = 1000) ## [1] 575 592 Okay, so n is actually the number of sequences of events to simulate. In other words, we have two “numbers of things” here: size is the number of repeated events in a single sequence; n is the number of sequences of events. Why is it important to distinguish between these two numbers? Let’s try an example to see. 5.1.2 Outcomes from different sequences of events The Mets and the Yankees are well known rival baseball teams from New York City. On a semi-regular basis, these two teams play so-called “subway series,” which are sequences of six games played against each other. Imagine that the probability that the Yankees win any single game against the Mets is 0.57. There have been 8 “subway series” played so far as part of the regular season. So in this scenario, the number of repeated event in a single sequence is 6, corresponding to the 6 games played in each subway series. The number of sequences is 8, corresponding to the 8 subway series that have been played. And the probability of a Yankees win in any single game is 0.57. We can use this to simulate the number of games won by the Yankees in each of the 8 subway series: rbinom(n = 8, prob = 0.57, size = 6) ## [1] 3 3 2 4 3 5 2 3 Each number that R gave us is a number of games won by the Yankees in a sequence of 6 games against the Mets. So the first number is the number of Yankees wins in the first subway series, the second number is the number of Yankees wins in the second subway series, and so on. Of course, if we run the simulation again, we will probably get a different set of outcomes for each subway series: rbinom(n = 8, prob = 0.57, size = 6) ## [1] 4 6 4 4 2 1 3 5 5.1.3 Simulated relative frequencies and probabilities Now let’s imagine playing a large number of subway series over the next several decades or centuries, say, 1000 of them. Like last time with 1000 coin flips, this is too many to be able to see all at once. And because this is a simulation, we can’t just run the same line of code again and expect it to give us the same result. So let’s get R to remember the outcomes of 1000 subway series. result &lt;- rbinom(n = 1000, prob = 0.57, size = 6) Now let’s do the same trick we did last time so we can get R to treat our simulated results as data. We turn our simulation into a “tibble”: subway_series &lt;- tibble(result) Finally, let’s modify our code for making a frequency/probability table above to take a look at the distribution of outcomes for the 1000 subway series we simulated49: subway_series %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 7 x 3 ## result n p ## * &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 7 0.007 ## 2 1 42 0.042 ## 3 2 176 0.176 ## 4 3 288 0.288 ## 5 4 290 0.290 ## 6 5 164 0.164 ## 7 6 33 0.033 Sometimes the outcome is a tie (3 games out of 6), sometimes the Yankees win the majority of games, and sometimes the Mets win the majority (when the Yankees win fewer than 3 games). The important thing to note is that these are probabilities of a number of wins out of a sequence of games, not the probability of winning any single game. In other words, the sample space is not just win/lose, it is the number of wins out of 6 games50. 5.1.4 Calculating binomial probabilities directly When we were simulating outcomes last time, we directly specified the probability of each outcome in the sample space. Now, what we specify are parameters, namely, the probability of an outcome in a single event and the number of repetitions of the event. So if we want to know the probability of a specific outcome, like 3 wins out of 6, we need to use the binomial distribution to figure it out. R lets us do this for single outcomes with the dbinom function. For example, let’s find the probability of a tie51 dbinom(x = 3, prob = 0.57, size = 6) ## [1] 0.2944828 As we would expect, this is close to the relative frequency of a tie when we simulated 1000 subway series. The meaning of probability is the same—it is the long-run relative frequency—it’s just that we are asking about the probability of a different type of outcome. We can also ask what is the probability that the Yankees win all six games52 ## [1] 0.03429645 5.1.5 Cumulative probabilities with the binomial Another part of R’s built-in binomial distribution is the ability to calculate cumulative probabilities. For example, we might be interested in the probability that the Mets win a majority of the games in a subway series. This corresponds to the Yankees winning either 0, 1, or 2 games. One way to calculate this would be to add up each of those individual probabilities: dbinom(x = 0, prob = 0.57, size = 6) + dbinom(x = 1, prob = 0.57, size = 6) + dbinom(x = 2, prob = 0.57, size = 6) ## [1] 0.2232135 But a more efficient way to do this is to use cumulative probabilities. As we’ve seen, a cumulative probability is just the sum of the probabilities for all outcomes up to and including a given value. We can get the same result that we got above using this more compact bit of code: pbinom(q = 2, prob = 0.57, size = 6) ## [1] 0.2232135 This makes it easier to find the probabilities for ranges of different outcomes. For example, we could get the probability that the Yankees either lose the series or tie it up (so 0, 1, 2, or 3 wins out of 6)53: ## [1] 0.5176963 But what if the opposite happens? What if the Yankees neither lose nor tie? That means they must have won the majority of games in the series (i.e., 4, 5, or 6 games). We could figure out the probability that the Yankees win the majority of games by adding up those individual probabilities like so: dbinom(x = 4, prob = 0.57, size = 6) + dbinom(x = 5, prob = 0.57, size = 6) + dbinom(x = 6, prob = 0.57, size = 6) ## [1] 0.4823037 But using cumulative probability, we can reverse that question: What is the probability that the Yankees don’t win 3 or fewer games? In other words, how much probability is “left over” after asking whether the Yankees win 3 or fewer games? Since the probability across all outcomes has to add up to one, we get 1 - pbinom(q = 3, prob = 0.57, size = 6) ## [1] 0.4823037 Using the same logic, we can ask what is the probability that the Yankees win 5 or more games54: ## [1] 0.189533 5.1.6 Summary of R’s built-in binomial distribution R makes it easy to use the binomial distribution by giving us several useful functions. We can summarize these below: Code Meaning rbinom(n = ?, prob = ?, size = ?) Simulate n sequences of size events, where the probability of an outcome of interest is prob. For each of the n sequences, return the number of times out of size that the outcome of interest occurred. dbinom(x = ?, prob = ?, size = ?) What is the probability that an outcome of interest occurs exactly x times out of a sequence of size repeated events, if the probability that the outcome occurs in a single event is prob? pbinom(q = ?, prob = ?, size = ?) What is the probability that an outcome of interest occurs q or fewer times out of a sequence of size repeated events, if the probability that the outcome occurs in a single event is prob? 5.2 The normal distribution in R Just like the binomial distribution lets us model situations where we have a sequence of repeated events, each of which has two possible outcomes, the normal distribution lets us model situations where we have a continuous outcome that has a certain mean and standard deviation. 5.2.1 Simulating outcomes from a normal distribution Just like we can simulate values from a binomial distribution using rbinom, we can simulate values from a normal distribution using rnorm. Adult women in the US have an average height of about 64 inches, with a standard deviation of 6 inches. These are the two parameters that we need for our normal distribution. Using these parameters, let’s imagine that we pick a random adult American women. How tall is she?55 rnorm(n = 1, mean = 64, sd = 6) ## [1] 66.93144 Now let’s pick 10 adult women at random and see how tall they are56 ## [1] 67.96766 61.54070 51.18494 69.00817 71.16475 66.77840 65.02384 67.07920 ## [9] 55.20665 60.07779 Each of those numbers represents the height of a simulated adult American woman. We can also get the heights for simulated adult American men by remembering that their mean height is 69 inches with a standard deviation again of 6 inches57 ## [1] 68.18499 64.88364 67.25053 75.90296 71.39535 67.54149 63.06030 69.86670 ## [9] 70.03507 80.95467 Eyeballing it, it does seem like those simulated men tend to be taller than our simulated women.58 Now let’s try simulating the heights of 10,000 women using our normal distribution model. To do this, let’s first try doing what we did above to simulate outcomes of subway series, just swapping out the very first line of code: result &lt;- rnorm(n = 10000, mean = 64, sd = 6) height_data &lt;- tibble(result) height_data %&gt;% group_by(result) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 10,000 x 3 ## result n p ## * &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41.1 1 0.0001 ## 2 43.1 1 0.0001 ## 3 43.2 1 0.0001 ## 4 43.5 1 0.0001 ## 5 44.0 1 0.0001 ## 6 44.4 1 0.0001 ## 7 44.5 1 0.0001 ## 8 44.9 1 0.0001 ## 9 44.9 1 0.0001 ## 10 45.3 1 0.0001 ## # … with 9,990 more rows Well that didn’t work very well at all! This illustrates the challenge of working with a continuous variable as opposed to a discrete one like number of games won. Every observed value is unique, so it occurs with a frequency of one. Luckily, we’ve already seen that histograms make it easy to visualize the relative frequencies of outcomes on continuous variables because they group together the outcomes into bins. height_data %&gt;% ggplot(aes(x=result)) + geom_histogram(binwidth = 1) Now we are beginning to see that classic bell curve shape: There are more values around the mean (64 inches) than farther from the mean, but it is still possible to get a few folks who are particularly tall (like Allison Janney) or short (like Kristen Chenoweth). 5.2.2 The mean and standard deviation The parameters of the normal distribution are its mean and standard deviation, which determine its center and spread, respectively. These parameters are also what you will tend to get if you summarize the central tendency and variability of samples from a normal distribution. This is one of the awesome things about the normal distribution—its parameters tell you exactly what you should expect your data to look like. Let’s see that by summarizing the mean and standard deviation of the heights of our 1000 simulated women: height_data %&gt;% summarize(M = mean(result), S = sd(result)) ## # A tibble: 1 x 2 ## M S ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.0 6.04 They may not be exactly 64 and 6, but they are pretty close. In fact, let’s simulate another group of 10,000 women and summarize their heights. result &lt;- rnorm(n = 10000, mean = 64, sd = 6) height_data &lt;- tibble(result) height_data %&gt;% summarize(M = mean(result), S = sd(result)) ## # A tibble: 1 x 2 ## M S ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.0 5.95 Again, pretty close to the parameters we used! 5.2.3 Calculating probabilities with the normal distribution Remember that we used the dbinom function in R to find the probability of a specific outcome in a sequence of repeated binary events. There is an analogous function in R for the normal distribution called dnorm, but we won’t really ever use it. The reason is the same as why our original attempt to make a frequency table didn’t really work: With a continuous variable, the probability of observing any single value is basically zero. Instead, we will follow the same line of reasoning that makes the histogram useful: For continuous variables, we aren’t interested in whether they take some specific value, but whether they fall into a particular “bin.” We don’t care if someone is exactly 71.05976… inches tall. We care whether they are, for example, between 71 and 72 inches tall. Or whether they are more than 64 inches tall. Or whether they are less than 80 inches tall. The probabilities of all of those kinds of things can be found by doing some clever tricks with cumulative probability. Remember that this is the probability of observing a value that is less than or equal to some given cutoff. This was how we figured out the probabilities of different numbers of wins. Now we can do the same for different ranges of height. Just like we had pbinom for cumulative probability, we also have pnorm. This code gives us the probability that a woman is less than or equal to 60 inches tall: pnorm(q = 60, mean = 64, sd = 6) ## [1] 0.2524925 This is awesome! But it doesn’t quite make clear where this probability is coming from. Let’s look back at our histogram of simulated heights, but now make it so that bars corresponding to heights less than 60 are filled with a different color: height_data %&gt;% ggplot(aes(x = result, fill = (result &lt;= 60))) + geom_histogram(binwidth = 1) What pnorm is doing is telling us is the area of the histogram that is colored teal (for being less than or equal to 60). But this immediately suggests that we can use pnorm to find the area colored red (for being more than 60). We know that probabilities have to add up to 1, so the area across all the bars of the histogram equals 1. So if we know the area for one color, we can figure out the area for the other because we know that, together, they have to add up to 1. That gives us all we need to figure out the probability that a woman is more than 60 inches tall59: ## [1] 0.7475075 Cumulative probability also lets us figure out the probability that a woman is, say, between 60 and 70 inches tall. The insight is that the probability that a woman is less than or equal to 70 inches “contains” the probability that a woman is less than or equal to 60 inches. So to find the probability we need to subtract out that overlap. pnorm(q = 70, mean = 64, sd = 6) - pnorm(q = 60, mean = 64, sd = 6) ## [1] 0.5888522 Let’s make a colored histogram again to get a better sense of what is going on here height_data %&gt;% ggplot(aes(x = result, fill = (result &gt; 60 &amp; result &lt;= 70))) + geom_histogram(binwidth = 1) Again, we are finding the area that is in the teal part of the histogram, corresponding to women between 60 and 70 inches tall. Using that same logic, we can figure out the probability that a woman is between 71 and 72 inches tall60: ## [1] 0.03046128 5.2.4 Finding “quantiles” with the normal distribution There’s one final trick we will explore about the normal distribution. Above, we saw how we could find the probability of observing a value that was less than or equal to a given cutoff. The cutoff value is typically called a “quantile”, hence why pbinom and pnorm label it q. But we can also turn that question around: What is the quantile for which there is a specific probability of being less than that value? In other words, if we know the cumulative probability is \\(p\\), what must \\(q\\) be? We can answer that kind of question in R using the qnorm function for normal distributions (there is also a qbinom function for binomial distributions, but we won’t typically use it). What is the height for which a woman has a 0.6 probability of being shorter than that height? qnorm(p = 0.6, mean = 64, sd = 6) ## [1] 65.52008 Again, let’s try making ourselves a colored histogram to understand what this answer means. height_data %&gt;% ggplot(aes(x = result, fill = (result &lt;= qnorm(p = 0.6, mean = 64, sd = 6)))) + geom_histogram(binwidth = 1) Notice that for the fill aesthetic, we’ve just swapped out a number we picked ahead of time (like 60) for the qnorm line from above. It looks like a bit more than half of the histogram is colored teal. This corresponds to the 0.6 probability associated with the quantile we found. Things will look quite a bit different if we ask what is the height for which a woman has a probability 0.99 of being shorter than that height?61 ## [1] 77.95809 Now if we make a colored histogram like we just did, it should look almost all teal, since there is such a large probability that a woman is shorter than the quantile we just found. Recall that male heights have a mean of 69 inches. Do you think the quantile corresponding to a probability of 0.99 will be higher for men than women? Try it and see!62 ## [1] 82.95809 5.2.5 Summary of R’s built-in normal distribution Code Meaning rnorm(n = ?, mean = ?, sd = ?) Simulate n samples from a normal distribution with a given mean and standard deviation (sd). pnorm(q = ?, mean = ?, sd = ?) What is the probability of observing a value less than or equal to q if values come from a normal distribution with the given mean and standard deviation (sd)? qnorm(p = ?, mean = ?, sd = ?) What is the value (“quantile”) for which there is a probability p of observing a value that is less than or equal to that value, if the value comes from a normal distribution with the given mean and standard deviation (sd)? 5.3 Wrap-up In this session we explored the functions built into R that allow us to use the binomial and normal distributions as “models” for various situations. We saw how we could describe a situation in terms of parameters and use R to simulate different outcomes, figure out probabilities of different types of outcomes, and for the normal distribution, how to find “quantiles” when we know the corresponding cumulative probability. Notice that the rbinom function also requires us to give it things named prob and size, like our old simulation code. How is the way we use prob and size similar between the two pieces of code? How are they different?↩︎ Describe what the 3 columns in the table (“result,” “n,” and “p”) each represent.↩︎ What parameter would you change in order to get the binomial distribution to describe probabilities of winning or losing a single game?↩︎ Compare the code for the dbinom and rbinom functions. What is similar and what is different?↩︎ What code would give us this probability?↩︎ How would we change the pbinom code to get this answer?↩︎ What code would tell us the probability of the Yankees winning 5 or more games in a series?↩︎ Compare the code for rnorm to the code for rbinom. What is similar and what is different?↩︎ What code would use the normal distribution to simulate the heights of 10 adult women?↩︎ What parameter of the normal distribution do we need to change in order to simulate adult male heights, rather than adult female heights?↩︎ When you simulated male and female heights, were there any women who were taller than at least one of the men? Why do you think this might be, even though men are on average taller than women?↩︎ What code did you use to find the probability that a woman is more than 60 inches tall?↩︎ What code did you use to find the probability that a woman is between 71 and 72 inches tall?↩︎ What code would answer this question?↩︎ What code did you use to find the 0.99 quantile for male heights? Hint: which parameter of the normal distribution is different for heights of men and women?↩︎ "],["lab6.html", "Lab 6 Sampling and the Central Limit Theorem: The Wisdom of Large Samples 6.1 Before we begin… 6.2 Heights 6.3 The Wisdom of the Crowd 6.4 Wrap-up", " Lab 6 Sampling and the Central Limit Theorem: The Wisdom of Large Samples We have just encountered a remarkable phenomenon which goes by the weighty but unclear name “central limit theorem.” This theorem says that if we draw a large enough sample from our population of interest, the mean of our sample will have a much better chance of being a good estimate of the mean of the population. Specifically, what happens is that there is a distribution of sample means which is approximately normal. The mean of this distribution is the population mean (which we’ve labeled \\(\\mu\\)) and the standard deviation of this distribution is called the “standard error of the mean” and is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\), where \\(\\sigma\\) is the population standard deviation and \\(n\\) is our sample size. So this distribution of sample means is centered on the “correct answer,” and gets narrower and narrower the larger our sample. This means that, the larger the sample, the better the chance that its mean is close to the population mean. But for this to happen, our sample has to be representative of the population. If the sample is biased in any way, then our estimate of the population mean will also be biased. One way a sample can be biased is if it is not a simple random sample from the population. In this session, we will see the “central limit theorem” in action to better understand what it means. We will also see how sampling bias hampers our ability to estimate population means. Finally, we will see that it is not just a statistical curiosity, but something that is surprisingly pervasive. 6.1 Before we begin… For this session, we will need the tidyverse package like usual. We will also need another package from R’s library called “infer.” Make sure you have it installed (if you already have it installed, you can skip the next line, but try running this line if something doesn’t work): install.packages(&quot;infer&quot;) Now, make sure you have loaded both the tidyverse package and the new “infer” package from R’s library: library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(infer) One last thing before we begin: Make your best guess about the answer to this question: What percent of the world’s airports are in the United States? 6.2 Heights Last time, we saw how when we used a model to simulate lots of individual observed values, the descriptive statistics for our simulated data were very close to the model parameters we used. We focused on the distribution of these individual values. Now, we will focus on the distribution of sample means. This is the distribution that represents our uncertainty about the true value of the population mean. 6.2.1 A simulated population Lets use the normal distribution to simulate the heights of an entire population of women, let’s say 50,000. Remember that the mean height is 64 inches and the standard deviation is 6 inches: height &lt;- rnorm(n = 50000, mean = 64, sd = 6) population &lt;- tibble(height) In the first line, we used the rnorm function to simulate the heights of n = 50000 women, where the average height is mean = 64 inches and the standard deviation of heights is sd = 6 inches. We told R to remember this collection of values under the name “height.” In the second line, we told R to treat “height” as if it were data (a “tibble”) and told R to remember our data under the name “population.” 6.2.2 A sample from our simulated population Now that we have a whole population at our disposal—which we can only do because we are simulating them!—we can experiment and see what the effect is of getting different samples from this population. The code below selects a simple random sample of 10 women from our population: population %&gt;% slice_sample(n = 10) ## # A tibble: 10 x 1 ## height ## &lt;dbl&gt; ## 1 55.2 ## 2 56.8 ## 3 58.9 ## 4 65.6 ## 5 65.0 ## 6 52.0 ## 7 58.1 ## 8 52.6 ## 9 59.5 ## 10 65.3 This is pretty similar to how we simulated things like coin flips earlier.63 The slice_sample function is just another way we can simulate drawing a “sample” of size n from a set of possibilities. Let’s try it again and see what a different sample of 10 women from this population might be: population %&gt;% slice_sample(n = 10) ## # A tibble: 10 x 1 ## height ## &lt;dbl&gt; ## 1 64.5 ## 2 67.8 ## 3 55.9 ## 4 60.7 ## 5 66.3 ## 6 69.0 ## 7 74.7 ## 8 57.0 ## 9 54.8 ## 10 64.3 Now let’s tell R to remember one of these samples under the name “sample_heights,” so we can work with it later. sample_heights &lt;- population %&gt;% slice_sample(n = 10) Now we can do our usual thing and find the mean and standard deviation of the heights in this one sample: sample_heights %&gt;% summarize(M = mean(height), S = sd(height)) ## # A tibble: 1 x 2 ## M S ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.9 5.73 As we might expect, they are somewhat close to the parameters we used to generate the population, but they are unlikely to match exactly. 6.2.3 Many samples from our simulated population If we kept getting samples of size 10 from our population, how often would their sample means be close to the true population mean? While we could just keep running that same code from above over and over again, R helps us automate this process: many_samples_size10 &lt;- population %&gt;% rep_slice_sample(n = 10, reps = 1000) What we just did was use the rep_slice_sample function to repeatedly sample groups of 10 women from our population64. Specifically, we got reps = 1000 samples of size 10, and we told R to remember these samples under the name “many_samples_size10.” Let’s have a look at what the resulting simulated data look like: many_samples_size10 ## # A tibble: 10,000 x 2 ## # Groups: replicate [1,000] ## replicate height ## &lt;int&gt; &lt;dbl&gt; ## 1 1 65.3 ## 2 1 72.3 ## 3 1 69.6 ## 4 1 67.1 ## 5 1 69.0 ## 6 1 70.4 ## 7 1 67.6 ## 8 1 54.5 ## 9 1 56.2 ## 10 1 55.1 ## # … with 9,990 more rows So there are two columns, one column is “height,” representing the height in inches of each simulated woman in our samples. The other column is “replicate” which tells us which sample (of the 1000 samples we made) the woman is in. This column will let us group the women into their respective samples. Now we can get the mean height for each sample: many_samples_size10 %&gt;% group_by(replicate) %&gt;% summarize(M = mean(height)) ## # A tibble: 1,000 x 2 ## replicate M ## * &lt;int&gt; &lt;dbl&gt; ## 1 1 64.7 ## 2 2 67.3 ## 3 3 62.5 ## 4 4 60.8 ## 5 5 63.0 ## 6 6 64.6 ## 7 7 62.2 ## 8 8 66.1 ## 9 9 64.1 ## 10 10 60.6 ## # … with 990 more rows So we begin to see how the sample mean changes depending on who happened to have been selected into the sample. But this is still too many numbers, so let’s go to our old standby, the histogram. First, let’s tell R to remember those sample means we just found: sample_means_size10 &lt;- many_samples_size10 %&gt;% group_by(replicate) %&gt;% summarize(M = mean(height)) Now, we are in shape to make a histogram representing the frequency with which different samples have different means: sample_means_size10 %&gt;% ggplot(aes(x=M)) + geom_histogram(binwidth=1) Certainly looks like a normal distribution! Now let’s see what the mean and standard deviation of the sample means are: sample_means_size10 %&gt;% summarize(MM = mean(M), SEM = sd(M)) ## # A tibble: 1 x 2 ## MM SEM ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.0 1.93 Remember that the standard deviation of sample means goes by the special name standard error of the mean, hence why we labeled it SEM.65 6.2.4 Many larger samples Let’s follow the same steps above, but instead of collecting many samples of size 10, let’s get many larger samples of size 100. First, let’s simulate 1000 samples, all of size 100, and tell R to remember them under the name “many_samples_size100.”66 Next, let’s find the means of each of those samples like we did above: sample_means_size100 &lt;- many_samples_size100 %&gt;% group_by(replicate) %&gt;% summarize(M = mean(height)) And have a look at a histogram: sample_means_size100 %&gt;% ggplot(aes(x=M)) + geom_histogram(binwidth=1) And the mean of sample means and standard error of the mean67: sample_means_size100 %&gt;% summarize(MM = mean(M), SEM = sd(M)) ## # A tibble: 1 x 2 ## MM SEM ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.0 0.600 6.2.5 The probability of being wrong Remember that, just like we never really get to see the whole population in any real-world settings, we also never really get to see the distribution of sample means in real life. In real life, we will probably only have one sample and, therefore, one sample mean. But because our sample mean comes from a distribution of sample means that is normal, we can use the normal distribution to figure out the probability that whatever sample mean we have is wrong, and by how much. 6.2.5.1 Using simulation First, let’s use our many simulated samples to see how many are wrong by different amounts. How many of our samples of size 10 had sample means that under-shot or over-shot the true population mean of 64 inches? sample_means_size10 %&gt;% group_by(M &gt; 64) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## `M &gt; 64` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 497 0.497 ## 2 TRUE 503 0.503 In the second line above, we grouped our samples by whether or not their sample means were greater than 64 (M &gt; 64). Looks like it is about even.68 This suggests that the sample means are not biased either high or low. Now, how many of our samples of size 10 had sample means that were within 1 inch of the true population mean of 64 inches? In other words, how many had sample means that were between 63 and 65 inches? sample_means_size10 %&gt;% group_by(63 &lt; M &amp; M &lt; 65) %&gt;% summarize(n=n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## `63 &lt; M &amp; M &lt; 65` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 600 0.6 ## 2 TRUE 400 0.4 By comparison, how many of our samples of size 100 had sample means that were within 1 inch of the true population mean of 64 inches?69 ## # A tibble: 2 x 3 ## `63 &lt; M &amp; M &lt; 65` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 97 0.097 ## 2 TRUE 903 0.903 Wow, that’s quite an improvement! The probability that our estimate of the population mean—the sample mean—is within 1 inch of the correct answer is much higher if our sample has 100 women than 10 women. 6.2.5.2 Using the central limit theorem Remember that we were only able to count those results above because we have many many samples of the same size. In real life, we almost never have this. That’s why the central limit theorem is useful—we can use what we know about the normal distribution to calculate the probability of being wrong and by how much. Let’s first think about our samples of size 10. The central limit theorem says that the means of these samples have a normal distribution with a mean of \\(\\mu = 64\\) inches and a standard deviation of \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{6}{\\sqrt{10}} \\left( \\approx 1.897 \\right)\\) inches. In R, we can write that standard deviation like this: 6 / sqrt(10), where sqrt stands for “square root.” Now, using the normal distribution, what is the probability that the mean we get from a sample of size 10 is within 1 inch of the true population mean?70 pnorm(q=65, mean = 64, sd = 6 / sqrt(10)) - pnorm(q = 63, mean = 64, sd = 6 / sqrt(10)) ## [1] 0.4018385 And what is the probability that the mean we get from a sample of size 100 is within 1 inch of the true population mean?71 ## [1] 0.9044193 6.2.6 Biased samples So far, we assumed that our samples were all simple random samples. What happens if this is no longer the case? What if instead our samples are biased? Imagine that instead of selecting women at random, we got a bunch of women in a large room and selected them based on how easy they were to see. Obviously this would lead us to select more tall than short women, because it is easier to see a tall person above a crowd (and a tall person could hide a shorter person behind them). We can simulate this kind of biased sampling by telling R to give more “weight” to a woman who is taller when selecting samples. This doesn’t mean that a short person will never get selected (maybe they are standing in front so no one is in the way), but it does make it less likely. Here’s an example of a single sample of size 10, where more “weight” is given to women who are taller: population %&gt;% slice_sample(n = 10, weight_by = height) ## # A tibble: 10 x 1 ## height ## &lt;dbl&gt; ## 1 65.6 ## 2 62.5 ## 3 65.7 ## 4 65.2 ## 5 62.4 ## 6 60.2 ## 7 46.7 ## 8 57.6 ## 9 74.8 ## 10 62.3 Now let’s repeat the same steps from above to get 1000 biased samples each of size 10 and size 100 (note that this may take longer than simulating unbiased samples): many_biased_samples_size10 &lt;- population %&gt;% rep_slice_sample(n=10, reps=1000, weight_by = height) many_biased_samples_size100 &lt;- population %&gt;% rep_slice_sample(n=100, reps=1000, weight_by = height) Now let’s get the means of each of these biased samples: biased_sample_means_size10 &lt;- many_biased_samples_size10 %&gt;% group_by(replicate) %&gt;% summarize(M = mean(height)) biased_sample_means_size100 &lt;- many_biased_samples_size100 %&gt;% group_by(replicate) %&gt;% summarize(M = mean(height)) Let’s look at some histograms to get a sense of their distribution shape. Here it is for biased samples of size 10: biased_sample_means_size10 %&gt;% ggplot(aes(x=M)) + geom_histogram(binwidth=1) And for biased samples of size 10072: biased_sample_means_size100 %&gt;% ggplot(aes(x=M)) + geom_histogram(binwidth=1) And finally, let’s have a look at the mean of the sample means and their standard errors for biased samples of size 10: biased_sample_means_size10 %&gt;% summarize(MM = mean(M), SEM = sd(M)) ## # A tibble: 1 x 2 ## MM SEM ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.6 1.84 And for biased samples of size 10073: biased_sample_means_size100 %&gt;% summarize(MM = mean(M), SEM = sd(M)) ## # A tibble: 1 x 2 ## MM SEM ## &lt;dbl&gt; &lt;dbl&gt; ## 1 64.6 0.606 6.3 The Wisdom of the Crowd We’ve seen how, at least when we have simple random samples, the larger a sample, the more likely it is that our sample mean is close to the actual population mean. But this isn’t just a statistical curiosity about estimating properties of populations, like their heights. It turns out that if you want to know anything about the world, you’re better off asking more people. This concept was originally identified by Galton (1907) and is now called the “wisdom of the crowd” (Surowiecki, 2004). The wisdom of crowds was studied recently by Steegen et al. (2014) who replicated a study by Vul &amp; Pashler (2008). They asked people questions about world facts like the one you answered at the beginning of the session. These people weren’t experts and weren’t allowed to look up the answer. But as we will see, their mean answer is actually pretty close to the truth. 6.3.1 Grab the data First, let’s download the guesses that the participants in Steegen et al. (2014) made to the question you answered at the beginning: wisdom_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/wisdom.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## age = col_double(), ## nationality = col_character(), ## sex = col_character(), ## guess = col_double() ## ) Taking a look at the data, each row represents a different person’s answer to the question. The answer they gave is in the “guess” column. There is also some demographic information about each person (age, sex, nationality). 6.3.2 Make a histogram Now let’s make a histogram of people’s guesses74: wisdom_data %&gt;% ggplot(aes(x=guess)) + geom_histogram(binwidth = 5) Doesn’t look especially normal, but it is also pretty spread out. This makes sense given that very few if any people in this sample would be expected to be experts in airport distribution. 6.3.3 What is the right answer? At the time these data were collected, the correct answer to the question, “what percent of the world’s airports are in the United States?” was 32.3% (how close were you?). How many people over- or under-shot the answer? wisdom_data %&gt;% group_by(guess &lt; 32.3) %&gt;% summarize(n = n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## `guess &lt; 32.3` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 155 0.329 ## 2 TRUE 316 0.671 6.3.4 What about the average? How close was the mean guess to the right answer? wisdom_data %&gt;% summarize(M = mean(guess)) ## # A tibble: 1 x 1 ## M ## &lt;dbl&gt; ## 1 27.0 Huh, not too bad! The difference between the mean guess and the right answer is about \\(32.3 - 27 = 5.3\\). How many people were at least as close to the correct answer as the mean guess, in other words, how many people guessed a value between 27 and \\(32.3 + 5.3 = 37.6\\)?75 ## # A tibble: 2 x 3 ## `27 &lt; guess &amp; guess &lt; 37.6` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 380 0.807 ## 2 TRUE 91 0.193 So the mean guess is doing better than the vast majority of individual guesses. This is pretty remarkable! The “wisdom of the crowd” is another case in which large samples get us closer to the truth.76 6.4 Wrap-up We’ve seen the “central limit theorem” in action: The larger our sample, the more likely it is that the sample mean is close to the population mean. This is valuable because, in reality, we almost never know the true population mean, so we have to estimate it using a sample. But we also saw that a biased sample can give us the wrong answer. Finally, we saw that the power of large samples extends to making guesses about the world at large—maybe instead of finding an expert, we just need a lot of guesses! Compare sampling from a population to simulating coin flips. When we select an individual from a population, what is the sample space? If we are doing simple random sampling, what is the probability that any particular individual is selected into our sample?↩︎ Compare this line of code with the line we used to get a single sample of size 10, above. What is similar and what is different?↩︎ Based on the “central limit theorem,” and the fact that the population standard deviation is \\(\\sigma = 6\\) and the samples are of size \\(n = 10\\), how does the value of SEM above compare with what the central limit theorem would predict?↩︎ What code would do this? Hint: what would you need to change in the code we used to produce many samples of size 10?↩︎ Compare SEM we found for our samples of size 100 with what the central limit theorem says the standard error of the mean should be—is it close? Compare SEM for samples of size 10 to SEM for samples of size 100, which is smaller?↩︎ By counting the number of samples that over-shot, we also end up knowing how many samples under-shot. Why is this?↩︎ What code produces the results below? Hint: what should we change in the code immediately above?↩︎ How does the probability we get from the normal distribution compare with the one we found by counting our samples, above?↩︎ What code produces this result? Hint: what do we need to change in the code above?↩︎ Do the shapes of the distributions of biased means still look roughly normal, or do they look a bit skewed? Is there a difference between biased samples of size 10 vs. 100?↩︎ What is different between the mean of sample means (MM) and the standard error of the mean (SEM) for biased samples versus unbiased samples? Does this difference make sense given the way our samples are biased?↩︎ Why do you think the histogram looks kind of “lumpy?” Hint: how much detail do you think people can give when making a guess like this?↩︎ What code would give us this result? Hint: remember how we found the number of sample mean heights that were within an inch of the true population average above.↩︎ Why do you think that the mean of a set of guesses does so much better than most of the individual guesses?↩︎ "],["lab7.html", "Lab 7 Confidence intervals and hypothesis testing 7.1 Be still, my (artificial) heart 7.2 Lay your weary head to rest 7.3 Wrap-up", " Lab 7 Confidence intervals and hypothesis testing As we’ve seen, one of the remarkable fruits of the “central limit theorem” is the confidence interval. This interval tells us where sample means are likely to be, assuming Samples are selected from the population using simple random sampling; and That we either know or hypothesize the parameters of the population we are sampling from. We explored the consequences of sampling last time. Today, we will see more about how to find confidence intervals in R and use them to test hypotheses. Specifically, we will use confidence intervals to test “null” hypotheses. As we’ve seen, there are four basic steps to testing a hypothesis using a confidence interval. Figure out what parameters would describe the population if the hypothesis were true. Decide how wide to make our confidence interval. Figure out the upper and lower boundaries of the confidence interval. Compare our sample to the confidence interval and, if it is outside the interval, reject our original hypothesis. In the first part of the session, we will do hypothesis tests with simulated data where we know whether the null hypothesis is true or not. In the second part of the session, we will do a hypothesis test on real data and see what we can learn about the population from which it was sampled. Before we begin, make sure you’ve got the “tidyverse” package loaded from R’s library. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 7.1 Be still, my (artificial) heart The resting heart rate within the typical US adult population has a mean of \\(\\mu = 71\\) beats-per-minute with a standard deviation of \\(\\sigma = 9\\) beats-per-minute. We all know that sudden excitement or shock tends to increase our heart rate, but interestingly, the effect of chronic stress on heart rate is not well established. There is evidence for both a higher resting heart rate due to sustained minor stress, as well as a lower resting heart rate due to frequent major stressful life events. In any case, it is clear that detecting abnormal heart rate can be important not just for physiological health, but psychological health as well. In this part of the session, we will simulate different samples of individuals with different resting heart rates and test the hypothesis that these individuals come from a population with the same heart rate parameters as the typical US adult population. We will follow the steps above to construct a confidence interval (steps 1–3), then compare our simulated sample against it (step 4). 7.1.1 Constructing confidence intervals Before we can even get to step 1, we need to be clear what our hypothesis is that we are testing. Our research hypothesis is that the sample of individuals comes from a population with a different mean heart rate than the typical US adult. The statistical hypothesis that we are testing is the null hypothesis. The null hypothesis is that our sample comes from a population with the same mean heart rate as the typical US adult population, with \\(\\mu = 71\\). If we can reject this null hypothesis, then we have reason to believe that our sample is meaningfully different from the typical population and we say it is statistically significant. 7.1.1.1 Figure out what parameters would describe the population if the hypothesis were true From the description above, we know the population mean (\\(\\mu = 71\\)) and standard deviation (\\(\\sigma = 9\\)). Let us assume that our sample size is \\(n = 40\\), which is typical for this kind of research. The central limit theorem tells us that our sample means will, then, have a normal distribution with mean \\(\\mu = 71\\) and standard error of the mean \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{9}{\\sqrt{40}}\\). We can use R to calculate what the standard error is. Exercise 7.1 What R code would give us the standard error of the mean? ## [1] 1.423025 7.1.1.2 Decide how wide to make our confidence interval Our confidence interval is defined by how unlikely a sample has to be before we are willing to reject the null hypothesis. A standard rule is to say that if a result has probability \\(0.05\\) or less if the null hypothesis were true, then we can reject the null hypothesis. This standard rule corresponds to a “95% confidence interval,” where the “95%” refers to \\(1 - 0.05 = 0.95\\), which is the probability that we will fail to reject the null hypothesis when it is true. Let us adopt the standard rule for now and use the 95% confidence interval. 7.1.1.3 Figure out the upper and lower boundaries of the confidence interval To find the upper and lower boundaries of the confidence interval, we have to divide the \\(0.05\\) probability of being outside the interval between the upper and lower ends of the scale. This means that there is a probability of \\(\\frac{0.05}{2} = 0.025\\) of being less than the lower boundary and a probability of \\(0.025 + 0.95 = 0.975\\) of being less than the upper boundary. The lower boundary of the 95% confidence interval is then qnorm(p = 0.025, mean = 71, sd = 9 / sqrt(40)) ## [1] 68.21092 while the upper boundary is qnorm(p = 0.975, mean = 71, sd = 9 / sqrt(40)) ## [1] 73.78908 Exercise 7.2 Based on the upper and lower boundaries we just found, what is the margin of error? 7.1.1.4 Visualization The logic of hypothesis testing can be easier to understand if we visualize the distribution of sample means and the 95% confidence interval. To do that, we will first use the normal distribution to simulate different sample means, then make a colored histogram of those sample means that distinguishes between ones that are inside the confidence interval and ones that are outside the confidence interval. First, let’s simulate a large number of means of possible samples of size \\(n = 40\\) that we might observe if the null hypothesis were true. M &lt;- rnorm(n = 10000, mean = 71, sd = 9 / sqrt(40)) Now let’s turn those into “data” so we can use R to make a histogram of these simulated sample means. sample_means_size40 &lt;- tibble(M) And finally make the histogram sample_means_size40 %&gt;% ggplot(aes(x = M)) + geom_histogram(binwidth=0.5) Now we need to color in this histogram depending on whether the sample mean is inside or outside the confidence interval. To do that, let’s tell R to remember the upper and lower boundaries we found earlier ci_lower &lt;- qnorm(p = 0.025, mean = 71, sd = 9 / sqrt(40)) ci_upper &lt;- qnorm(p = 0.975, mean = 71, sd = 9 / sqrt(40)) That makes it easier to fill the bars with different colors: sample_means_size40 %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth=0.5) 7.1.2 Compare our sample to the confidence interval In this part of our session, we will simulate different samples and compare them to the confidence interval we just found. We will see how different our sample needs to be from the typical population before we consistently reject the null hypothesis. 7.1.2.1 When the null hypothesis is true First, let’s imagine that the sample of individuals we get actually does come from a population with the same mean heart rate as typical US adults, i.e. with \\(\\mu = 71\\). We will tell R to remember our sample under the label “heart_rate_sample.” heart_rate_sample &lt;- rnorm(n = 40, mean = 71, sd = 9) Exercise 7.3 Compare the code just we used to simulate a sample of heart rates with the code we used to simulate sample means. Why is the mean the same? Why is the standard deviation different? mean(heart_rate_sample) ## [1] 70.02283 Is the sample mean outside the confidence interval, which would lead us to reject the null hypothesis, even though it is actually true? Exercise 7.4 Try running the previous two lines of code a few more times (simulate a new sample and find its mean). How many times did it take before you simulated a sample with a mean that fell outside the 95% confidence interval? 7.1.2.2 When the null hypothesis is false Now let’s imagine that the sample we get comes from a population that has a higher resting heart rate, on average, than the typical US adult population. Will we correctly reject the null hypothesis? Let’s say that the mean heart rate in this different population is 74. heart_rate_sample &lt;- rnorm(n = 40, mean = 74, sd = 9) How does the mean of this sample compare to the confidence interval we found above? mean(heart_rate_sample) ## [1] 74.82869 Exercise 7.5 Run the previous two lines of code 10 times. Out of those ten times, how often did the sample mean fall outside the 95% confidence interval? Exercise 7.6 In the code we just used, we assumed that our samples came from a population with a mean of 74. How much bigger do you think this mean would need to be before almost all samples would fall outside the 95% confidence interval? 75? 76? 100? Why? 7.2 Lay your weary head to rest Having tested hypotheses using simulated resting heart rate data, let’s see about testing a hypothesis using some real data. These data have to do with a different kind of rest, sleep. These are the number of hours slept in a particular night for a simple random sample of \\(n = 110\\) US college students. We will use hypothesis testing with confidence intervals to infer whether college students tend to sleep the recommended amount per night. First, let’s make sure to download the data to get it into R student_sleep &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/student_sleep.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## hours = col_double() ## ) If you click on the “student_sleep” data in R’s environment pane in the upper right, you’ll see that it is just a single column called “hours,” where each row in the number of hours slept by a different student. Let’s also make a histogram to get a sense of what the data look like. student_sleep %&gt;% ggplot(aes(x = hours)) + geom_histogram(binwidth = 1) Exercise 7.7 Describe the shape of the distribution of number of hours slept. Does it seem symmetrical, skewed? How many modes does the distribution seem to have? 7.2.1 Figure out what parameters would describe the population if the hypothesis were true Our research hypothesis is that college students do not sleep the recommended amount per night. According to the National Sleep Foundation, the recommended distribution of sleep for young adults has a mean of \\(\\mu = 8\\) hours and a standard deviation of \\(\\sigma = 1\\) hour. As a result, the null hypothesis that we are going to test is that the mean number of hours slept by college students is equal to the recommended population mean of \\(\\mu = 8\\). Notice that we are again using a “hypothetical” population to specify our null hypothesis. This hypothetical population is college students who sleep the recommended amount per night. We are going to see if our sample could plausibly have come from such a population. 7.2.2 Decide how wide to make our confidence interval Again, we have a choice to make here about how willing we are to make a “Type I error,” that is, to reject the null hypothesis even when it is true. For the sake of argument, let’s say that we want to be very conservative and will try to minimize our risk of making a Type I error. So instead of a 95% confidence interval, let’s make a 99% confidence interval. This means that the probability of making a Type I error is only \\(0.01\\). 7.2.3 Figure out the upper and lower boundaries of the confidence interval We now have all the ingredients we need to make a confidence interval. We have the population mean (\\(\\mu = 8\\)), population standard deviation (\\(\\sigma = 1\\)), sample size (\\(n = 110\\)), and confidence interval width (99%). The lower bound of our confidence interval is then ## [1] 7.754404 while the upper boundary is ## [1] 8.245596 Exercise 7.8 What is the R code for finding those lower and upper boundaries of the 99% confidence interval? Hint: what do you need to change in the code we used for finding the confidence interval boundaries in the heart rate example above? 7.2.4 Compare our sample to the confidence interval Finally, let’s find our sample mean and compare it to the confidence interval to decide whether or not to reject our null hypothesis: student_sleep %&gt;% summarize(M = mean(hours)) ## # A tibble: 1 x 1 ## M ## &lt;dbl&gt; ## 1 7.42 That is definitely outside the bounds of our 99% confidence interval! As a result, we reject the hypothesis that the mean number of hours slept by the population of college students is equal to the recommended amount of \\(\\mu = 8\\) hours. Exercise 7.9 Although we rejected the null hypothesis in this case, take a look at the histogram we made above of the complete distribution of student sleep time. Is it the case that every single student sleeps less than the recommended amount? What does this mean for how we should interpret the results of our hypothesis test? 7.3 Wrap-up In this session, we saw how we can use R to find confidence intervals and how we can use those intervals to test null hypotheses. We saw how hypothesis tests are not perfect—they can lead us to reject the null hypothesis even if it is true (Type I error) and they don’t always lead us to reject the null hypothesis even when it is false (Type II error). Even so, hypothesis tests can be a powerful way to learn about the world using data. "],["lab8.html", "Lab 8 Statistical Power 8.1 The Power of Positive Feedback 8.2 Achieving a target level of power 8.3 Using data to test the null hypothesis 8.4 Wrap-up", " Lab 8 Statistical Power If we conduct a null hypothesis test and decide to reject the null hypothesis, there are two possibilities: Either we incorrectly rejected the null hypothesis, in which case we made a Type I error; or we correctly rejected the null hypothesis (and thereby avoided a Type II error). We’ve seen that we get to decide the probability of a Type I error because it is the same thing as our “alpha level.” But what is the probability that we correctly reject the null hypothesis? This probability is called the power of our test and is often called “beta” (\\(\\beta\\)). The challenge in figuring out what “beta” is that we do not know ahead of time how far from the null hypothesis our results might be. The difference between the true population parameters and those assumed by the null hypothesis is called effect size. We already encountered this last session, in which we wondered how different the heart rates in a sample would need to be before we could always reject the null hypothesis. What we were doing was changing the “effect size” and seeing that, with a bigger effect size, power also increased. In this session, we will explore the factors that contribute to power and conduct a null hypothesis test on real data to assess the effectiveness of a computer-based learning activity. Before we begin, let’s make sure to load the tidyverse package from R’s library. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 8.1 The Power of Positive Feedback We will be looking at a study reported by Day et al. (2015). They were interested in how to convey to middle school students the concept of “positive feedback.” Positive feedback occurs whenever changing one part of a system leads to changes in another part of a system, which then causes additional change in the first part (A increases B and B increases A). Positive feedback is common in many natural, social, and technological systems. For example, having more wealth (A) allows you to put money into investments (B) which has the potential to give you even more wealth (A). Day et al. (2015) wondered whether students could learn the concept of positive feedback by playing with a computer simulation of the relationship between ice and climate. The ice covering Earth’s north and south poles helps keep the planet cool because ice reflects sunlight rather than absorbing it (think of how dark clothes get warmer in sunlight than light colored clothes). Positive feedback occurs because if the global temperature increases (A), this melts the ice (B), which then leads to more warming (A) because there is less ice to reflect sunlight back into space (for crossword puzzle fans, the technical term for how reflective a surface is “albedo”). The simulation was meant to illustrate this relationship to students so they could identify similar kinds of positive feedback situations elsewhere. Although their whole study looked at a number of things, we will focus on one particular research question: Can students learn the concept of positive feedback from a computer simulation? 8.1.1 Look at the data Let’s first take a look at their data to get a sense of how they addressed these two research questions. Load it into R: feedback_learning &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/learning_subset.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## ID = col_double(), ## Period = col_double(), ## AdvPlacement = col_double(), ## Grade = col_double(), ## Gender = col_character(), ## Pretest = col_double(), ## Posttest = col_double(), ## Improvement = col_double() ## ) Click on the new “feedback_learning” data that’s now in R’s environment (upper right panel). Each row represents data from a single student and there are several columns. For now, let’s focus on the last three columns: Pretest: This is a score (between 8 and 40) on a test designed to measure understanding of the concept of positive feedback. This test was given prior to doing the computer simulation activity. The score is based on 8 questions which can give between 1 and 5 points. Posttest: This is another score (between 8 and 40) on a similar test, but given after the activity. Improvement: This is the difference between the posttest and pretest scores, and this is how we operationalize the construct of “learning.” If a student does better after the activity than before, they will have a positive improvement. If they do worse, they will have a negative improvement. And if they don’t learn anything, their improvement is zero. In the next section, we will do some computer simulations ourselves! Specifically, we will explore various factors that affect our power to detect whether the activity actually led to student learning or not. Exercise 8.1 We are going to use the normal distribution as our model of the average improvement. Why is the normal distribution more appropriate than the binomial distribution? (Hint: think about what kinds of values a normal distribution can produce and what kinds of values a binomial distribution can produce.) 8.1.2 Specifying our null hypothesis If the answer to the research question (above) was “no,” then we should expect no difference between the pre- and posttest scores. This tells us that our null hypothesis is that the population mean “Improvement” is zero. We will do a two-tailed test because we want to be open to the possibility that the activity actually harmed students’ understanding. This gives us our null and alternative hypotheses: Null hypothesis (\\(H_0\\)): \\(\\mu = 0\\) Alternative hypothesis (\\(H_1\\)): \\(\\mu \\neq 0\\) We are still missing something: Although we know what the population mean would be if the null hypothesis were true, what about the population standard deviation? In other words, what would be the typical variability between students on these tests? For the moment, let’s speculate that the population standard deviation is \\(\\sigma = 5\\), representing the range of scores on a single question on the test. What we are saying is that we think the typical variability between students is around one question’s worth of points. To make things easier for us later on, let’s give these numbers special labels in R: null_mean &lt;- 0 null_sd &lt;- 5 Finally, we need to think about the sample size so that we can use the Central Limit Theorem to figure out where to put the boundaries of our confidence interval. For now, let’s assume that we will have a sample of 30 students and give that number a special label as well: sample_size &lt;- 30 8.1.3 Building our confidence interval Let’s say that we will reject the null hypothesis if the sample we get has less than a 0.05 probability of happening if the null hypothesis were true. In other words, we are setting (for now) our alpha level to 0.05, and as a result we need to find a 95% confidence interval. Let’s give this a label so we can use it in R: alpha_level &lt;- 0.05 As mentioned above, we will use the normal distribution to find the upper and lower boundaries. ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) Notice that, in addition to giving labels to the lower and upper ends of our confidence interval (ci_lower and ci_upper, respectively), we used the labels we made earlier to specify the mean and sd in qnorm. Very handy! 8.1.4 Visualizing the distribution of sample means Finally, let’s visualize the distribution of sample means that we would get if the null hypothesis were true. Like we’ve done before, we will use colors to distinguish between samples that fall outside our confidence interval—and into the critical region leading us to reject the null hypothesis—from those that fall inside it. First, let’s simulate a large number of possible sample means assuming the null hypothesis is true: M &lt;- rnorm(n=10000, mean = null_mean, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) Now let’s make a colored histogram so we can see how often sample means fall inside or outside our confidence interval. sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) We can also check and see how often one of the sample means from the null hypothesis ends up outside the confidence interval, leading us to make a Type I error by incorrectly rejecting the null hypothesis: sample_means %&gt;% group_by((M &gt; ci_lower) &amp; (M &lt; ci_upper)) %&gt;% summarize(n = n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## `(M &gt; ci_lower) &amp; (M &lt; ci_upper)` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 514 0.0514 ## 2 TRUE 9486 0.949 Exercise 8.2 In the table above, the row labeled “FALSE” corresponds to simulated sample means that fell outside the confidence interval, which would lead us to (incorrectly) reject the null hypothesis. How close is the probability “p” in that column to the alpha level we decided on earlier? 8.2 Achieving a target level of power A typical goal in many research areas is to have at least an 0.8 probability of correctly rejecting the null hypothesis. In this section, we will imagine different scenarios with the aim of achieving this level of power. 8.2.1 Effect size Effect size is the difference between the actual population parameters and the population parameters we assume in the null hypothesis. In that sense, the null hypothesis assumes the “effect size” is zero. In this case, “effect size” refers to how much test scores improve as a function of the learning activity. Let’s imagine what the distribution of sample means would look like if the effect size were 1, in other words, if the learning activity improved scores by, on average, a single point. First, let’s put a label on this guess to make our lives easier: effect_size &lt;- 1 Now, let’s do something similar to what we did above and simulate a whole bunch of sample means assuming this effect size: M &lt;- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) And we can visualize what this distribution looks like, along with how much of it falls in the critical region, leading us to correctly reject the null hypothesis. sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) We can approximate the probability of correctly rejecting the null hypothesis using our simulation: sample_means %&gt;% group_by((M &gt; ci_lower) &amp; (M &lt; ci_upper)) %&gt;% summarize(n = n()) %&gt;% mutate(p = n / sum(n)) ## # A tibble: 2 x 3 ## `(M &gt; ci_lower) &amp; (M &lt; ci_upper)` n p ## * &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 1954 0.195 ## 2 TRUE 8046 0.805 But we can also find power exactly by using the normal distribution: pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.1947752 Exercise 8.3 Try setting effect_size to different values (e.g., effect_size &lt;- 0.5) and then re-run the lines of code we just used (i.e., simulate new sample means, make a histogram, and calculate power using the normal distribution). Try to find an effect size where the power ends up as close as you can get it to 0.8 without going over. What effect size did you find? 8.2.2 Sample size Now let’s reset our hypothetical effect size back to 1 point. effect_size &lt;- 1 Now instead of messing around with effect size, let’s see if we can change the sample size to try and achieve a power of 0.8. This is a bit trickier because we also have to adjust our confidence interval limits, because they also depend on sample size. For example, let’s try increasing our sample size to 40. sample_size &lt;- 40 Now, we have to re-calculate our confidence interval limits in addition to everything else: ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) M &lt;- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.2441412 Exercise 8.4 Try setting sample_size to different values (e.g., sample_size &lt;- 13) and then re-run the lines of code we just used (i.e., find the CI limits, simulate new sample means, make a histogram, and calculate power using the normal distribution). Try to find a sample size where the power ends up as close as you can get it to 0.8 without going over. What sample size did you find? 8.2.3 Alpha level Let’s put the sample size back to its original value of 30. sample_size &lt;- 30 Now, we are going to mess around with the alpha level in order to achieve 0.8 power. In other words, we are changing the width of our confidence interval so that it allows us to correctly reject the null hypothesis 80% of the time. Exercise 8.5 Is there a potential problem with messing with the alpha level this way? Are we increasing the probability of making a different kind of error? For example, the code below is based on changing our alpha level from 0.05 to 0.1. Once again, we have to re-do our confidence interval: alpha_level &lt;- 0.1 ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) M &lt;- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.2944317 . Exercise 7.8 Try changing alpha_level in the code above to different values and then re-run the code to make a histogram and calculate power. Try to find an alpha level where the power ends up as close as you can get it to 0.8 without going over. What alpha level did you find? 8.2.4 Population standard deviation There’s one final thing that can affect power, and that is the population standard deviation (\\(\\sigma\\)). Obviously, this is not something we can actually control in the real world, but it is important to consider when thinking about the power to correctly reject the null hypothesis. If the population is more variable, it will be harder to detect any differences that are the result of our intervention. Let’s first reset our alpha level back to 0.05 alpha_level &lt;- 0.05 As above, when we mess with the population standard deviation, this trickles down into every aspect of our hypothesis test, since null_sd appears pretty often. For example, let’s try making the population twice as variable as it was before by setting null_sd to 10. null_sd &lt;- 10 ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) M &lt;- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.08501566 Exercise 8.6 Try changing null_sd in the code above to different values and then re-run the code following it to make a histogram and calculate power. Try to find a population SD where the power ends up as close as you can get it to 0.8 without going over. What population SD did you find? 8.3 Using data to test the null hypothesis Now that we’ve explored the factors that contribute to the power of this—and any—null hypothesis test, let’s see how the data actually played out. First, remember that we had to make a reasonable guess about the population SD before. Now, we are going to estimate it using the sample SD of our actual data. This has some consequences we will discuss later in the course, but for now we will accept this as is: null_sd &lt;- sd(feedback_learning$Improvement) Above, the $ pulls out the Improvement column from our feedback_learning dataset, so null_sd is now the standard deviation of the improvement values we actually observed. Also, notice that the sample size they had was 69, so let’s make sure to adjust that as well: sample_size &lt;- 69 8.3.1 Confidence interval limits Now, we find the upper and lower limits of our confidence interval: ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) And we can find the sample mean (and give it a label, since we’ll be using it later): sample_mean &lt;- mean(feedback_learning$Improvement) Because this falls outside the confidence interval, we reject the null hypothesis! 8.3.2 The \\(p\\) value Let’s also find the \\(p\\) value, the probability that we would observe a result at least as extreme as our result if the null hypothesis were true. Because the sample mean is larger than the mean according to the null hypothesis, we need to find the probability of being above the sample mean: 1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)) ## [1] 0.0004771661 But remember that because this is a two-tailed test, we need to multiply the number above by 2, to account for both the upper and lower parts of the critical region. 2 * (1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))) ## [1] 0.0009543322 This is the final \\(p\\) value, and we can see that it is less than our alpha level of 0.05. Exercise 8.7 By rejecting the null hypothesis, our results are statistically significant. Do you believe they are practically significant? In other words, do you believe that the improvement in scores resulting from the learning activity is large enough to care about? Why or why not? 8.3.3 Estimated power Finally, let’s try something interesting. Assume that the sample mean reflects the actual effect size: effect_size &lt;- sample_mean Now we can use this to simulate plausible sample means we might have observed if the effect size were equal to the sample mean. As a result, we can see what the power of this test would be if applied to similar studies in the future. All we need to do now is run the very same code we used in our simulations above to visualize and calculate the power that this study had. M &lt;- rnorm(n=10000, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) sample_means &lt;- tibble(M) sample_means %&gt;% ggplot(aes(x = M, fill = (M &gt; ci_lower) &amp; (M &lt; ci_upper))) + geom_histogram(binwidth = 0.5) pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.9104759 Exercise 8.8 Based on the power we just calculated, do you feel more confident believing that we correctly rejected the null hypothesis? Does this change your opinion about whether the results are practically significant? Why or why not? 8.4 Wrap-up We conducted simulations to see how effect size, sample size, alpha level, and population variability all affect the power of a test to correctly reject the null hypothesis. We conducted such a test on actual data studying whether a computer simulation learning activity increased student understanding of the concept of “positive feedback.” In addition, we saw how to use an observed effect size to calculate the power of a test. "],["lab9.html", "Lab 9 \\(t\\) tests 9.1 \\(t\\) tests on tiny data 9.2 \\(t\\) tests with real data 9.3 Wrap-up", " Lab 9 \\(t\\) tests In this session, we will see how we can use R to do all the calculations involved in conducting the various types of \\(t\\) test we have been learning about. In addition, we will begin to get acquainted with some more advanced R functions that make it easier to do these tests on real data, including some helpful visualizations for understanding the relationship between \\(t\\) values and \\(p\\) values. For this session, we are going to need to load both our standard tidyverse package as well as the infer package from R’s library. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(infer) 9.1 \\(t\\) tests on tiny data To get acquainted with how \\(t\\) tests work in R, let’s first try them out on some simple artificial data before applying them to some real data. Imagine that we’re designing an advertisement and want to make sure it is attention-grabbing. The ad will be embedded in a webpage. We use an eye tracker to monitor people’s eye movements over the course of 1 minute while looking at a fake webpage containing the ad. We operationalize the construct of attention-grabbing by measuring the proportion of that time that a person is looking at the ad. The result is, for each person, a number between 0 (never looked at the ad) and 1 (only looked at the ad). Here are measurements from a sample of \\(N = 5\\) individuals: X_1 &lt;- c(0.507, 0.052, 0.172, 0.066, 0.204) Notice that we have labeled them X_1 because we will soon have more than one set of measurements. 9.1.1 One-sample \\(t\\) test If participants were looking at the ad half the time on average, their population mean would be \\(\\mu = 0.5\\). Our first research question is therefore, “do people look at the ad more than half the time?” 9.1.1.1 State your hypotheses Given this research question, our null hypothesis is that the mean ad proportion is less than or equal to 0.5 (\\(H_0\\): \\(\\mu \\leq 0.5\\)) while our alternative hypothesis is that the mean ad proportion is more than 0.5 (\\(H_1\\): \\(\\mu &gt; 0.5\\)). This is, therefore, a one-sided test. 9.1.1.2 Set your alpha level Let’s adopt a relatively lenient alpha level of 0.1. 9.1.1.3 Find the \\(t\\) value To get the ingredients we need for the \\(t\\) value, we need to know the deviation of the sample mean from the null hypothesis as well as the estimated standard error of the mean. The deviation from the null hypothesis is the difference between the sample mean and that specified by the null hypothesis, which is 0.5. mean(X_1) - 0.5 ## [1] -0.2998 Meanwhile, the estimated standard error is the sample standard deviation (sd(X_1)) divided by the square root of the sample size (length(X_1)). sd(X_1) / sqrt(length(X_1)) ## [1] 0.08213063 Taking the ratio of the above two quantities gives us our \\(t\\) value: t &lt;- (mean(X_1) - 0.5) / (sd(X_1) / sqrt(length(X_1))) t ## [1] -3.650283 9.1.1.4 Find the \\(p\\) value Because this is a one-sided test and the alternative hypothesis is that the population mean is greater than 0.5, we need to to know the probability of observing a value that is at least as large as the one we have. We also need to know the degrees of freedom, which in this case is \\(5 - 1 = 4\\) (length(X_1) - 1). 1 - pt(q = t, df = length(X_1) - 1) ## [1] 0.989117 9.1.1.5 Decide whether or not to reject the null hypothesis Since the \\(p\\) value is bigger than our alpha level, we fail to reject the null hypothesis. We have no evidence that people look at the ad more than half the time. 9.1.2 Paired sample \\(t\\) test Perhaps spurred by the disappointing results of that one-sample \\(t\\) test, we decide to change the colors in the ad and bring the same sample of 5 people back. Again, we record the proportion of the time that they spend looking at our new ad, which we label X_2: X_2 &lt;- c(0.907, 0.115, 0.548, 0.202, 0.347) Now we have a pair of measurements from each person. So the first entry in X_1 and the first entry in X_2 come from the same person, same for the second entry, third entry, etc. Our research question now is, “did changing the ad affect how much people looked at it at all?” To address this question, we will do a paired sample \\(t\\) test. This involves looking at the differences for each person, which we can find in R like so (we will also give them a label D to make it easier): D &lt;- X_2 - X_1 (Note that you can do the subtraction in any order, the important thing is to remember which order you picked so you know what the difference means!) 9.1.2.1 State your hypotheses Notice that now our question is about any kind of effect, so this will be a two-tailed test. The null hypothesis is that the mean difference is zero (\\(H_0\\): \\(\\mu_D = 0\\)) and the alternative hypothesis is that the mean difference is not zero (\\(H_1\\): \\(\\mu_D \\neq 0\\)). 9.1.2.2 Set your alpha level Again, let’s keep our alpha level from before and say that our alpha level is 0.1. 9.1.2.3 Find the \\(t\\) value Remember that the paired sample \\(t\\) test is much like the one-sample \\(t\\) test, only instead of using raw measurements, we use the differences which we labeled D. ## [1] 3.256259 Exercise 8.2 Modify the slice of code we used in the previous section to find the \\(t\\) value given above and make sure that it is labeled t_D (the “D” is for “difference”). What code did you write to find this new \\(t\\) value? Hint: remember what the mean from the null hypothesis is! 9.1.2.4 Find the \\(p\\) value Since the \\(t\\) value is positive, this means it is on the upper end of the \\(t\\) distribution. To find the probability of seeing a value that is at least as high as the one we saw, we can use 1 - pt(q = t_D, df = length(D) - 1) ## [1] 0.01559538 But remember that we have to multiply that by 2 since this is a two-tailed test. So this is our \\(p\\) value: 2 * (1 - pt(q = t_D, df = length(D) - 1)) ## [1] 0.03119075 9.1.2.5 Decide whether or not to reject the null hypothesis Since the \\(p\\) value is smaller than our alpha level, we reject the null hypothesis. We have reason to believe that changing the ad really did affect each individual’s viewing patterns. Exercise 9.1 Although the test we just did was only about differences in any direction, based on the \\(t\\) value does it seem like changing the ad increased or decreased the proportion of time people spent looking at the ad? 9.1.3 Independent samples \\(t\\) test Let’s imagine that, instead of bringing back the same sample of 5 people to view our new ad, we brought in a different sample. While we’re imagining, let’s say that this new sample still produce the same measurements. So our “data” is the same, only now it is not “paired,” it comes from two independent samples. Our research question remains the same: “did changing the ad affect how much people looked at it at all?” But now, to address this question, we will do an independent samples \\(t\\) test. 9.1.3.1 State your hypotheses Because we have two samples from (potentially) two populations, our null and alternative hypotheses are about a difference in population parameters. This is still a two-tailed test. The null hypothesis is that the mean difference is zero (\\(H_0\\): \\(\\mu_2 - \\mu_1 = 0\\)) and the alternative hypothesis is that the mean difference is not zero (\\(H_1\\): \\(\\mu_2 - \\mu_1 \\neq 0\\)). Notice that we are keeping the order of the subtraction (2 minus 1) the same as above, to make it easier to compare. 9.1.3.2 Set your alpha level Again, let’s keep our alpha level from before and say that our alpha level is 0.1. 9.1.3.3 Find the \\(t\\) value To find the \\(t\\) value, remember that we first have to find the pooled sample standard deviation. In mathematical terms, this is \\(\\hat{\\sigma}_P = \\sqrt{\\frac{df_1 \\hat{\\sigma}^2_1 + df_2 \\hat{\\sigma}^2_2}{df_1 + df_2}}\\). We can write that out in R like so: df_1 &lt;- length(X_1) - 1 df_2 &lt;- length(X_2) - 1 sd_pooled &lt;- sqrt((df_1 * sd(X_1)^2 + df_2 * sd(X_2)^2) / (df_1 + df_2)) sd_pooled ## [1] 0.2584249 With our pooled SD in hand, we can now find the \\(t\\) value t_Indep &lt;- (mean(X_2) - mean(X_1)) / (sd_pooled * sqrt(1 / length(X_1) + 1 / length(X_2))) t_Indep ## [1] 1.368067 9.1.3.4 Find the \\(p\\) value Again, since the \\(t\\) value is positive, this means it is on the upper end of the \\(t\\) distribution. To find the probability of seeing a value that is at least as high as the one we saw, we can use 1 - pt(q = t_Indep, df = length(X_1) + length(X_2) - 2) ## [1] 0.1042398 Note that we have a different number of degrees of freedom here because we are treating these data as being from two independent samples. But again we have to multiply the probability above by 2 since this is a two-tailed test. So this is our final \\(p\\) value: 2 * (1 - pt(q = t_Indep, df = length(X_1) + length(X_2) - 2)) ## [1] 0.2084797 9.1.3.5 Decide whether or not to reject the null hypothesis This time, the \\(p\\) value is bigger than our alpha level, so we fail to reject the null hypothesis. Funny—the same data lead us to different conclusions depending on whether it came from just one sample versus two independent samples! Exercise 7.6 Why do you think we were able to reject the null hypothesis using the paired samples \\(t\\) test, but not with the independent samples \\(t\\) test, even though the actual data were the same in both cases? Think about the fact that there is variability both between different individuals and within a single individual (e.g., if you do something multiple times, chances are you won’t act exactly the same each time). Does one type of test effectively eliminate one of these kinds of variability? How might that affect the ability to the test to detect particular kinds of differences? 9.2 \\(t\\) tests with real data We’ve now seen how to do all the calculations for a \\(t\\) test in R. Because R was built for statistics, you won’t be surprised that it can automate many of those things for us, as we’ll see. As usual, though, when a computer does the “mindless” work, we have to be even more “mindful” of how to interpret the results it gives us. In this example, we will look at a study by Sam Mehr, Lee Ann Song (aptly named), and Liz Spelke (Mehr et al., 2016). They wanted to see whether music is an important social cue for infants. Specifically, they wondered whether infants tend to prefer people who sing familiar songs. The idea is that someone singing a song you know is probably a member of the same social group as you are. They conducted an experiment in which a child’s parents were taught a new melody and were instructed to sing it to their child at home over the course of 1–2 weeks. After this exposure period, the parents brought their infant back to the lab. The infant was seated in front of a screen showing videos of two unfamiliar adults just smiling in silence. They recorded the proportion of the time that the infant looked at each individual during this “before” phase. Then, one of these unfamiliar people sang the melody that the parents had been singing for 1–2 weeks, while the other sang a totally new song. Finally, during the “after” phase, the infant saw the same videos of each person silently smiling and the researchers recorded the proportion of the time spent looking at the person who sang the familiar song. The overall aim of this experiment is to see whether infants will prefer to look at the singer of the familiar melody after hearing them sing it, even though they’ve never seen this person before. 9.2.1 Check out the data First, let’s get the data into R: lullaby &lt;- read_csv(&quot;https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/lullaby_wide.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## id = col_double(), ## Before = col_double(), ## After = col_double() ## ) Have a look at the data by clicking on lullaby in RStudio’s “Environment” pane. Each row is data from a specific infant. There are three variables in this dataset: “id”: Identifies each infant in the study. “Before”: In the phase before the infants heard anyone sing, what proportion of the time did they look at the person who would eventually sing the familiar melody? “After”: In the phase after the infants heard the two people sing, what proportion of the time did they look at the person who sang the familiar melody? Let’s summarize the looking preferences for infants before and after hearing the people sing: lullaby %&gt;% summarize(M_before = mean(Before), S_before = sd(Before), M_after = mean(After), S_after = sd(After)) ## # A tibble: 1 x 4 ## M_before S_before M_after S_after ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.521 0.177 0.593 0.179 Exercise 7.7 Based on the sample means and standard deviations we just found, does it seem like there might be a preference to look more at the person who would sing the familiar melody, either before or after hearing them sing? 9.2.2 Did infants show any prior bias? Before we can ask whether infants prefer to look at someone who sings a familiar melody, we need to see whether they show any kind of bias to look at one person or another before hearing them sing. If they did show a bias to look at one of the individuals before hearing them, then we wouldn’t be able to attribute their behavior to the familiarity of the song. Let’s first inspect the data by making a histogram: lullaby %&gt;% ggplot(aes(x = Before)) + geom_histogram(binwidth=0.1) Exercise 9.2 Based on the histogram above, does it seem like there are any outliers or skew that are very far from a normal distribution? Our research question is, “do infants prefer to look at one person over the other, even before hearing them sing?” We will address this question using a one-sample \\(t\\) test. 9.2.2.1 State your hypotheses An unbiased infant would look equally often at both people, so the proportion of the time they looked at the singer of the familiar song would be 0.5. This is our null hypothesis: \\(H_0\\): \\(\\mu = 0\\). So our alternative hypothesis is just that \\(H_1\\): \\(\\mu \\neq 0\\). 9.2.2.2 Set your alpha level Let’s choose an alpha level of 0.05, same as the one Mehr et al. (2016) chose. 9.2.2.3 Find the \\(t\\) value R’s infer package makes it easy to get our \\(t\\) value. Let’s first look at the code for doing this, then explain line-by-line what it means: lullaby %&gt;% specify(response = Before) %&gt;% hypothesize(null = &#39;point&#39;, mu = 0.5) %&gt;% calculate(stat = &#39;t&#39;) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.674 The first line, like usual, tells R what data to work with. In the second line, we specify that the variable we want to use for our test is the Before variable. The variable we are interested in is called a response variable. In the third line, we tell R that our null hypothesis is a “point” (that is, a single value) at mu = 0.5. Finally, the fourth line tells R to calculate the t statistic based on what we told it in the first 3 lines. Let’s tell R to remember that \\(t\\) value. We’ll call it t_before, since it refers to the looking before the infants heard anything. t_before &lt;- lullaby %&gt;% specify(response = Before) %&gt;% hypothesize(null = &#39;point&#39;, mu = 0.5) %&gt;% calculate(stat = &#39;t&#39;) 9.2.2.4 Find the \\(p\\) value We can do something fancy now and visualize the appropriate \\(t\\) distribution corresponding to our null hypothesis: t_before %&gt;% visualize(method = &#39;theoretical&#39;) ## Warning: Check to make sure the conditions have been met for the theoretical ## method. {infer} currently does not check these for you. The first line tells R to use the \\(t\\) value we just found as “data.” The second line instructs R to visualize the theoretical \\(t\\) distribution if the null hypothesis were true. This distribution is what will let us calculate the \\(p\\) value. In fact, we can visualize what the \\(p\\) value will be by showing where the \\(t\\) statistic for our sample falls on the \\(t\\) distribution: t_before %&gt;% visualize(method = &#39;theoretical&#39;) + shade_p_value(obs_stat = t_before, direction = &#39;two-sided&#39;) ## Warning: Check to make sure the conditions have been met for the theoretical ## method. {infer} currently does not check these for you. All we did was add a third line where we told R to shade the area of the \\(t\\) distribution that is at least as extreme as our sample (that’s what obs_stat = t_before did). What counts as “extreme” depends on whether our test is one- or two-sided, so we had to tell R that the direction of our test was two-sided. The area of the shaded region on the plot above is what our \\(p\\) value is. But of course, we can’t get a number just by looking at a graph. Here’s how we actually get all the numbers we need out of R: lullaby %&gt;% t_test(response = Before, alternative = &#39;two-sided&#39;, mu = 0.5, conf_level = 0.95) ## # A tibble: 1 x 6 ## statistic t_df p_value alternative lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.674 31 0.505 two.sided 0.457 0.585 The first line, as usual, tells R what data to use. The remaining lines tell R all about the t_test we want to perform. We tell R that the response variable (the one we want to test) is Preference; that we are doing a test with an alternative hypothesis that is two-sided; that the mean according to the null hypothesis is mu = 0.5; and that our confidence level (conf_level) is 0.95, which is one minus our alpha level. The output we got from R told us a lot: statistic: The value of the \\(t\\) statistic. t_df: The number of degrees of freedom. p_value: The \\(p\\) value. alternative: Reminding us that we were doing a two-sided test. lower_ci: The lower limit of the updated confidence interval (assuming the population mean is equal to the sample mean). The width of the interval is defined by the conf_level we told R already. upper_ci: The upper limit of the updated confidence interval (assuming the population mean is equal to the sample mean). 9.2.2.5 Decide whether or not to reject the null hypothesis Finally, we can use the \\(p\\) value we just found, in combination with the alpha level from above (which we decided was 0.05) to say that we fail to reject the null hypothesis. As a result, we have no reason to believe that there is any bias in who the infants look at before hearing them sing. 9.2.3 Do infants prefer to look at the singer of the familiar melody? What about where infants look after they hear the two people sing? Do they show any kind of preference? Again, we can use a one-sample \\(t\\) test to find out. In fact, all we need to do is reuse different bits of code from the previous section, just changing the variable of interest from “Before” to “After.” Let’s begin by making a histogram of the After variable. Exercise 9.3 What code did you use to make the histogram above? Do you see any evidence of outliers or skew that are very different from what we might see in a normal distribution? Our research question is, “do infants prefer to look at one person over the other after hearing them sing?” 9.2.3.1 State your hypotheses Again, an infant without any preference would look equally often at both people, so the proportion of the time they looked at the singer of the familiar song would be 0.5. This is, again, our null hypothesis: \\(H_0\\): \\(\\mu = 0\\). Again, our alternative hypothesis is just that \\(H_1\\): \\(\\mu \\neq 0\\). 9.2.3.2 Set your alpha level Let’s keep our alpha level of 0.05. 9.2.3.3 Find the \\(t\\) value Let’s tell R to remember our new \\(t\\) value so that we can visualize it like we did before. We’re going to call this new \\(t\\) value t_after to keep it distinct. Exercise 9.4 What code did you use to make t_after? (Hint: what do you change in the code we used above to make t_before?) 9.2.3.4 Find the \\(p\\) value Let’s visualize our \\(t\\) statistic to get a sense of what the \\(p\\) value will be t_after %&gt;% visualize(method = &#39;theoretical&#39;) + shade_p_value(obs_stat = t_after, direction = &#39;two-sided&#39;) ## Warning: Check to make sure the conditions have been met for the theoretical ## method. {infer} currently does not check these for you. Finally, let’s get the whole shebang: ## # A tibble: 1 x 6 ## statistic t_df p_value alternative lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.96 31 0.00586 two.sided 0.529 0.658 Exercise 9.5 What code did you use to get the t_test output above? 9.2.3.5 Decide whether or not to reject the null hypothesis Comparing the \\(p\\) value we just found to our alpha level of 0.05, we reject the null hypothesis. We have reason to believe that the infants have a preference to look at one singer over the other. 9.2.4 Did their preferences change? Based on what we’ve done so far, it seems like before hearing the two people sing, the infants didn’t look at either one more than the other, but after hearing them sing, the infants did show a preference. Now let’s ask a more direct research question: For each infant, did hearing someone sing a familiar song lead that infant to prefer looking at that person? This question can be addressed using a paired samples \\(t\\) test. 9.2.4.1 State your hypotheses Based on the way our research question is phrased, we are looking at a one-tailed test because we are asking about a difference in a specific direction—an increase in preferential looking. So our null hypothesis is that the difference in preference is less than or equal to zero (\\(H_0\\): \\(\\mu_D \\leq 0\\)) while our alternative hypothesis is that the difference is greater than zero (\\(H_0\\): \\(\\mu_D &gt; 0\\)). 9.2.4.2 Set your alpha level Let’s stay consistent and say that our alpha level is still 0.05. 9.2.4.3 Find the \\(t\\) value Like we did above, we need to first find the difference for each infant between their looking preferences before and after hearing the people sing. We can do that in R using the mutate function, which we’ve previously used to calculate probabilities based on relative frequency. The code below creates a new variable called Difference by subtracting the Before preference from the After preference: lullaby %&gt;% mutate(Difference = After - Before) ## # A tibble: 32 x 4 ## id Before After Difference ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101 0.437 0.603 0.166 ## 2 102 0.413 0.683 0.270 ## 3 103 0.754 0.724 -0.0304 ## 4 104 0.439 0.282 -0.157 ## 5 105 0.475 0.499 0.0239 ## 6 106 0.871 0.951 0.0800 ## 7 107 0.237 0.418 0.181 ## 8 108 0.759 0.938 0.179 ## 9 109 0.416 0.5 0.0837 ## 10 110 0.800 0.586 -0.213 ## # … with 22 more rows We can insert that second line of code above in order to calculate the \\(t\\) statistic corresponding to our difference scores: t_diff &lt;- lullaby %&gt;% mutate(Difference = After - Before) %&gt;% specify(response = Difference) %&gt;% hypothesize(null = &#39;point&#39;, mu = 0) %&gt;% calculate(stat = &#39;t&#39;) Exercise 9.6 Compare the code used to make t_diff with the code we previously used to make t_before and t_after. What is similar and what is different? 9.2.4.4 Find the \\(p\\) value Now that we have found t_diff, let’s see where our sample falls on the \\(t\\) distribution corresponding to the null hypothesis: t_diff %&gt;% visualize(method = &#39;theoretical&#39;) + shade_p_value(obs_stat = t_diff, direction = &#39;greater&#39;) ## Warning: Check to make sure the conditions have been met for the theoretical ## method. {infer} currently does not check these for you. Exercise 9.7 Compare the visualization of the \\(t\\) distribution for t_diff to the ones we made for t_before and t_after. Is there a difference in which sides of the distribution are shaded? Why might this be? Finally, we can obtain the \\(p\\) value along with the rest of the results of the \\(t\\) test: ## # A tibble: 1 x 6 ## statistic t_df p_value alternative lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.42 31 0.0109 greater 0.0216 Inf Exercise 9.8 How did you modify the code we used above for the previous two \\(t\\) tests to get the result above? Do you notice anything different about the output we got from R for this one-tailed test, relative to the two-tailed tests we did above? 9.2.4.5 Decide whether or not to reject the null hypothesis Comparing the \\(p\\) value we just found to our alpha level of 0.05, we reject the null hypothesis. We have reason to believe that infants’ preferences changed after hearing the two people sing, such that they look more often at the singer of the familiar melody. 9.3 Wrap-up In this session, we saw both how to conduct all the individual calculations in a \\(t\\) test as well as how to use R to do a \\(t\\) test “all at once.” We saw how to do one-sample, paired samples, and independent samples \\(t\\) tests in step-by-step form. We also saw how to do one-sample and paired samples \\(t\\) tests in a fancier form with helpful visual aids that are better suited for dealing with real data. We can also do independent samples \\(t\\) tests in this fancier way, but we will have to wait until next week to see a demonstration. Nonetheless, it is clear that R has powerful facilities for doing the kinds of hypothesis tests that are the bread and butter of inferential statistics. "],["coderef.html", "A R Code Reference A.1 Binomial distribution A.2 Normal distribution", " A R Code Reference A.1 Binomial distribution For the following examples, I am using the following labeled values so you can see the kind of result R will give you. this_prob &lt;- 0.3 this_size &lt;- 10 given_value &lt;- 5 upper_value &lt;- 9 lower_value &lt;- 3 given_probability &lt;- 0.2 To use these bits of code yourself, just replace the label with the appropriate number or assign a new value under the same label (e.g., this_prob &lt;- 0.941). A.1.1 Probability of observing an outcome exactly equal to a given value dbinom(x = given_value, prob = this_prob, size = this_size) ## [1] 0.1029193 A.1.2 Probability of observing a value less than or equal to a given value pbinom(q = given_value, prob = this_prob, size = this_size) ## [1] 0.952651 A.1.3 Probability of observing a value greater than a given value 1 - pbinom(q = given_value, prob = this_prob, size = this_size) ## [1] 0.04734899 A.1.4 Probability of observing a value inside an interval between an upper and lower value pbinom(q = upper_value, prob = this_prob, size = this_size) - pbinom(q = lower_value, prob = this_prob, size = this_size) ## [1] 0.3503834 A.1.5 Probability of observing a value outside an interval between an upper and lower value 1 - (pbinom(q = upper_value, prob = this_prob, size = this_size) - pbinom(q = lower_value, prob = this_prob, size = this_size)) ## [1] 0.6496166 or 1 - pbinom(q = upper_value, prob = this_prob, size = this_size) + pbinom(q = lower_value, prob = this_prob, size = this_size) ## [1] 0.6496166 A.1.6 The value for which there is a given probability of observing an outcome less than or equal to that value qbinom(p = given_probability, prob = this_prob, size = this_size) ## [1] 2 A.1.7 Testing a null hypothesis The following are important bits of code for conducting null hypothesis tests with the binomial distribution. For the following examples, I have told R to remember the following labeled values, in addition to the ones above. null_prob &lt;- 0.5 sample_value &lt;- 6 alpha_level &lt;- 0.1 A.1.7.1 One-tailed test (alternative is that probability is less than null probability) A.1.7.1.1 Boundary of confidence interval ci_boundary &lt;- qbinom(p = alpha_level, prob = null_prob, size = this_size) A.1.7.1.2 \\(p\\) value pbinom(q = sample_value, prob = null_prob, size = this_size) ## [1] 0.828125 A.1.7.2 One-tailed test (alternative is that probability is greater than null probability) A.1.7.2.1 Boundary of confidence interval ci_boundary &lt;- qbinom(p = 1 - alpha_level, prob = null_prob, size = this_size) A.1.7.2.2 \\(p\\) value 1 - pbinom(q = sample_value - 1, prob = null_prob, size = this_size) ## [1] 0.3769531 A.2 Normal distribution For the following examples, I am using the following labeled values so you can see the kind of result R will give you. this_mean &lt;- 0 this_sd &lt;- 1 given_value &lt;- 1.5 upper_value &lt;- 2 lower_value &lt;- -1 given_probability &lt;- 0.2 To use these bits of code yourself, just replace the label with the appropriate number or assign a new value under the same label (e.g., this_mean &lt;- -4). A.2.1 Probability of observing a value less than or equal to a given value pnorm(q = given_value, mean = this_mean, sd = this_sd) ## [1] 0.9331928 A.2.2 Probability of observing a value greater than a given value 1 - pnorm(q = given_value, mean = this_mean, sd = this_sd) ## [1] 0.0668072 A.2.3 Probability of observing a value inside an interval between an upper and lower value pnorm(q = upper_value, mean = this_mean, sd = this_sd) - pnorm(q = lower_value, mean = this_mean, sd = this_sd) ## [1] 0.8185946 A.2.4 Probability of observing a value outside an interval between an upper and lower value 1 - (pnorm(q = upper_value, mean = this_mean, sd = this_sd) - pnorm(q = lower_value, mean = this_mean, sd = this_sd)) ## [1] 0.1814054 or 1 - pnorm(q = upper_value, mean = this_mean, sd = this_sd) + pnorm(q = lower_value, mean = this_mean, sd = this_sd) ## [1] 0.1814054 A.2.5 The value for which there is a given probability of observing an outcome less than or equal to that value qnorm(p = given_probability, mean = this_mean, sd = this_sd) ## [1] -0.8416212 A.2.6 Testing a null hypothesis The following are important bits of code for conducting null hypothesis tests with the normal distribution (see this lab for additional examples). For the following examples, I have told R to remember the following labeled values. null_mean &lt;- 0 null_sd &lt;- 1 sample_size &lt;- 30 sample_mean &lt;- 0.3 effect_size &lt;- 0.5 alpha_level &lt;- 0.1 A.2.6.1 Standard error of the mean null_sd / sqrt(sample_size) ## [1] 0.1825742 A.2.6.2 One-tailed test (alternative is that mean is less than null mean) A.2.6.2.1 Boundary of confidence interval ci_boundary &lt;- qnorm(p = alpha_level, mean = null_mean, sd = null_sd / sqrt(sample_size)) A.2.6.2.2 \\(p\\) value pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)) ## [1] 0.9498259 A.2.6.2.3 Power pnorm(q = ci_boundary, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 2.907878e-05 A.2.6.3 One-tailed test (alternative is that mean is greater than null mean) A.2.6.3.1 Boundary of confidence interval ci_boundary &lt;- qnorm(p = 1 - alpha_level, mean = null_mean, sd = null_sd / sqrt(sample_size)) A.2.6.3.2 \\(p\\) value 1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)) ## [1] 0.05017412 A.2.6.3.3 Power 1 - pnorm(q = ci_boundary, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.9274503 A.2.6.4 Two-tailed test A.2.6.4.1 Boundaries of confidence interval ci_lower &lt;- qnorm(p = alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) ci_upper &lt;- qnorm(p = 1 - alpha_level / 2, mean = null_mean, sd = null_sd / sqrt(sample_size)) A.2.6.4.2 \\(p\\) value If sample_mean &gt; null_mean: 2 * (1 - pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size))) ## [1] 0.1003482 If sample_mean &lt;= null_mean: 2 * pnorm(q = sample_mean, mean = null_mean, sd = null_sd / sqrt(sample_size)) ## [1] 0.1003482 A.2.6.4.3 Power pnorm(q = ci_lower, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) + 1 - pnorm(q = ci_upper, mean = null_mean + effect_size, sd = null_sd / sqrt(sample_size)) ## [1] 0.8629755 "],["references.html", "References", " References Anscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27(1), 17–21. Day, S. B., Motz, B. A., &amp; Goldstone, R. L. (2015). The cognitive costs of context: The effects of concreteness and immersiveness in instructional examples. Frontiers in Psychology, 6(1876), 1–13. Galton, F. (1907). Vox populi. Nature, 75(1949), 450–451. Mehr, S. A., Song, L. A., &amp; Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27(4), 486–501. Ryan, K. F., &amp; Gauthier, I. (2016). Gender differences in recognition of toy faces suggest a contribution of experience. Vision Research, 129, 69–76. Steegen, S., Dewitte, L., Tuerlinckx, F., &amp; Vanpaemel, W. (2014). Measuring the crowd within again: A pre-registered replication study. Frontiers in Psychology, 5(786), 1–8. Surowiecki, J. (2004). The wisdom of crowds. Random House. Vul, E., &amp; Pashler, H. (2008). Measuring the crowd within: Probabilistic representations within individuals. Psychological Science, 19(7), 645–647. Yu, B. P., Masoro, E. J., Murata, I., Bertrand, H. A., &amp; Lynd, F. T. (1982). Life span study of SPF Fischer 344 male rats fed ad libitum or restricted diets: Longevity, growth, lean body mass and disease. Journal of Gerontology, 37(2), 130–141. "]]
