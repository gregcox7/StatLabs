[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"collection laboratory activities part APSY210.","code":""},{"path":"lab1.html","id":"lab1","chapter":"Lab 1 Exploring Data with R","heading":"Lab 1 Exploring Data with R","text":"session, learn bit data explore using R/RStudio. point learn bit data variables get feel power tools learning use rest semester.follow along activity, asked run bits code RStudio. code can copied--pasted “Console” lower left RStudio. also asked modify code run , can typing code console.document shows snippet code, also shows typical R output result running code. example, following snippet code adds two numbers; way R prints result shown snippet code.Another point today’s activity illustrate even though statistics dealing data, data meaningful. just numbers names, peek world. offer glimpses someone’s life, workings natural process, social structure, etc. dataset limited wide glimpse gives us, point statistics learn make decisions based glimpse.","code":"\n2 + 3## [1] 5"},{"path":"lab1.html","id":"r-and-rstudio","chapter":"Lab 1 Exploring Data with R","heading":"1.1 R and RStudio","text":"labs make use RStudio, graphical interface statistical computing language R. R language represents current state art statistical computing academic industrial research. likely remain relevant many years come free open-source, meaning widely accessible improvements extensions made continuously large community professionals hobbyists. fact, many best features R using extensions made people outside “core” development team R. extensions called “packages,” represent bundles code useful statistics.RStudio interface makes easier work R language, also free can installed computers running modern operating system (Windows, Mac, Linux, etc.). R RStudio already installed computers classroom, well Technology-Enhanced Classrooms Library Public Computing Sites campus. working computer, easier time install RStudio . Installing RStudio requires installing R first.","code":""},{"path":"lab1.html","id":"installation-on-your-own-computer","chapter":"Lab 1 Exploring Data with R","heading":"1.1.1 Installation on your own computer","text":"can install R computer following instructions : https://cran.rstudio.com/installing R, can install RStudio Desktop following instructions : https://rstudio.com/products/rstudio/download/","code":""},{"path":"lab1.html","id":"running-r-in-a-browser","chapter":"Lab 1 Exploring Data with R","heading":"1.1.2 Running R in a browser","text":"Even don’t access computer RStudio installed locally, can use access internet. can run RStudio online : https://rstudio.cloud/. downside cap amount time can spend using online version, better using local installation whenever possible. may need create account use online version, free .","code":""},{"path":"lab1.html","id":"required-packages","chapter":"Lab 1 Exploring Data with R","heading":"1.2 Required packages","text":"RStudio running, run following line code copying pasting “>” “Console” lower left pane RStudio window.working computer campus, see messages similar ones shown . messages tell us R loaded helpful “packages” “library” use. may notice messages “conflicts,” nothing need worry .working computer online version RStudio, might gotten error tried run last line. get error, run following line install “tidyverse” package need course.Installing “tidyverse” package takes , ’ll need . installed, can just run library(tidyverse) start RStudio everything need.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\ninstall.packages(\"tidyverse\")"},{"path":"lab1.html","id":"troubleshooting-tip","chapter":"Lab 1 Exploring Data with R","heading":"1.2.1 Troubleshooting tip","text":"“tidyverse” package automatically loaded first start RStudio, make sure run linefirst thing time open RStudio. Chances , try running something get error, “tidyverse” package loaded. course, ’s possible thing can cause error! ’s good try first see clears things .","code":"\nlibrary(tidyverse)"},{"path":"lab1.html","id":"meet-your-data","chapter":"Lab 1 Exploring Data with R","heading":"1.3 Meet your data","text":"data looking passenger records RMS Titanic, oceanliner famously sank April 15, 1912. Though liner filled capacity, lax safety precautions—including failure carry enough lifeboats—meant many passengers died unable evacuate ship struck iceberg.","code":""},{"path":"lab1.html","id":"load-the-data","chapter":"Lab 1 Exploring Data with R","heading":"1.3.1 Load the data","text":"Run following line code console load data RStudio “environment”:now see “titanic” pop upper right hand pane RStudio window. Click “titanic” upper right take look data.","code":"\ntitanic <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/titanic.csv\")## Rows: 1309 Columns: 11## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (7): residence, sex, name, ticket, cabin, embarked, hometown\n## dbl (3): class, age, fare\n## lgl (1): survived## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab1.html","id":"check-out-the-variables","chapter":"Lab 1 Exploring Data with R","heading":"1.3.2 Check out the variables","text":"RStudio looks like (helpful colored labels):upper left RStudio screen, ’ll see bunch columns. data “raw” form. row represents specific passenger (“case”) column represents different variable.Exercise 1.1  Find example following types variable dataset. Explain reasoning choice.Numerical (either discrete continuous)Ordinal categoricalNominal categorical","code":""},{"path":"lab1.html","id":"answering-questions-with-data","chapter":"Lab 1 Exploring Data with R","heading":"1.4 Answering questions with data","text":"Now ’ve gotten acquainted kind data , can begin using answer questions. involve simplifying data, turning summary form makes easier understand. summaries fall heading “descriptive statistics,” meant describe important aspects data. four types summaries explore today frequency tables, proportions, bar charts, histograms. questions attempt answer survived perished Titanic.","code":""},{"path":"lab1.html","id":"frequency-tables","chapter":"Lab 1 Exploring Data with R","heading":"1.4.1 Frequency tables","text":"One way answer literally question, “survived died Titanic?” read names 1300 passengers dataset. problem give us much sense particular people might survived versus . Treating survival response variable, like treat variables data explanatory variables. gives us sense types people less likely survived, tells us Titanic disaster whole.begin, first thing can construct frequency table simply counts number people survived number people died. summary give us sense scale disaster. RStudio window, see big open space just data? called “console” work. Copy paste code “Console.” code appear right “>.” , hit enter run see results.got table counted number people survive. went 1300 rows multiple variables just two numbers. pretty concise summary! ? Let’s break bit code:titanic name dataset.group_by(survived) tells R group cases dataset whether survived (TRUE) (FALSE).summarize(n=n()) tells R take grouped cases summarize counting number people group labeling resulting number “n.”funky symbol %>% connects three steps makes sure R order want. symbol called “pipe.”Let’s try things get sense code . happens change n = n() last line Number = n()?Everything looks except instead column labeled “n,” labeled “Number.” bit equals sign frequency table labeled.Now let’s try something seems like small change: Instead n = n() last line, let’s write n = m(). one letter, surely can’t big difference?R doesn’t like ! reports error doesn’t know m(). ’s n() function, instruction tells R count number something. hand, m() doesn’t mean anything R throws hands.can also get counts based variables. example, let’s ask many passengers “class” changing grouping variable code:Since ’re roll, let’s see can count number passengers without college degrees:Didn’t work! got error message.Exercise 1.2  Explain words trying put degree group_by didn’t work.Finally, let’s construct frequency table using multiple variables. lets us answer complex questions like, many British women aboard Titanic? question involves two variables, residence (people ) sex (whether classified male female). can put variables group_by line find answer.Exercise 1.3  Make frequency table answers question, “many second class passengers survived?” find helpful try filling blank :final code used? many second class passengers survived?","code":"\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = n())## # A tibble: 2 × 2\n##   survived     n\n##   <lgl>    <int>\n## 1 FALSE      809\n## 2 TRUE       500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(Number = n())## # A tibble: 2 × 2\n##   survived Number\n##   <lgl>     <int>\n## 1 FALSE       809\n## 2 TRUE        500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = m())## Error: Problem with `summarise()` input `n`.\n## x could not find function \"m\"\n## ℹ Input `n` is `m()`.\n## ℹ The error occurred in group 1: survived = FALSE.\ntitanic %>%\n  group_by(class) %>%\n  summarize(n = n())## # A tibble: 3 × 2\n##   class     n\n##   <dbl> <int>\n## 1     1   323\n## 2     2   277\n## 3     3   709\ntitanic %>%\n  group_by(degree) %>%\n  summarize(n = n())## Error: Must group by variables found in `.data`.\n## * Column `degree` is not found.\ntitanic %>%\n  group_by(residence, sex) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'residence'. You can override using the `.groups` argument.## # A tibble: 6 × 3\n## # Groups:   residence [3]\n##   residence sex        n\n##   <chr>     <chr>  <int>\n## 1 American  Female   108\n## 2 American  Male     150\n## 3 British   Female    94\n## 4 British   Male     208\n## 5 Other     Female   264\n## 6 Other     Male     485titanic %>%\n  group_by(___) %>%\n  summarize(n = n())"},{"path":"lab1.html","id":"titanic-props","chapter":"Lab 1 Exploring Data with R","heading":"1.4.2 Proportions","text":"may heard , trying evacuate Titanic, rule put “women children first” onto lifeboats. suggests hypothesis, assuming rule actually followed: female passengers survived often male passengers.see data consistent hypothesis, can begin using code similar ’ve using count number male female passengers either survive.table contains information need test hypothesis, hard read different numbers male female passengers. want know whether greater proportion female passengers survived, compared proportion male passengers survived.find proportions taking count dividing sum counts. Specifically, group “” want find proportion elements group “” characteristic “B,” find\\[\n\\text{Proportion B} = \\frac{\\text{Number B}}{\\text{Total number 's}}\n\\]\ncan code adding line code used make counts.new column p proportion represents proportion people group (either male female) either survive. now easier see proportion much higher female male passengers, consistent hypothesis.Exercise 1.4  Write run code gives us table, like one , shows proportion people Class (First, Second, Third) survived died. , find helpful start code already used modify accordingly.code use? Class highest proportion survivors?","code":"\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.## # A tibble: 4 × 3\n## # Groups:   sex [2]\n##   sex    survived     n\n##   <chr>  <lgl>    <int>\n## 1 Female FALSE      127\n## 2 Female TRUE       339\n## 3 Male   FALSE      682\n## 4 Male   TRUE       161\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   sex [2]\n##   sex    survived     n     p\n##   <chr>  <lgl>    <int> <dbl>\n## 1 Female FALSE      127 0.273\n## 2 Female TRUE       339 0.727\n## 3 Male   FALSE      682 0.809\n## 4 Male   TRUE       161 0.191"},{"path":"lab1.html","id":"titanic-bar","chapter":"Lab 1 Exploring Data with R","heading":"1.4.3 Bar charts","text":"looking patterns trends data, often easier see visualization rather table numbers. Bar charts make numerical relationships easy see visually, don’t need compare bunch numbers.example, made table count number passengers class. bar chart conveys information terms height bars.Pretty neat! now easy see many 3rd class passengers 1st 2nd, interestingly, fewer 2nd class 1st class passengers.code used similar ’ve using, differs important ways:\n* first line , telling R dataset using (titanic).\n* second line tells R want make plot want put variable class along horizontal axis plot (x axis). “gg” front “plot” refers “grammar graphics,” language R uses describe plots. language, different parts plot called “aesthetics,” x = class falls inside parenthetical labeled aes(thetic).\n* final line just tells R want make bar chart. grammar graphics, different types charts called geoms.\n* Notice second 2 lines connected + rather %>% symbol. historical accident, meaning two symbols basically . telling R order follow instructions.put bar charts side--side, can use compare groups. R, putting multiple graphs together called faceting. graph “facet.” can tell R make facets based specific variable adding line code, like :line end splits plot different “facets,” one level residence variable. Note put “faceting” variable name quotes (reason). result makes easy see distribution passengers across classes different depending —Americans Titanic tended wealthier first class passengers, relative passengers Britain elsewhere.Exercise 1.5  Make bar chart shows number people either survive depending country residence. , fill blanks code :code use? possible reason relative number survivors different depending passengers ?Attach copy plot made. can saving chart file using “Export” menu “Plots” pane RStudio.","code":"\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar()\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar() +\n  facet_wrap(\"residence\")titanic %>%\n  ggplot(aes(x = ___)) +\n  geom_bar() +\n  facet_wrap(\"___\")"},{"path":"lab1.html","id":"histograms","chapter":"Lab 1 Exploring Data with R","heading":"1.4.4 Histograms","text":"far, summarizing categorical variables. also numerical variables data, example age passenger well much paid tickets. Let’s try making frequency table figure many people different ages sailed Titanic:Well ’s helpful! R didn’t even bother show us whole thing. Though can see something interesting: Age measured years, passengers least one year old, age whole number. fractions years passengers less year old—ages measured months rather years.main point even though age can measured less fine-grained manner, age effectively continuous. don’t want know many passengers exactly 31.3491 years old, want get general sense distribution ages across passengers.can construct summary conveys information using histogram. histogram similar bar chart; difference bar charts categorical variables histograms numerical variables. code constructs histogram summarize passenger age:resulting histogram shows bunch bars, height indicate number passengers within particular age range. Notice got couple messages R addition plot, one “non-finite values” another “picking better value.” R says, “non-finite values,” talking people age recorded. unfortunate thing real data: sometimes missing pieces. didn’t stop R making plot wanted using non-missing data, R wanted warn us just case.message “picking better value” important: make histogram, looking many things fall within particular range values, say, ages 4 8. decide ranges? don’t tell R , decide divide range values 30 “bins,” corresponds range values width. usually want.Instead, decide big small want ranges . following code tells R make histogram using “bins” 2 years “wide” (0-1, 2-3, etc.):just like bar charts, can split histogram different “facets.” pair histograms shows distribution ages passengers either survive:Exercise 1.6  Try making several different histograms passenger age split survival, using different bin widths:bin width believe gives best visual summary ? shapes histograms suggest influence “women children first” rule survived?Attach copy plot made favorite bin width (, can saving plot file using “Export” menu RStudio’s Plots pane).","code":"\ntitanic %>%\n  group_by(age) %>%\n  summarize(n = n())## # A tibble: 99 × 2\n##      age     n\n##    <dbl> <int>\n##  1 0.167     1\n##  2 0.333     1\n##  3 0.417     1\n##  4 0.667     1\n##  5 0.75      3\n##  6 0.833     3\n##  7 0.917     2\n##  8 1        10\n##  9 2        12\n## 10 3         7\n## # … with 89 more rows\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2)## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(\"survived\")## Warning: Removed 263 rows containing non-finite values (stat_bin).titanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = ___) +\n  facet_wrap(\"survived\")"},{"path":"lab1.html","id":"wrap-up","chapter":"Lab 1 Exploring Data with R","heading":"1.5 Wrap-up","text":"Today began adventure using RStudio explore data. saw look data summarize various helpful ways. frequency tables, bar charts, histograms.Frequency tables count number times particular value particular variable (combination values across multiple variables) occurs dataset.can use counts frequency tables calculate proportions, better conveying relative values.Bar charts display counts categorical variables visual form makes easier compare .Histograms let us visually summarize counts numerical variables putting “bins,” width need decide.","code":""},{"path":"lab2.html","id":"lab2","chapter":"Lab 2 Sampling","heading":"Lab 2 Sampling","text":"previous activity, got exposure can explore data R way can use data help answer questions. activity, get sense data come processes produce data can force us change interpretations.Generally, data come sample larger population. Depending sample selected, may give us biased perspective larger population. particular, non-random samples (observational studies), must think carefully processes cases end sample. processes, might social nature, introduce confounding variables, can systematic differences different groups sample, requiring us interpret data differently.","code":""},{"path":"lab2.html","id":"first-things-first","chapter":"Lab 2 Sampling","heading":"2.1 First things first","text":"First, start RStudio.usual, next thing starting RStudio load tidyverse package R’s library using following line code (run RStudio console, like last time):Now ’s done, can get data.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()"},{"path":"lab2.html","id":"sex-bias-sampling-bias-or-both","chapter":"Lab 2 Sampling","heading":"2.2 Sex Bias, Sampling Bias, or Both?","text":"1973, University California Berkeley noted disturbing fact graduate school admissions: considerably male applicants admitted female applicants. University, fearing lawsuit, wanted know reflected systematic sex discrimination admissions offices. shall see story turned complex (full details, see Bickel et al. (1973)).","code":""},{"path":"lab2.html","id":"load-the-data-1","chapter":"Lab 2 Sampling","heading":"2.2.1 Load the data","text":"First, run code load 1973 Berkeley admissions data workspace:berkeley dataset now visible “Environment” pane upper right corner RStudio. Click look raw data.applicant selected sample, clearly random. row data refers specific applicant 1973. applicant, three observed variables:Admit: Either “Admitted” “Rejected,” depending whether applicant admitted rejected.Gender: Either “Male” “Female.”Department: letter “” “F”; department names obscured privacy.Exercise 2.1  type (nominal categorical, ordinal categorical, numerical) variables dataset?","code":"\nberkeley <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/berkeley.csv\")## Rows: 4526 Columns: 3## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): Admit, Gender, Department## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab2.html","id":"are-more-males-than-females-admitted","chapter":"Lab 2 Sampling","heading":"2.2.2 Are more males than females admitted?","text":"noted , Berkeley concerned greater proportion male applicants admitted female applicants. verify whether true, let’s find proportions running following chunk code:rightmost column table (labeled p) gives proportion applicants gender either admitted rejected. Based table, looks like Berkeley right concerned—proportion male applicants admitted higher proportion female applicants admitted.Exercise 2.2  Compare chunk code just ran code used last activity section. parts looks similar parts look different?","code":"\nberkeley %>%\n  group_by(Gender, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Gender'. You can override using the `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   Gender [2]\n##   Gender Admit        n     p\n##   <chr>  <chr>    <int> <dbl>\n## 1 Female Admitted   557 0.304\n## 2 Female Rejected  1278 0.696\n## 3 Male   Admitted  1198 0.445\n## 4 Male   Rejected  1493 0.555"},{"path":"lab2.html","id":"is-there-a-bias-in-all-departments","chapter":"Lab 2 Sampling","heading":"2.2.3 Is there a bias in all departments?","text":"Even overall bias, case departments, just ?Exercise 2.3  Make new table gives proportion male female applicants admitted department. may find helpful fill blanks following chunk code (note new line end “filters” redundant rows table make long):departments admit higher proportion female applicants admit higher proportion male applicants?","code":"berkeley %>%\n  group_by(___, ___, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n)) %>%\n  filter(Admit == \"Admitted\")"},{"path":"lab2.html","id":"resolving-the-paradox","chapter":"Lab 2 Sampling","heading":"2.2.4 Resolving the paradox","text":"appear paradox hands: ignore differences departments, likely male applicant gets admitted female applicant. within departments, opposite true: greater proportion female applicants admitted male applicants. words, seems confounding variable work, related differences departments.try figure confound might , look two additional issues. first issue , department, proportion applicants female, regardless whether admitted ? can address question running following chunk code:Exercise 2.4  Based table just produced, departments male female applicants? overlap departments identified previous exercise admitting higher proportion female male applicants?second issue consider overall rate admission department.Exercise 2.5  Make table shows proportion applicants admitted department, regardless gender. may find useful modify chunk code just used.code use? Put departments order highest lowest admission rate.Finally, let’s create visualization may help us put pieces together. chunk code produces set bar charts, one department. similar made previous activity, new “aesthetic,” namely, use variable (Gender) fill bars different colors:Exercise 2.6  Compare code just ran create colored bar chart kind code used make bar charts previous activity. similar different?plot just made, total height bar represents total number applicants department either admitted rejected. bar divided two parts different colors, representing numbers male female applicants bar. can use total height bars see relative number people admitted vs. rejected department; can use amount red vs. teal panel see relative number female vs. male applicants department.Putting pieces together, found following:Across departments, proportion female applicants admitted lower proportion male applicants admitted.Within departments, proportion female applicants admitted higher proportion male applicants admitted.Departments high overall admission rates receive fewer female applicants departments low admission rates.Finally, note , although names specific departments removed, departments B (easy get received predominantly male applicants) physical sciences engineering whereas departments E F (hard get received considerably female applicants) social sciences humanities.Exercise 2.7  Even though results suggest sex bias admissions level individual departments, kinds bias results suggest might going ? potential biases related types sampling biases ’ve discussed class book?thinking , may help think two questions (though certainly may relevant):might someone choose apply particular department?might departments able admit applicants others?","code":"\nberkeley %>%\n  group_by(Department, Gender) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Department'. You can override using the `.groups` argument.## # A tibble: 12 × 4\n## # Groups:   Department [6]\n##    Department Gender     n      p\n##    <chr>      <chr>  <int>  <dbl>\n##  1 A          Female   108 0.116 \n##  2 A          Male     825 0.884 \n##  3 B          Female    25 0.0427\n##  4 B          Male     560 0.957 \n##  5 C          Female   593 0.646 \n##  6 C          Male     325 0.354 \n##  7 D          Female   375 0.473 \n##  8 D          Male     417 0.527 \n##  9 E          Female   393 0.673 \n## 10 E          Male     191 0.327 \n## 11 F          Female   341 0.478 \n## 12 F          Male     373 0.522\nberkeley %>%\n  ggplot(aes(x = Admit, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(\"Department\")"},{"path":"lab2.html","id":"wrap-up-1","chapter":"Lab 2 Sampling","heading":"2.3 Wrap-up","text":"paradox confronted activity name: “Simpson’s Paradox.” Check good video demonstration paradox .Simpson’s Paradox occurs whenever pattern appears aggregate (like bias female applicants departments) disappears reverses look different subgroups (like apparent bias favor female applicants within departments). done activity, Simpson’s Paradox can resolved careful exploration data consideration potential confounding variables associated different subgroups.","code":""},{"path":"lab3.html","id":"lab3","chapter":"Lab 3 Describing data","heading":"Lab 3 Describing data","text":"session divided two parts. first, use R understand two measures variability work: standard deviation close cousin, variance. see valuable computer can quickly produce numerical visual summaries data.computers great mindless computation, means us people better mindful stuff. need think numbers mean may important understanding world.second part session analyze data experiment. use visual numerical summaries help draw conclusions going experiment.","code":""},{"path":"lab3.html","id":"load-the-tidyverse-library","chapter":"Lab 3 Describing data","heading":"3.1 Load the tidyverse library","text":"starting RStudio, begin making sure ’ve got tidyverse package loaded R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()"},{"path":"lab3.html","id":"variance-and-standard-deviation","chapter":"Lab 3 Describing data","heading":"3.2 Variance and standard deviation","text":"Two important widely-used numerical summaries variability variance standard deviation. Although two different names, really two ways looking thing. standard deviation often used summary variability, although shall see variance useful purposes later course.summaries involve somewhat long chain calculations. important see chain understand numbers telling us. part session, use R find variance standard deviation simple simulated data. First, follow long chain. , see find summaries quickly efficiently.","code":""},{"path":"lab3.html","id":"the-big-picture","chapter":"Lab 3 Describing data","heading":"3.2.1 The big picture","text":"good numerical summary variability tell us much expect observed values variable differ “typical.” “standard deviation” means: summary typical (“standard”) amount observed values variable tend differ (“deviate”) central tendency. variance standard deviation based using mean describe central tendency.","code":""},{"path":"lab3.html","id":"load-the-data-2","chapter":"Lab 3 Describing data","heading":"3.2.2 Load the data","text":"see variance standard deviation calculated, let’s create small fake data set, creatively call my_data. contains 5 observations single variable called “X.”Although data real, similar one might observe random sample IQ scores (mean 100 standard deviation 15 typical population).","code":"\nmy_data <- tibble(X = c(91, 102, 107, 109, 111))"},{"path":"lab3.html","id":"step-by-step","chapter":"Lab 3 Describing data","heading":"3.2.3 Step by step","text":"now follow chain calculations, step step, results finding variance standard deviation set observed values like “X” variable simulated data.","code":""},{"path":"lab3.html","id":"find-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.1 Find the mean","text":"First, find mean observed values. R , many ways, fancy calculator. find mean observed values X variable plugging formula mean already know:using R calculator advantages hand, downside still type/copy numbers. Moreover, wanted use result something else (), ’d copy , , greater chances make mistake.Rather typing numbers manually, instead use R’s mean function within “summarize” line ’ve used :Exercise 3.1  ’ve used “summarize” function previous sessions, always preceded group_by line. Say words didn’t use group_by line find mean chunk code just ran.","code":"\n(91 + 102 + 107 + 109 + 111) / 5## [1] 104\nmy_data %>%\n    summarize(mean(X))## # A tibble: 1 × 1\n##   `mean(X)`\n##       <dbl>\n## 1       104"},{"path":"lab3.html","id":"find-the-deviations-from-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.2 Find the deviations from the mean","text":"next step find, observed value data, deviation mean. mathematical notation, deviation can written \\((x_i - \\bar{x})\\) \\(x_i\\) shorthand observed value \\(\\bar{x}\\) mean.previous sessions, ’ve used mutate line transform counts proportions. Now use find deviations:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X))## # A tibble: 5 × 2\n##       X deviation\n##   <dbl>     <dbl>\n## 1    91       -13\n## 2   102        -2\n## 3   107         3\n## 4   109         5\n## 5   111         7"},{"path":"lab3.html","id":"square-the-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.3 Square the deviations","text":"Now ’ve found deviations, square , using mutate line:Note , R, caret symbol ^ used exponentiation.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2)## # A tibble: 5 × 3\n##       X deviation squared_deviation\n##   <dbl>     <dbl>             <dbl>\n## 1    91       -13               169\n## 2   102        -2                 4\n## 3   107         3                 9\n## 4   109         5                25\n## 5   111         7                49"},{"path":"lab3.html","id":"add-up-the-squared-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.4 Add up the squared deviations","text":"Now, add squared deviations using sum function part summarize line:Exercise 3.2  Modify chunk code just ran add deviations instead squared_deviations.code use?sum deviations?Relate sum deviations idea mean “balance point” distribution numbers.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation))## # A tibble: 1 × 1\n##   sum_squared_deviation\n##                   <dbl>\n## 1                   256"},{"path":"lab3.html","id":"variance-divide-by-n---1","chapter":"Lab 3 Describing data","heading":"3.2.3.5 Variance: Divide by \\(n - 1\\)","text":"variance sum squared deviations divided \\(n - 1\\), \\(n\\) number observed values, “sample size.” get \\(n\\), can include n = n() summarize line use another mutate line get variance:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1))## # A tibble: 1 × 3\n##   sum_squared_deviation     n variance\n##                   <dbl> <int>    <dbl>\n## 1                   256     5       64"},{"path":"lab3.html","id":"standard-deviation-take-the-square-root","chapter":"Lab 3 Describing data","heading":"3.2.3.6 Standard deviation: Take the square root","text":"Finally, get standard deviation taking square root variance, final mutate line. R, sqrt function stands square root.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1)) %>%\n    mutate(standard_deviation = sqrt(variance))## # A tibble: 1 × 4\n##   sum_squared_deviation     n variance standard_deviation\n##                   <dbl> <int>    <dbl>              <dbl>\n## 1                   256     5       64                  8"},{"path":"lab3.html","id":"all-at-once","chapter":"Lab 3 Describing data","heading":"3.2.4 All at once","text":"just saw complete chain calculations involved finding variance standard deviation. might guessed fact R mean function, also functions directly calculate variances standard deviations. functions called var sd, respectively, can use summary line, like :code just ran much compact, readable, less liable lead typos, naturally use built-var sd functions pretty often!Exercise 3.3  Take look used mutate summarize lines calculating variance standard deviation step--step. words, describe basic difference mutate line used summarize line used .Hint: Think result got adding either mutate summarize line affected number rows /columns result.","code":"\nmy_data %>%\n    summarize(mean = mean(X), variance = var(X), standard_deviation = sd(X))## # A tibble: 1 × 3\n##    mean variance standard_deviation\n##   <dbl>    <dbl>              <dbl>\n## 1   104       64                  8"},{"path":"lab3.html","id":"diet-and-lifespan","chapter":"Lab 3 Describing data","heading":"3.3 Diet and Lifespan","text":"seen use R calculate numerical summaries central tendency variability, let’s see operate real data. data looking come study Yu et al. (1982). studied lifespan sample rats randomly assigned one two different diets: One group rats allowed eat freely, however wanted; another group fed restricted diet 60% calories free-eating rats .","code":""},{"path":"lab3.html","id":"load-the-data-3","chapter":"Lab 3 Describing data","heading":"3.3.1 Load the data","text":"First, need get data R. following code download import data current R session:“Environment” panel upper right, ’ll see new entry called “rats.” data just imported. Click look data, appear upper left. just two variables dataset, Diet Lifespan (measured days). Diet explanatory variable Lifespan response variable.","code":"\nrats <- read_csv('https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv')## Rows: 195 Columns: 2## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Diet\n## dbl (1): Lifespan## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab3.html","id":"summarizing-the-data-visually","chapter":"Lab 3 Describing data","heading":"3.3.2 Summarizing the data visually","text":"Although numerical summaries useful, always accompanied visual summaries order get complete understanding data.","code":""},{"path":"lab3.html","id":"histograms-1","chapter":"Lab 3 Describing data","heading":"3.3.2.1 Histograms","text":"Let’s make facetted histogram shows distribution lifespans rats diet. use option facet_wrap lets us set number columns (ncol) can see facets stacked atop one another.Exercise 3.4  First, let’s describe can see histograms (may also try bin widths histograms like).Describe shape two distributions. sure note number modes, skewness, whether may outliers either group.Compare two distributions. sure note whether seem differences central tendency variability two groups.","code":"\nrats %>%\n    ggplot(aes(x=Lifespan)) +\n    geom_histogram(binwidth=90) +\n    facet_wrap(\"Diet\", ncol = 1)"},{"path":"lab3.html","id":"boxplots","chapter":"Lab 3 Describing data","heading":"3.3.2.2 Boxplots","text":"’ve also seen boxplots can useful visual summaries data. easy produce R using geom_boxplot instead geom_histogram:Exercise 3.5  Compare boxplots histograms just made.Compare chunks code used make faceted histogram code used make boxplot. Note similarities differences, especially different variable names appear two chunks code.Describe anything data might easier see boxplot opposed histogram.","code":"\nrats %>%\n    ggplot(aes(x=Lifespan, y=Diet)) +\n    geom_boxplot()"},{"path":"lab3.html","id":"summarizing-the-data-numerically","chapter":"Lab 3 Describing data","heading":"3.3.3 Summarizing the data numerically","text":"Finally, let’s use R get numerical summary lifespans rats two diets. first part session, focused mean standard deviation (variance), also seen median inter-quartile range (IQR) can also used numerical summaries central tendency variability.R can find median IQR us:Exercise 3.6  Fill blanks following chunk code find mean standard deviation lifespans rats diet:code use?Hint: ’ve filled blanks correctly, get following result.Compare means standard deviations histograms made earlier. differences mean lifespan groups reflected two histograms? differences standard deviation lifespan reflected two histograms?numerical summaries central tendency variability provide concise description lifespans rats two different diets. descriptions help us address research question: restricted diet effect lifespan?Exercise 3.7  Based visual numerical summaries made, let us draw conclusions research question.Describe differences lifespan rats restricted diet versus rats free diet.Can conclude differences diet caused observed differences lifespan?population generalize results?","code":"\nrats %>%\n    group_by(Diet) %>%\n    summarize(Median = median(Lifespan), IQR = IQR(Lifespan))## # A tibble: 2 × 3\n##   Diet       Median   IQR\n##   <chr>       <dbl> <dbl>\n## 1 Free         710   116 \n## 2 Restricted  1036.  300.___ %>%\n    group_by(___) %>%\n    ___(Mean = ___, SD = ___)## # A tibble: 2 × 3\n##   Diet        Mean    SD\n##   <chr>      <dbl> <dbl>\n## 1 Free        684.  134.\n## 2 Restricted  969.  285."},{"path":"lab3.html","id":"wrap-up-2","chapter":"Lab 3 Describing data","heading":"3.4 Wrap-up","text":"session, saw find two important numerical summaries variability: variance standard deviation. analyzed data experiment studying relationship diet lifespan sample rats, using visual numerical summaries get complete picture relationships explanatory response variables.","code":""},{"path":"lab4.html","id":"lab4","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"Lab 4 Relationships Between Numerical Variables","text":"session, use R help us describe relationships numerical variables. Typically, purpose see well can predict explain differences response variable terms differences explanatory variable. seen, several useful tools work : scatterplots, correlation, linear regression.","code":""},{"path":"lab4.html","id":"load-the-tidyverse","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.1 Load the tidyverse","text":"usual, started RStudio, first thing sure load tidyverse package R’s library using line :","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()"},{"path":"lab4.html","id":"always-plot-your-data","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.2 Always plot your data","text":"Numerical summaries data extremely useful compact descriptions things like central tendency variability. also seen Pearson correlation coefficient useful summary strength direction relationship numerical variables. numerical summaries can also misleading, shall see following example.","code":""},{"path":"lab4.html","id":"load-the-data-4","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.2.1 Load the data","text":"First, need import data R using code .data artificial devised Anscombe (1973).","code":"\nanscombe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv\")## Rows: 44 Columns: 3## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Group\n## dbl (2): X, Y## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"correlations-for-different-sets-of-measurements","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.2.2 Correlations for different sets of measurements","text":"four groups observations data. groups labeled “,” “B,” “C,” “D.” Observations made two variables, labeled “X” “Y.” Click anscombe environment panel RStudio (upper right) take look.Using R, can quickly get numerical summaries data. code provides mean standard deviation X Y values group, well correlation X Y group.notice anything funny results? looks like basically differences groups!","code":"\nanscombe %>%\n    group_by(Group) %>%\n    summarize(Mean_X = mean(X), Mean_Y = mean(Y), SD_X = sd(X), SD_Y = sd(Y), r = cor(X, Y))## # A tibble: 4 × 6\n##   Group Mean_X Mean_Y  SD_X  SD_Y     r\n##   <chr>  <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 A          9   7.50  3.32  2.03 0.816\n## 2 B          9   7.50  3.32  2.03 0.816\n## 3 C          9   7.5   3.32  2.03 0.816\n## 4 D          9   7.50  3.32  2.03 0.817"},{"path":"lab4.html","id":"scatterplots","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.2.3 Scatterplots","text":"Rather numerical summary, now let’s use R visually summarize data using scatterplots. put X variable horizontal (“x”) axis Y variable vertical (“y”) axis. use Group “facetting” variable:Exercise 4.1  scatterplots , respond following questions:scatterplot indicate probably association two variables? , describe type relationship.correlation coefficient (r) provide good summary whether two variables related? ?","code":"\nanscombe %>%\n    ggplot(aes(x=X, y=Y)) +\n    geom_point() +\n    facet_wrap(\"Group\")"},{"path":"lab4.html","id":"and-now-for-a-word","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3 And now for a word","text":"Psycholinguistics study perceptual cognitive processes involved learning, understanding, producing language. One ways psycholinguists study language processing using “lexical decision task.” lexical decision task, participants shown strings letters; sometimes, make real words (like “AUTHOR”) sometimes don’t (like “AWBLOR”). time someone takes decide string letters real word (AUTHOR) measure easily knowledge word can accessed. looking relationships lexical decision time (LDT) different properties word, can begin understand processes organize access knowledge language. words, lexical decision time tells us “mental dictionary” structured.English Lexicon Project collecting kind data lot people many different words English language. report mean lexical decision time word, along number properties word. treat mean lexical decision time response variable examine relationships number explanatory variables.","code":""},{"path":"lab4.html","id":"load-the-data-5","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.1 Load the data","text":"Run following line code download subset data English Lexicon Project.case/observation dataset particular word. addition lexical decision time, several potential explanatory variables measured word.","code":"\nelp <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/elp.csv\")## Rows: 31433 Columns: 25## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): Word, POS\n## dbl (23): Length, Freq_KF, Freq_HAL, SUBTLWF, SUBTLCD, Ortho_N, Phono_N, OLD...## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"examine-the-distribution-of-lexical-decision-times","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.2 Examine the distribution of lexical decision times","text":"LDT variable contains lexical decision time word dataset. LDT measured milliseconds. Let us first examine distribution lexical decision times using code :Exercise 4.2  Describe distribution lexical decision times across words (can re-run code look binwidths like). sure note number modes, skewness, whether potential outliers.","code":"\nelp %>%\n  ggplot(aes(x = LDT)) +\n  geom_histogram(binwidth=20)"},{"path":"lab4.html","id":"word-length-as-an-explanatory-variable","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.3 Word length as an explanatory variable","text":"natural research question ask point whether LDT can explained word length. words, number letters word affect easy recognize?","code":""},{"path":"lab4.html","id":"scatterplot","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.3.1 Scatterplot","text":"First, make scatterplot two variables. Word length recorded variable named Length.scatterplot suggests positive relationship described line.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point()"},{"path":"lab4.html","id":"overlaying-a-line","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.3.2 Overlaying a line","text":"can easily put best-fitting linear regression line top scatterplot get visual sense well linear model describe relationship. , add line called geom_smooth(method = \"lm\"). geom_smooth puts “smooth” lines curves plot, including method = \"lm\" parentheses tells R specifically want linear model.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"lab4.html","id":"finding-the-correlation","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.3.3 Finding the correlation","text":"can get correlation coefficient using cor function, like :","code":"\nelp %>%\n    summarize(r = cor(LDT, Length))## # A tibble: 1 × 1\n##       r\n##   <dbl>\n## 1 0.543"},{"path":"lab4.html","id":"finding-the-slope-and-intercept","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.3.4 Finding the slope and intercept","text":"Finally, can get slope intercept best-fitting line. line code looks little different ’ve done things now. just single line called “lm” “linear model.” parentheses, two instructions separated comma. first says explanatory response variables using funky format [response variable name] ~ [explanatory variable name]. second instruction parentheses tells R data find variables .’s looks together:result line gives us intercept best fitting line, well slope. slope labeled terms name explanatory variable.Exercise 4.3  Based result got running code , additional letter word , much longer take recognize word?","code":"\nlm(LDT ~ Length, data = elp)## \n## Call:\n## lm(formula = LDT ~ Length, data = elp)\n## \n## Coefficients:\n## (Intercept)       Length  \n##      546.33        28.15"},{"path":"lab4.html","id":"age-of-acquisition-as-an-explanatory-variable","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.4 Age of Acquisition as an explanatory variable","text":"Although makes sense longer word take longer recognize, another important aspect word learned. reasonable think word learn early life easier access one learned recently.English Lexicon Project also records mean “age acquisition” word. mean age (years) someone first learns word. variable labeled Age_Of_Acquisition elp data. Now, follow steps looking relationship Length LDT, instead Length explanatory variable, use “age acquisition.”following exercises, sure refer code used previous section.","code":""},{"path":"lab4.html","id":"scatterplot-1","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.4.1 Scatterplot","text":", let’s first make scatterplot visualize relationship LDT age acquisition.Exercise 4.4  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis):code use?describe relationship LDT age acquisition?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___()"},{"path":"lab4.html","id":"overlaying-a-line-1","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.4.2 Overlaying a line","text":"Now, let’s add line scatterplot.Exercise 4.5  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis) best-fitting line overlaid top.code use?line seem good fit data? areas line seems -shoot -shoot data?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___() +\n  ___(method = \"lm\")"},{"path":"lab4.html","id":"finding-the-correlation-1","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.4.3 Finding the correlation","text":"Now let’s find correlation (, sure look code used previous section guide):Exercise 4.6  Fill blanks find correlation LDT Age_Of_Acquisition:code use?correlation LDT Age Acquisition stronger weaker correlation LDT Length?ELP dataset, Age Acquisition measured years LDT measured milliseconds. correlation LDT Age Acquisition change Age Acquisition measured months LDT measured seconds? ?","code":"___ %>%\n    summarize(r = cor(___, ___))"},{"path":"lab4.html","id":"finding-the-slope-and-intercept-1","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.4.4 Finding the slope and intercept","text":"Finally, let’s find slope intercept best-fitting regression line Age Acquisition LDT:Exercise 4.7  Fill blanks code find intercept slope line using Age Acquisition explanatory variable LDT response variable:code use?According linear model just found, much longer take recognize word learned age 10, relative word learned age 9?make sense try extend relationship young ages (e.g., 6 months old)? Explain reasoning.expect relationship continue words learned relatively later life, like technical words learn college work? , shape expect relationship words learned later life ?","code":"lm(___ ~ ___, data = ___)"},{"path":"lab4.html","id":"summary","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.3.5 Summary","text":"seen can predict long takes recognize word terms either length (number letters) age learned (Age Acquisition). results tell us “mental dictionary” organized just things like spelling, also life experience.","code":""},{"path":"lab4.html","id":"wrap-up-3","chapter":"Lab 4 Relationships Between Numerical Variables","heading":"4.4 Wrap-up","text":"seen scatterplots, correlation, linear regression valuable tools describing relationships numerical variables. tools help us explain details behavior reveal structure memory. tools always used carefully always visualization, since numerical summaries alone can misleading.","code":""},{"path":"lab5.html","id":"lab5","chapter":"Lab 5 Hypothesis testing with randomization","heading":"Lab 5 Hypothesis testing with randomization","text":"session, get initial practice testing hypotheses randomization. practice cover nuances hypothesis testing statistics, touch many key ideas see different forms throughout rest course.first part, get sense permutation works, helps us simulate sample might look like null hypothesis true. second part, use permutation test serious hypotheses different types people might make decisions differently.","code":""},{"path":"lab5.html","id":"required-packages-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1 Required packages","text":"addition tidyverse package normally use, today also need load infer package. might guess name, infer package provides lot useful tools inference statistics.","code":""},{"path":"lab5.html","id":"first-try-this","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.1 First try this","text":"already tidyverse package installed, since ’ve using since first lab. run code, well:working University computers lab library, infer package already installed. infer package installed, able run following line problems:","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab5.html","id":"if-necessary-install-the-infer-package","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.2 If necessary, install the infer package","text":"got error loading infer package, need install first. Luckily, just involves running following line:Now able load infer package without problems.","code":"\ninstall.packages(\"infer\")"},{"path":"lab5.html","id":"did-kobe-have-a-hot-hand","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2 Did Kobe have a hot hand?","text":"get handle big ideas hypothesis testing general, permutation particular, let’s first look special dataset. data pertain Kobe Bryant LA Lakers playing Orlando Magic 2009 NBA finals. Commentators time remarked Kobe seemed “hot hand.” words, claiming Kobe made basket, likely make basket next shot.","code":""},{"path":"lab5.html","id":"load-the-data-6","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.1 Load the data","text":"Run following line code download dataset consisting every shooting attempt Kobe made game, including whether went (.e., shot “Hit” “Miss”):","code":"\nkobe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/kobe.csv\")## Rows: 111 Columns: 7## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (6): vs, quarter, time, description, shot, prev_shot\n## dbl (1): game## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab5.html","id":"framing-the-hypotheses","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.2 Framing the hypotheses","text":"can translate claim hot hand null hypothesis alternative hypothesis. us believe “hot hand” claim, first rule possibility Kobe’s hit proportion regardless whether previous shot went . possibility null hypothesis. alternative hypothesis Kobe really hot hand made greater proportion hits already made hit missing.","code":""},{"path":"lab5.html","id":"check-out-the-data","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.3 Check out the data","text":"Now ’ve framed hypotheses, let’s see whether data suggest reject null hypothesis. just use simple frequency table like ’ve used :, prev_shot refers whether Kobe’s previous shot hit (“H”) miss (“M”). final column p gives us proportions Hits Misses following either Hit Miss.Exercise 5.1  Let’s call proportion hits previous shot hit \\(\\hat{p}_{\\text{Prev. H}}\\) proportion hits previous shot miss \\(\\hat{p}_{\\text{Prev. M}}\\).Based proportions frequency table, \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\)?Given answer (), data consistent Kobe “hot hand?”","code":"\nkobe %>%\n    group_by(prev_shot, shot) %>%\n    summarize(n=n()) %>%\n    mutate(p = n / sum(n))## `summarise()` has grouped output by 'prev_shot'. You can override using the `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   prev_shot [2]\n##   prev_shot shot      n     p\n##   <chr>     <chr> <int> <dbl>\n## 1 H         H        18 0.36 \n## 2 H         M        32 0.64 \n## 3 M         H        25 0.410\n## 4 M         M        36 0.590"},{"path":"lab5.html","id":"model-the-null-hypothesis","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.4 Model the null hypothesis","text":"null hypothesis true, whether Kobe made shot depend whether previous shot Hit Miss. means , null hypothesis true, able shuffle Kobe’s previous shots still obtain pattern results looks similar just saw. course, null hypothesis false, shuffling Kobe’s previous shots produce pattern results look different actually saw.","code":""},{"path":"lab5.html","id":"a-single-shuffle","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.4.1 A single shuffle","text":"get sense “shuffling” means , let’s first take look Kobe’s shots just second quarter first game. , use new trick, filter function:== asks question, two things equal? game == 1 pulls shots game equals 1, .e., shots first game. Similarly, quarter == 2 pulls shots quarter equals 2, second quarter game. Put together, get filter pulls shots game first quarter second. Neat trick!include line, can get frequency table just handful shots, like :model shots might gone null hypothesis true, need specify relevant variables well hypothesis. Let’s see , unpack :first line, added simulated_data <- R remember result name simulated_data.third line, specify variable explanatory variable (prev_shot) variable response variable (shot). using squiggly thing used linear regression ([response variable] ~ [explanatory variable]). response variable binary (.e., two-level) categorical variable, also need tell R “success” Hit (abbreviated “H”).fourth line tell R null hypothesis , namely, explanatory response variables independent (associated).Finally, fifth line generates simulated dataset randomly permuting—, shuffling—columns original data containing explanatory response variables.Run following line can see simulated shots Kobe look like:Note simulation probably look one shown columns shuffled randomly.Finally, way quickly calculate difference proportions interested (\\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\)). can actual data simulated data. actual data:calculate line magic happens. tell R statistic interested “diff props” (difference proportions), tell R order calculate (“H” minus “M”).Exercise 5.2  Fill blank calculate thing simulated data:result got? larger, smaller, calculated using actual data second quarter first game?","code":"\nkobe %>%\n    filter(game == 1, quarter == 2)## # A tibble: 8 × 7\n##   vs     game quarter time  description                          shot  prev_shot\n##   <chr> <dbl> <chr>   <chr> <chr>                                <chr> <chr>    \n## 1 ORL       1 2       5:58  Kobe Bryant makes 20-foot jumper     H     H        \n## 2 ORL       1 2       5:22  Kobe Bryant makes 14-foot jumper     H     H        \n## 3 ORL       1 2       4:37  Kobe Bryant misses driving layup     M     H        \n## 4 ORL       1 2       3:30  Kobe Bryant makes 9-foot two point … H     M        \n## 5 ORL       1 2       2:55  Kobe Bryant makes 14-foot running j… H     H        \n## 6 ORL       1 2       1:55  Kobe Bryant misses 19-foot jumper    M     H        \n## 7 ORL       1 2       0:38  Kobe Bryant misses 27-foot three po… M     M        \n## 8 ORL       1 2       0:04  Kobe Bryant makes driving layup      H     M\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    group_by(prev_shot, shot) %>%\n    summarize(n=n()) %>%\n    mutate(p = n / sum(n))## `summarise()` has grouped output by 'prev_shot'. You can override using the `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   prev_shot [2]\n##   prev_shot shot      n     p\n##   <chr>     <chr> <int> <dbl>\n## 1 H         H         3 0.6  \n## 2 H         M         2 0.4  \n## 3 M         H         2 0.667\n## 4 M         M         1 0.333\nsimulated_data <- kobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")\nsimulated_data## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## Null Hypothesis: independence\n## # A tibble: 8 × 3\n## # Groups:   replicate [1]\n##   shot  prev_shot replicate\n##   <fct> <fct>         <int>\n## 1 H     H                 1\n## 2 H     H                 1\n## 3 H     H                 1\n## 4 M     M                 1\n## 5 M     H                 1\n## 6 M     H                 1\n## 7 H     M                 1\n## 8 H     M                 1\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##      stat\n##     <dbl>\n## 1 -0.0667___ %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))"},{"path":"lab5.html","id":"kobenull","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.4.2 Many shuffles","text":"’s single shuffle just part data look like, ’s good thing computer bunch times!Let’s get back looking full dataset. next chunk code generate 1000 different shuffled datasets simulate kinds results occur null hypothesis true, simulated dataset calculated difference proportions \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\). Finally, tell R remember using name null_dist “null distribution.”Run following chunk code:Now can check result:“replicate” column just number labels simulated dataset “stat” column difference \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\) simulated dataset.can get better summary visualizing distribution results using histogram:","code":"\nnull_dist <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1000, type = \"permute\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))\nnull_dist## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## Null Hypothesis: independence\n## # A tibble: 1,000 × 2\n##    replicate    stat\n##        <int>   <dbl>\n##  1         1 -0.159 \n##  2         2 -0.0862\n##  3         3 -0.0134\n##  4         4 -0.159 \n##  5         5  0.0230\n##  6         6  0.0230\n##  7         7 -0.0134\n##  8         8 -0.0862\n##  9         9  0.0230\n## 10        10 -0.0862\n## # … with 990 more rows\nnull_dist %>%\n    visualize()"},{"path":"lab5.html","id":"find-the-p-value","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.5 Find the \\(p\\) value","text":"Remember \\(p\\) value proportion simulated datasets least extreme actual data. case, means proportion simulated datasets difference proportions bigger observed. Let’s tell R remember difference running chunk code :Now, let’s see many simulated datasets bigger . following line code, tell R observed “statistic” (obs_stat) difference just told remember (obs_diff) well fact interested many simulations produced results “greater” observed:Finally, help visualize observed difference falls relative distribution differences simulated data (note uses shade_p_value rather get_p_value):red line value actually observed parts histogram shaded pink represent simulated datasets “extreme” observed.","code":"\nobs_diff <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))\nnull_dist %>%\n    get_p_value(obs_stat = obs_diff, direction = \"greater\")## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.756\nnull_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = \"greater\")"},{"path":"lab5.html","id":"form-a-conclusion","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.6 Form a conclusion","text":"Remember testing null hypothesis Kobe hot hand. reject null hypothesis data unlikely null hypothesis true, .e., \\(p\\) value low. Assume adopt significance level 0.05, reject null hypothesis \\(p\\) value less level.Exercise 5.3  reject null hypothesis? Explain reasoning. conclusion say whether Kobe “hot hand?”","code":""},{"path":"lab5.html","id":"do-people-on-the-autism-spectrum-make-more-consistent-choices","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3 Do people on the autism spectrum make more consistent choices?","text":"kind hypothesis testing just fun case Kobe’s hot hand use answer serious research questions.Autism condition many facets. Individuals autism cognitive impairments often different cognitive “styles.” particular, thought people autism “detail-oriented.” can detriment trying find general pattern, might benefit situations many irrelevant distractions.potential benefit studied Farmer et al. (2017). experiment included group participants diagnosed autism spectrum (cognitive impairments) well group neuro-typical controls. looked participants’ choices pairs consumer products presented alongside third, less desirable “decoy” option. “rational” choice affected presence decoy, fact people often swayed irrelevant options. Might people autism make choices consistent—“rational”—ignore irrelevant decoy?rest lab, conduct hypothesis test address question. follow basic outline procedure followed address “hot hand” question, sure refer previous section guidance.","code":""},{"path":"lab5.html","id":"load-the-data-7","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.1 Load the data","text":"First, let’s load relevant data collected Farmer et al. (2017) following line code:response variable Choice, either “Consistent” (participant’s choice affected decoy) “Inconsistent” (participant’s choice affected decoy).explanatory variable Group, either “ASC” (“Autism Spectrum Condition”) “NT” (“Neuro-Typical” control).","code":"\nasc_choice <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/asc_choice_job.csv\")## Rows: 302 Columns: 9## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (6): Pcp, Group, Product, Gender, Residence, Choice\n## dbl (3): ICAR, AQScore, Age## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab5.html","id":"framing-the-hypotheses-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.2 Framing the hypotheses","text":"research question , “proportion consistent choices higher participants ASC group NT group?”Exercise 5.4  null hypothesis alternative hypothesis correspond research question?","code":""},{"path":"lab5.html","id":"check-out-the-data-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.3 Check out the data","text":"next step construct frequency table can know proportions consistent choices group find difference.Exercise 5.5  Fill blanks find proportions consistent choices group.proportion consistent choices ASC group?proportion consistent choices NT group?difference proportions?","code":"___ %>%\n    group_by(___, ___) %>%\n    summarize(n = n()) %>%\n    mutate(p = n / sum(n))"},{"path":"lab5.html","id":"model-the-null-hypothesis-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.4 Model the null hypothesis","text":"null hypothesis true, participant’s choice shouldn’t depend group . shuffling participants randomly groups, can therefore simulate data look null hypothesis true.Exercise 5.6  Fill blanks code simulate 1000 datasets assuming null hypothesis true find resulting differences proportions.guidance, check Kobe example (especially section). sure note:name relevant dataset?names explanatory response variables?“Consistent” choice counts “success.”want 1000 simulations.difference proportions, want look ASC minus NT.final chunk code used?","code":"null_dist <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))"},{"path":"lab5.html","id":"find-the-p-value-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.5 Find the \\(p\\) value","text":"find \\(p\\) value, first need get observed difference proportions tell R remember label obs_diff.Exercise 5.7  Fill blanks find observed difference proportions actual data tell R remember label obs_diff. Hint: see wrote previous exercise!code use?Finally, position find \\(p\\) value visualize observed difference proportion falls relative differences expected null hypothesis true. goes well, results close following, though randomness involved, results strictly identical!","code":"obs_diff <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\nnull_dist %>%\n    get_p_value(obs_stat = obs_diff, direction = \"greater\")## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.012\nnull_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = \"greater\")"},{"path":"lab5.html","id":"form-a-conclusion-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3.6 Form a conclusion","text":"Assume adopt significance level 0.05, reject null hypothesis \\(p\\) value less level.Exercise 5.8  reject null hypothesis? Explain reasoning. conclusion say whether participants diagnosis autism made consistent choices neuro-typical participants?","code":""},{"path":"lab5.html","id":"wrap-up-4","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.4 Wrap-up","text":"session, got practice using R perform hypothesis tests using randomization. Specifically, used type shuffling called permutation. Shuffling allows us simulate various ways particular dataset look like null hypothesis true. found \\(p\\) value visualized null distribution order get sense whether actual data unlikely null hypothesis true.","code":""},{"path":"lab6.html","id":"lab6","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"Lab 6 Confidence intervals with bootstrapping","text":"session, get practice using R find confidence intervals via bootstrapping. initial exposure hypothesis testing previous session, meant “first contact” basics techniques. first part, revisit Kobe data get view resampling works, since heart bootstrapping technique. second part, use bootstrapping create confidence intervals representing people’s tendencies making moral decisions.","code":""},{"path":"lab6.html","id":"required-packages-2","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1 Required packages","text":"Like last time, require tidyverse infer packages session, make sure load R’s library line .","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab6.html","id":"what-is-kobes-field-goal-percentage","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2 What is Kobe’s field goal percentage?","text":"Last time, asked whether Kobe Bryant “hot hand” 2009 NBA finals. used randomization test null hypothesis Kobe’s chance making hit regardless whether previous shot hit .time just interested proportion Kobe’s shots actually make basket. traditional basketball term Kobe’s “field goal percentage.” constitutes population parameter label mathematically \\(\\pi_{\\text{Kobe}}\\). “population” shots Kobe ever attempted across career. sample shots Kobe attempted 2009 NBA finals. sample yields point estimate \\(\\hat{p}_{\\text{Kobe}}\\) population parameter \\(\\pi_{\\text{Kobe}}\\).","code":""},{"path":"lab6.html","id":"load-the-data-8","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.1 Load the data","text":"Run following line code download dataset consisting every shooting attempt Kobe made 2009 NBA finals. data looked last time:","code":"\nkobe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/kobe.csv\")## Rows: 111 Columns: 7## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (6): vs, quarter, time, description, shot, prev_shot\n## dbl (1): game## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab6.html","id":"what-is-the-point-estimate","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.2 What is the point estimate?","text":"interested time just proportion Kobe’s shots hits rather misses. today, don’t care outcome previous shot .get point estimate proportion Kobe’s shots hits rather misses, can use following chunk code:Exercise 6.1  chunk code used find observed difference hit proportions last time, function whether Kobe’s previous shot hit :Compare code last time code just used find proportion Kobe’s shots hits. Note similarities differences try describe differences might . Hint: consider last time explanatory response variable, now interested single (response) variable.","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.387\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))"},{"path":"lab6.html","id":"model-the-randomness","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3 Model the randomness","text":"Although sample gives us point estimate (\\(\\hat{p}_{\\text{Kobe}}\\)) Kobe’s true field goal percentage (\\(\\pi_{\\text{Kobe}}\\)), know point estimate unlikely perfect estimate. know sample population subject sampling variability can treated effectively “random.” confidence interval based modeling sampling variability produced actual sample got, can know range values population parameter remain plausible.Bootstrapping models randomness using sample estimate population repeatedly resampling estimated population. Let’s see works.","code":""},{"path":"lab6.html","id":"a-single-resample","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3.1 A single resample","text":"get sense resampling works, let’s focus smaller number shots 2nd quarter 1st game. Like last time, filter function let’s us pull just shots:following chunk code resamples Kobe’s shots quarter produce new sample shots:Exercise 6.2  chunk code used last time generate single “shuffle” shots:Compare code last time code just used resampling. Note similarities differences. Hint: matter bootstrapping “null hypothesis?”final step bootstrapping calculate sample statistic—case, proportion hits—randomly generated re-sample. can adding calculate line end code used make resample:","code":"\nkobe %>%\n    filter(game == 1, quarter == 2)## # A tibble: 8 × 7\n##   vs     game quarter time  description                          shot  prev_shot\n##   <chr> <dbl> <chr>   <chr> <chr>                                <chr> <chr>    \n## 1 ORL       1 2       5:58  Kobe Bryant makes 20-foot jumper     H     H        \n## 2 ORL       1 2       5:22  Kobe Bryant makes 14-foot jumper     H     H        \n## 3 ORL       1 2       4:37  Kobe Bryant misses driving layup     M     H        \n## 4 ORL       1 2       3:30  Kobe Bryant makes 9-foot two point … H     M        \n## 5 ORL       1 2       2:55  Kobe Bryant makes 14-foot running j… H     H        \n## 6 ORL       1 2       1:55  Kobe Bryant misses 19-foot jumper    M     H        \n## 7 ORL       1 2       0:38  Kobe Bryant misses 27-foot three po… M     M        \n## 8 ORL       1 2       0:04  Kobe Bryant makes driving layup      H     M\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\")## Response: shot (factor)\n## # A tibble: 8 × 2\n## # Groups:   replicate [1]\n##   replicate shot \n##       <int> <fct>\n## 1         1 H    \n## 2         1 H    \n## 3         1 H    \n## 4         1 H    \n## 5         1 H    \n## 6         1 M    \n## 7         1 H    \n## 8         1 H\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1  0.75"},{"path":"lab6.html","id":"many-resamples","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3.2 Many resamples","text":"can remove filter line generate single bootstrap resample proportion based full data.course, whole point bootstrapping get range plausible values need one resample! like last time, want get R remember proportions resample can use later; call boot_dist. following chunk code uses bootstrapping produce distribution sample proportions, variability mimics sampling variability work generating original observed sample:like last time, can use histogram get nice visual summary bootstrap distribution:","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.306\nboot_dist <- kobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")\nboot_dist %>%\n    visualize()"},{"path":"lab6.html","id":"create-the-interval","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.4 Create the interval","text":"find confidence interval, need find “middle” bootstrap distribution. also need decide wide “middle” . example, create 95% confidence interval, need find middle 95% distribution .middle 95% two different quantiles bootstrap distribution. Specifically, 2.5% 97.5% quantiles define boundaries 95% confidence interval. values 2.5% simulated proportions interval 2.5% simulated proportions interval. , total, 5% values outside interval meaning remainder (95%) inside .can find relevant quantiles using faithful summarize function, now applied bootstrap distribution:list numbers probs gives probabilities , terms proportions rather percent (0.025 instead 2.5%, example).Exercise 6.3  Modify following code get boundaries 90% confidence interval:code use?boundaries found?many things, R shortcuts! following code also finds 95% confidence interval, use level set wide want interval. Note putting result label boot_ci can use help us visualize interval:Now can add 95% confidence interval saved boot_ci visualization:","code":"\nboot_dist %>%\n    summarize(CI = quantile(stat, probs = c(0.025, 0.975)))## # A tibble: 2 × 1\n##      CI\n##   <dbl>\n## 1 0.297\n## 2 0.478boot_dist %>%\n    summarize(CI = quantile(stat, probs = c(___, ___)))\nboot_ci <- boot_dist %>%\n    get_confidence_interval(level = 0.95)\nboot_dist %>%\n    visualize() +\n    shade_confidence_interval(endpoints = boot_ci)"},{"path":"lab6.html","id":"form-a-conclusion-2","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.5 Form a conclusion","text":"Now position form conclusion Kobe’s true field goal percentage based sample .Exercise 6.4  Answer following based 95% confidence interval just constructed Kobe’s field goal percentage.plausible Kobe makes half shots attempts? Explain reasoning.Kobe’s actual career field goal percentage 44.7%. words, true value population parameter \\(\\pi_{\\text{Kobe}} = 0.447\\). value contained within 95% confidence interval?","code":""},{"path":"lab6.html","id":"how-willing-are-people-to-sacrifice-one-life-to-save-many","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3 How willing are people to sacrifice one life to save many?","text":"well-known Vulcan dictum needs many outweigh needs (one). may true Vulcans necessarily morally acceptable humans. Moreover, even human endorses basic logic better save lives fewer lives may willing put logic practice. Fortunately, rarely put situations make choice.Philosophers psychologists studied moral decision making many ways. One way asking people hypothetical scenario. common scenario called “Trolley problem”:man blue standing railroad tracks notices empty trolley car rolling control. moving fast anyone hits die. Ahead main track five people. one person standing side track doesn’t rejoin main track. man blue nothing, trolley hit five people main track, one person side track. man blue flips switch next , divert trolley side track hit one person, hit five people main track.man blue ? sacrifice one person side track save five people main track? nothing, leaving five people main track fate? Awad et al. (2020) presented scenarios like large group people world (roughly 70,000), aim studying different cultural backgrounds might influence people’s moral judgments.presented different versions Trolley problem. One like description , man blue flip switch. different version went like :man blue standing footbridge railroad tracks notices empty trolley car rolling control. moving fast anyone hits die. Ahead track five people. large person standing near man blue footbridge, large person weighs enough trolley slow hit (man blue weigh enough slow trolley). man blue nothing, trolley hit five people track. man blue pushes one person, one person fall onto track, trolley hit one person, slow one person, hit five people farther track.second version, called “footbridge” version, man blue take much active role, literally throwing another person tracks save five. sense, though, amounts question: man blue sacrifice one person save five people?pictures, provided Awad et al. (2020), depict scenarios corresponding two different versions Trolley problem:section, look judgments Awad et al. (2020) obtained people see whether majority willing sacrifice one life save five scenarios.","code":""},{"path":"lab6.html","id":"load-the-data-9","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.1 Load the data","text":"Let’s load relevant data collected Awad et al. (2020) following line code:purposes, just data people living USA, still quite large sample! Now trolley data appeared RStudio’s environment pane (upper right), click explore structure data.","code":"\ntrolley <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/trolley.csv\")## Rows: 32163 Columns: 12## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (9): _id, Scenario, Sacrifice, UserID, Session_id, Template, lang, count...\n## dbl (1): Scenario_order\n## lgl (2): answerLeft, seenOther## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab6.html","id":"what-is-the-point-estimate-1","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.2 What is the point estimate?","text":"First, let’s get point estimate proportion people sacrifice someone. separately version Trolley problem, making use filter function like Kobe’s data .response variable Sacrifice either “Yes” “.” purposes, call “Yes” “success” (though course arguable whether either option Trolley problem considered “success”).Exercise 6.5  code provides starting point find point estimates proportion people scenario sacrifice someone save five others.proportion people sample “Switch” scenario chose sacrifice someone save five others?proportion people sample “Footbridge” scenario chose sacrifice someone save five others?","code":"trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    calculate(stat = \"___\")"},{"path":"lab6.html","id":"model-the-randomness-1","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.3 Model the randomness","text":"Now need model randomness involved sampling variability get bootstrap distributions proportions people recommend sacrificing scenario. Generate 1000 bootstrap resamples scenario.Exercise 6.6  code provides starting point use bootstrapping obtain distribution sample proportions gotten due sampling variability. Notice save two bootstrap distributions two different names, boot_dist_switch boot_dist_footbridge. goes well, appear RStudio environment pane, otherwise won’t see anything console.code use generate two bootstrap distributions?","code":"boot_dist_switch <- trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nboot_dist_footbridge <- trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")"},{"path":"lab6.html","id":"create-the-intervals","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.4 Create the intervals","text":"Now ’ve created bootstrap distributions, use following code starting point generate 95% confidence intervals . Hint: remember names gave bootstrap distributions previous exercise; also recall level number 0 1, rather percentage.Finally, can use following code outline visualize either two distributions filling blanks appropriate boot_dist_ boot_ci_.Exercise 6.7  –95% confidence interval proportion choose sacrifice “Switch” scenario?95% confidence interval proportion choose sacrifice “Footbridge” scenario?","code":"boot_ci_switch <- ___ %>%\n    get_confidence_interval(level = ___)\n\nboot_ci_footbridge <- ___ %>%\n    get_confidence_interval(level = ___)___ %>%\n    visualize() +\n    shade_confidence_interval(endpoints = ___)"},{"path":"lab6.html","id":"form-a-conclusion-3","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.5 Form a conclusion","text":"Now time interpret confidence intervals just found context particular research scenario.Exercise 6.8  confidence intervals just found tell us proportion people US population recommend sacrificing one life save five? proportion seem depend specifics scenario (.e., whether “Switch” “Footbridge” version)? version, say half population recommend sacrificing one person save five?","code":""},{"path":"lab6.html","id":"wrap-up-5","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.4 Wrap-up","text":"session, got practice using R construct confidence intervals using bootstrapping. used R generate many “resampled” dataset simulate kinds samples seen due sampling variability. allows us say values population parameter plausible given sample population.","code":""},{"path":"lab7.html","id":"lab7","chapter":"Lab 7 The normal distribution","heading":"Lab 7 The normal distribution","text":"venerable normal distribution “bell curve” almost like mascot statistics. Although variables real life distributed according normal distribution, real value describing sampling distributions. seen sampling distributions hypothesis testing confidence intervals: Sampling distributions represent variability point estimate like proportion mean due randomness involved selecting samples population. According central limit theorem, much time, sampling distributions approximately normal shape.session, first use normal distribution model population distributions, distribution values particular variable across whole population. Along way, get hang using R find proportions intervals based normal distribution. , second part, use normal distribution model sampling distributions. give us insight standard error relates things like sample size.","code":""},{"path":"lab7.html","id":"required-packages-tidyverse-and-infer","chapter":"Lab 7 The normal distribution","heading":"7.1 Required packages: tidyverse and infer","text":"require tidyverse infer packages session, make sure load R’s library:","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.3     ✓ dplyr   1.0.5\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   2.0.0     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab7.html","id":"the-data-national-health-and-nutrition-examination-surveys-nhanes","chapter":"Lab 7 The normal distribution","heading":"7.2 The data: National Health and Nutrition Examination Surveys (NHANES)","text":"data using lab originally collected US National Center Heath Statistics 2009 2012. subset effectively simple random sample entire US population, though use observations many collected. Load data following line:","code":"\nnhanes <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/nhanes.csv\")## Rows: 4924 Columns: 76## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (31): SurveyYr, Gender, AgeDecade, Race1, Race3, Education, MaritalStatu...\n## dbl (41): ID, Age, AgeMonths, HHIncomeMid, Poverty, HomeRooms, Weight, Heigh...\n## lgl  (4): Length, HeadCirc, TVHrsDayChild, CompHrsDayChild## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-population-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3 The normal distribution as a model for a population distribution","text":"","code":""},{"path":"lab7.html","id":"hours-of-sleep-sleephrsnight","chapter":"Lab 7 The normal distribution","heading":"7.3.1 Hours of Sleep (SleepHrsNight)","text":"variable focus section number hours people report sleeping night, measured hours. recorded individual sample variable SleepHrsNight.","code":""},{"path":"lab7.html","id":"visualize-the-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.1 Visualize the distribution","text":"usual, begin visualizing actual data. reminder, assume time data constitute entire population interest. histogram shows population distribution nightly hours sleep:","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(binwidth = 1)"},{"path":"lab7.html","id":"is-the-normal-distribution-a-good-model","chapter":"Lab 7 The normal distribution","heading":"7.3.1.2 Is the normal distribution a good model?","text":"Next, want know whether normal distribution good model population distribution just visualized. , draw curve representing normal distribution top histogram just made.remember normal distribution just shape. center spread determined two population parameters: mean (\\(\\mu\\)) standard deviation (\\(\\sigma\\)). parameters ?Let’s first take wild guess say \\(\\mu = 8\\) \\(\\sigma = 1\\). can draw distribution using chunk code :Notice needed add aes(y = ..density..) histogram line, reasons clear next paragraph. final line lets us draw function graph. function called dnorm “density normal distribution.” say arguments function , list say mean sd normal distribution .whole “density” thing comes : histogram shows absolute counts. normal distribution specifies relative frequency different values. relative frequency called “density.” dnorm function gives us normal distribution. adding aes(y = ..density..) histogram line told R show density (relative frequency) rather absolute counts.Anyway, clear mean 8 hours standard deviation 1 hour make good fit. use instead actual mean SD data:Let’s give numbers label can use later:Exercise 7.1  Fill blanks code visualize fit normal distribution using values mean SD just found.normal distribution good job approximating shape population distribution?","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = 8, sd = 1), color = 'darkred')\nnhanes %>%\n    summarize(mean(SleepHrsNight), sd(SleepHrsNight))## # A tibble: 1 × 2\n##   `mean(SleepHrsNight)` `sd(SleepHrsNight)`\n##                   <dbl>               <dbl>\n## 1                  6.86                1.32\nmean_sleep <- nhanes %>%\n    summarize(mean(SleepHrsNight)) %>%\n    pull()\n\nsd_sleep <- nhanes %>%\n    summarize(sd(SleepHrsNight)) %>%\n    pull()nhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"proportions-from-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.3 Proportions from a normal distribution","text":"Using normal distribution, possible estimate proportion values number. example, following uses normal distribution estimate proportion people say get less eight hours sleep per night:Conversely, proportion get eight hours sleep per night:Notice difference whether lower.tail TRUE FALSE. lower.tail = TRUE, asking proportion value q, lower.tail = FALSE asking proportion value q.Exercise 7.2  Use pnorm function find following proportions:proportion people get less 6 hours sleep per night.proportion people get 10 hours sleep per night.proportion people get either less 6 hours sleep per night 10 hours sleep per night (Hint: can add proportions things overlap.)","code":"\npnorm(q = 8, mean = mean_sleep, sd = sd_sleep, lower.tail = TRUE)## [1] 0.8058069\npnorm(q = 8, mean = mean_sleep, sd = sd_sleep, lower.tail = FALSE)## [1] 0.1941931"},{"path":"lab7.html","id":"quantiles-from-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.4 Quantiles from a normal distribution","text":"saw last time, things like confidence intervals defined terms “quantiles.” Finding quantile opposite finding proportion. R, opposite relationship clear just swap p q. qnorm function find value q proportion p values either q.example, want know “25th percentile,” quantile q p = 0.25 less value:’s neat trick, though: 0.25 less value, means 0.75 value. flip lower.tail FALSE p 0.75, get answer:Exercise 7.3  Use qnorm function find following quantiles:quantile 0.025 data value?quantile 0.025 data value?two values middle 95% data fall? (Hint: much data fall middle 95%?)","code":"\nqnorm(p = 0.25, mean = mean_sleep, sd = sd_sleep, lower.tail = TRUE)## [1] 5.964337\nqnorm(p = 0.75, mean = mean_sleep, sd = sd_sleep, lower.tail = FALSE)## [1] 5.964337"},{"path":"lab7.html","id":"bad-days-daysmenthlthbad","chapter":"Lab 7 The normal distribution","heading":"7.3.2 Bad Days (DaysMentHlthBad)","text":"part survey, people asked many days last 30 consider mental health poor. recorded variable named DaysMentHlthBad.Just like number hours slept per night, find mean standard deviation DaysMentHlthBad see whether normal distribution seems provide good approximation population distribution.Exercise 7.4  Fill blanks chunk code find mean standard deviation DaysMentHlthBad plot data along approximating normal distribution.Describe shape distribution observed data shown histogram.normal distribution provide good fit data?Make reasoned guess population distribution might shape , taking note fact respondents give number days 30.","code":"mean_mental_health <- nhanes %>%\n    summarize(mean(___)) %>%\n    pull()\n\nsd_mental_health <- nhanes %>%\n    summarize(sd(___)) %>%\n    pull()\n\nnhanes %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-sampling-distribution","chapter":"Lab 7 The normal distribution","heading":"7.4 The normal distribution as a model for a sampling distribution","text":"previous section, got sense can use normal distribution approximate distribution observed values population:Find mean standard deviation.Check see well normal distribution mean standard deviation fits histogram observed data.now follow two steps, instead using normal distribution model population distribution, use model sampling distribution, , distribution summary statistics get across many samples population.section, assume nhanes dataset represents entire population interest. simulate drawing many different samples different sizes population calculate summary statistic sample. sampling distribution distribution summary statistics see well can approximate normal distribution.focus just one questions asked NHANES survey. question asks whether someone ever tried using marijuana. information resides variable Marijuana person’s response either “Yes” “.”Exercise 7.5  Without even looking data, make sense try use normal distribution approximate population distribution responses Marijuana question? ?","code":""},{"path":"lab7.html","id":"the-true-proportion","chapter":"Lab 7 The normal distribution","heading":"7.4.1 The “true” proportion","text":"assuming nhanes data represent entire population, can directly find “true” value population parameter \\(\\pi_{\\text{Marijuana}}\\), , proportion people ever tried marijuana. can use code :Actually, since interested proportion said “Yes,” can find proportion using compact code:“trick” previous chunk code : Marijuana == \"Yes\" checks see, person, whether response Marijuana question “Yes.” , gives person “score” 1, , gives person “score” 0. Recall mean sum set numbers divided total number numbers. find mean scores using mean(Marijuana == \"Yes\"), really adding number people “scored” 1 dividing total number people.","code":"\nnhanes %>%\n    group_by(Marijuana) %>%\n    summarize(n = n()) %>%\n    mutate(pi = n / sum(n))## # A tibble: 2 × 3\n##   Marijuana     n    pi\n##   <chr>     <int> <dbl>\n## 1 No         2045 0.415\n## 2 Yes        2879 0.585\nnhanes %>%\n    summarize(pi_marijuana = mean(Marijuana == \"Yes\"))## # A tibble: 1 × 1\n##   pi_marijuana\n##          <dbl>\n## 1        0.585"},{"path":"lab7.html","id":"simulating-many-samples","chapter":"Lab 7 The normal distribution","heading":"7.4.2 Simulating many samples","text":"Last time, used bootstrapping simulate happen collected many samples population using sample actually estimate whole population. Now, access population directly. can therefore draw many samples given size want!example, following chunk code randomly samples 5 people population gets R remember sample name sample_size5.Notice still people’s responses question NHANES survey. means can get summary statistic Marijuana variable sample, just like population.Exercise 7.6  Based code used find “true” proportion population, get point estimate based sample 5 people just drew (sample_size5).proportion people tried marijuana sample?expect get proportion another random sample population? ?’ve seen, great thing computers can boring repetitive things us quickly. Now, draw 1000 samples size 5 population get R remember name samples_size5.","code":"\nsample_size5 <- nhanes %>%\n    sample_n(size = 5)\n\nsample_size5## # A tibble: 5 × 76\n##      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1   Race3 Education   \n##   <dbl> <chr>    <chr>  <dbl> <chr>         <dbl> <chr>   <chr> <chr>       \n## 1 65978 2011_12  male      57 50-59            NA White   White Some College\n## 2 53457 2009_10  female    30 30-39           364 Mexican <NA>  8th Grade   \n## 3 53965 2009_10  male      31 30-39           375 Mexican <NA>  Some College\n## 4 54006 2009_10  male      20 20-29           240 Mexican <NA>  Some College\n## 5 63200 2011_12  female    59 50-59            NA White   White College Grad\n## # … with 67 more variables: MaritalStatus <chr>, HHIncome <chr>,\n## #   HHIncomeMid <dbl>, Poverty <dbl>, HomeRooms <dbl>, HomeOwn <chr>,\n## #   Work <chr>, Weight <dbl>, Length <lgl>, HeadCirc <lgl>, Height <dbl>,\n## #   BMI <dbl>, BMICatUnder20yrs <chr>, BMI_WHO <chr>, Pulse <dbl>,\n## #   BPSysAve <dbl>, BPDiaAve <dbl>, BPSys1 <dbl>, BPDia1 <dbl>, BPSys2 <dbl>,\n## #   BPDia2 <dbl>, BPSys3 <dbl>, BPDia3 <dbl>, Testosterone <dbl>,\n## #   DirectChol <dbl>, TotChol <dbl>, UrineVol1 <dbl>, UrineFlow1 <dbl>, …___ %>%\n    summarize(p_hat = ___)\nsamples_size5 <- nhanes %>%\n    rep_sample_n(size = 5, reps = 1000)\n\nsamples_size5## # A tibble: 5,000 × 77\n## # Groups:   replicate [1,000]\n##    replicate    ID SurveyYr Gender   Age AgeDecade AgeMonths Race1   Race3\n##        <int> <dbl> <chr>    <chr>  <dbl> <chr>         <dbl> <chr>   <chr>\n##  1         1 54148 2009_10  male      34 30-39           418 Mexican <NA> \n##  2         1 60229 2009_10  male      50 50-59           605 Mexican <NA> \n##  3         1 68152 2011_12  male      56 50-59            NA Black   Black\n##  4         1 57109 2009_10  female    41 40-49           500 Other   <NA> \n##  5         1 71164 2011_12  male      48 40-49            NA White   White\n##  6         2 57175 2009_10  male      49 40-49           590 White   <NA> \n##  7         2 69456 2011_12  male      52 50-59            NA Black   Black\n##  8         2 66289 2011_12  male      41 40-49            NA White   White\n##  9         2 59541 2009_10  male      49 40-49           593 White   <NA> \n## 10         2 67261 2011_12  male      57 50-59            NA White   White\n## # … with 4,990 more rows, and 68 more variables: Education <chr>,\n## #   MaritalStatus <chr>, HHIncome <chr>, HHIncomeMid <dbl>, Poverty <dbl>,\n## #   HomeRooms <dbl>, HomeOwn <chr>, Work <chr>, Weight <dbl>, Length <lgl>,\n## #   HeadCirc <lgl>, Height <dbl>, BMI <dbl>, BMICatUnder20yrs <chr>,\n## #   BMI_WHO <chr>, Pulse <dbl>, BPSysAve <dbl>, BPDiaAve <dbl>, BPSys1 <dbl>,\n## #   BPDia1 <dbl>, BPSys2 <dbl>, BPDia2 <dbl>, BPSys3 <dbl>, BPDia3 <dbl>,\n## #   Testosterone <dbl>, DirectChol <dbl>, TotChol <dbl>, UrineVol1 <dbl>, …"},{"path":"lab7.html","id":"summary-statistics-for-each-sample","chapter":"Lab 7 The normal distribution","heading":"7.4.3 Summary statistics for each sample","text":"Notice different samples labeled using variable replicate. can use get proportion marijuana triers sample using old group_by routine. tell R remember sample proportions name sample_props_mari_size5:Notice proportions calculated sample tend vary. can visualize using histogram:","code":"\nsample_props_mari_size5 <- samples_size5 %>%\n    group_by(replicate) %>%\n    summarize(p_hat = mean(Marijuana == \"Yes\"))\n\nsample_props_mari_size5## # A tibble: 1,000 × 2\n##    replicate p_hat\n##        <int> <dbl>\n##  1         1   0  \n##  2         2   0.8\n##  3         3   0.6\n##  4         4   0.8\n##  5         5   0.4\n##  6         6   0.8\n##  7         7   0.4\n##  8         8   0.8\n##  9         9   1  \n## 10        10   0.6\n## # … with 990 more rows\nsample_props_mari_size5 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2)"},{"path":"lab7.html","id":"approximating-with-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.4.4 Approximating with a normal distribution","text":"Finally, see whether can approximate sampling distribution normal distribution, need find mean standard deviation sampling distribution. Recall, standard deviation sampling distribution special name: standard error (SE).tell R remember numbers names mean_props_mari_size5 se_props_mari_size5:Now draw normal curve top histogram made earlier:bad!Exercise 7.7  Use qnorm function help find 95% confidence interval proportion marijuana triers based samples size 5.may help start “skeleton” , put lower upper boundaries confidence interval labels ci_lower ci_upper respectively.95% confidence interval, according normal distribution?anything strange interval, given meant describe plausible values proportion? suggest whether normal distribution good approximation particular sampling distribution?","code":"\nmean_props_mari_size5 <- sample_props_mari_size5 %>%\n    summarize(mean(p_hat)) %>%\n    pull()\n\nse_props_mari_size5 <- sample_props_mari_size5 %>%\n    summarize(sd(p_hat)) %>%\n    pull()\nsample_props_mari_size5 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2) +\n    stat_function(fun = dnorm, args = list(mean = mean_props_mari_size5, sd = se_props_mari_size5), color = 'darkred')ci_lower <- qnorm(p = ___, mean = ___, sd = ___, lower.tail = ___)\nci_upper <- qnorm(p = ___, mean = ___, sd = ___, lower.tail = ___)"},{"path":"lab7.html","id":"a-larger-sample","chapter":"Lab 7 The normal distribution","heading":"7.4.5 A larger sample","text":"following chunk code re-everything just , now sample size 80 instead 5:Exercise 7.8  –normal distribution provide good “fit” sampling distribution samples size 80?95% confidence interval based sample size 80, according normal distribution? (Hint: see previous exercise.)Compare standard error samples size 5 (se_props_mari_size5) samples size 80 (se_props_mari_size80). standard error change sample size?Compare mean sampling distributions samples size 5 (mean_props_mari_size5) samples size 80 (mean_props_mari_size80). mean change much sample size? close means “true” proportion population found earlier?Based relationships mean sampling distribution, standard error, sample size, conjecture reason might call standard deviation sampling distribution “standard error.” (Hint: consider typically trying use sample statistic imperfect estimate population parameter.)","code":"\nsamples_size80 <- nhanes %>%\n    rep_sample_n(size = 80, reps = 1000)\n\nsample_props_mari_size80 <- samples_size80 %>%\n    group_by(replicate) %>%\n    summarize(p_hat = mean(Marijuana == \"Yes\"))\n\nmean_props_mari_size80 <- sample_props_mari_size80 %>%\n    summarize(mean(p_hat)) %>%\n    pull()\n\nse_props_mari_size80 <- sample_props_mari_size80 %>%\n    summarize(sd(p_hat)) %>%\n    pull()\n\nsample_props_mari_size80 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.0125) +\n    stat_function(fun = dnorm, args = list(mean = mean_props_mari_size80, sd = se_props_mari_size80), color = 'darkred')"},{"path":"lab7.html","id":"wrap-up-6","chapter":"Lab 7 The normal distribution","heading":"7.5 Wrap-up","text":"session, saw can use normal distribution model either population distribution sampling distribution. Sometimes normal distribution fits well, sometimes , important us check whether provides reasonable approximation . saw use normal distribution find intervals proportions. using normal distribution model sampling distribution, standard deviation (spread) called “standard error” close relationship sample size.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
