[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"collection laboratory activities part APSY210.","code":""},{"path":"lab1.html","id":"lab1","chapter":"Lab 1 Exploring data with R","heading":"Lab 1 Exploring data with R","text":"session, learn bit data explore using R/RStudio. point learn bit data variables get feel power tools learning use rest semester.follow along activity, asked run bits code RStudio. code can copied--pasted “Console” lower left RStudio. also asked modify code run , can typing code console.document shows snippet code, also shows typical R output result running code. example, following snippet code adds two numbers; way R prints result shown snippet code.Another point today’s activity illustrate even though statistics dealing data, data meaningful. just numbers names, peek world. offer glimpses someone’s life, workings natural process, social structure, etc. dataset limited wide glimpse gives us, point statistics learn make decisions based glimpse.","code":"\n2 + 3## [1] 5"},{"path":"lab1.html","id":"r-and-rstudio","chapter":"Lab 1 Exploring data with R","heading":"1.1 R and RStudio","text":"labs make use RStudio, graphical interface statistical computing language R. R language represents current state art statistical computing academic industrial research. likely remain relevant many years come free open-source, meaning widely accessible improvements extensions made continuously large community professionals hobbyists. fact, many best features R using extensions made people outside “core” development team R. extensions called “packages”, represent bundles code useful statistics.RStudio interface makes easier work R language, also free can installed computers running modern operating system (Windows, Mac, Linux, etc.). R RStudio already installed computers classroom, well Technology-Enhanced Classrooms Library Public Computing Sites campus. working computer, easier time install RStudio . Installing RStudio requires installing R first.","code":""},{"path":"lab1.html","id":"installation-on-your-own-computer","chapter":"Lab 1 Exploring data with R","heading":"1.1.1 Installation on your own computer","text":"can install R computer following instructions : https://cran.rstudio.com/installing R, can install RStudio Desktop following instructions : https://rstudio.com/products/rstudio/download/","code":""},{"path":"lab1.html","id":"running-r-in-a-browser","chapter":"Lab 1 Exploring data with R","heading":"1.1.2 Running R in a browser","text":"Even don’t access computer RStudio installed locally, can use access internet. can run RStudio online : https://rstudio.cloud/. downside cap amount time can spend using online version, better using local installation whenever possible. may need create account use online version, free .","code":""},{"path":"lab1.html","id":"required-packages","chapter":"Lab 1 Exploring data with R","heading":"1.2 Required packages","text":"RStudio running, run line code grey box copying pasting “>” “Console” lower left pane RStudio window.working computer campus, see messages similar ones shown . messages tell us R loaded helpful “packages” “library” use. may notice messages “conflicts”, nothing need worry .working computer online version RStudio, might gotten error tried run last line. get error, run following line install “tidyverse” package need course.Installing “tidyverse” package takes , ’ll need . installed, can just run library(tidyverse) start RStudio everything need.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\ninstall.packages(\"tidyverse\")"},{"path":"lab1.html","id":"troubleshooting-tip","chapter":"Lab 1 Exploring data with R","heading":"1.2.1 Troubleshooting tip","text":"“tidyverse” package automatically loaded first start RStudio, make sure run linefirst thing time open RStudio. Chances , try running something get error, “tidyverse” package loaded. course, ’s possible thing can cause error! ’s good try first see clears things .","code":"\nlibrary(tidyverse)"},{"path":"lab1.html","id":"meet-your-data","chapter":"Lab 1 Exploring data with R","heading":"1.3 Meet your data","text":"data looking passenger records RMS Titanic, oceanliner famously sank April 15, 1912. Though liner filled capacity, lax safety precautions—including failure carry enough lifeboats—meant many passengers died unable evacuate ship struck iceberg.","code":""},{"path":"lab1.html","id":"load-the-data","chapter":"Lab 1 Exploring data with R","heading":"1.3.1 Load the data","text":"Run following line code console load data RStudio “environment”:now see “titanic” pop upper right hand pane RStudio window. Click “titanic” upper right take look data.","code":"\ntitanic <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/titanic.csv\")## Rows: 1309 Columns: 11\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (7): residence, sex, name, ticket, cabin, embarked, hometown\n## dbl (3): class, age, fare\n## lgl (1): survived\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab1.html","id":"check-out-the-variables","chapter":"Lab 1 Exploring data with R","heading":"1.3.2 Check out the variables","text":"RStudio looks like (helpful colored labels):upper left RStudio screen, ’ll see bunch columns. data “raw” form. row represents specific passenger (“case”) column represents different variable.Exercise 1.1  Find example following types variable dataset. Explain reasoning choice.Numerical (either discrete continuous)Ordinal categoricalNominal categorical","code":""},{"path":"lab1.html","id":"answering-questions-with-data","chapter":"Lab 1 Exploring data with R","heading":"1.4 Answering questions with data","text":"Now ’ve gotten acquainted kind data , can begin using answer questions. involve simplifying data, turning summary form makes easier understand. summaries fall heading “descriptive statistics”, meant describe important aspects data. four types summaries explore today frequency tables, proportions, bar charts, histograms. questions attempt answer survived perished Titanic.","code":""},{"path":"lab1.html","id":"frequency-tables","chapter":"Lab 1 Exploring data with R","heading":"1.4.1 Frequency tables","text":"One way answer literally question, “survived died Titanic?” read names 1300 passengers dataset. problem give us much sense particular people might survived versus . Treating survival response variable, like treat variables data explanatory variables. gives us sense types people less likely survived, tells us Titanic disaster whole.begin, first thing can construct frequency table simply counts number people survived number people died. summary give us sense scale disaster. RStudio window, see big open space just data? called “console” work. Copy paste code “Console”. code appear right “>”. , hit enter run see results.got table counted number people survive. went 1300 rows multiple variables just two numbers. pretty concise summary! ? Let’s break bit code:titanic name dataset.group_by(survived) tells R group cases dataset whether survived (TRUE) (FALSE).summarize(n=n()) tells R take grouped cases summarize counting number people group labeling resulting number “n”.funky symbol %>% connects three steps makes sure R order want. symbol called “pipe”.Let’s try things get sense code . happens change n = n() last line Number = n()?Everything looks except instead column labeled “n”, labeled “Number”. bit equals sign frequency table labeled.Now let’s try something seems like small change: Instead n = n() last line, let’s write n = m(). one letter, surely can’t big difference?R doesn’t like ! reports error doesn’t know m(). ’s n() function, instruction tells R count number something. hand, m() doesn’t mean anything R throws hands.can also get counts based variables. example, let’s ask many passengers “class” changing grouping variable code:Exercise 1.2  Try running chunk code written count number passengers without college degrees:Explain words chunk code work.Finally, let’s construct frequency table using multiple variables. lets us answer complex questions like, many British women aboard Titanic? question involves two variables, residence (people ) sex (whether classified male female). can put variables group_by line find answer.","code":"\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = n())## # A tibble: 2 × 2\n##   survived     n\n##   <lgl>    <int>\n## 1 FALSE      809\n## 2 TRUE       500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(Number = n())## # A tibble: 2 × 2\n##   survived Number\n##   <lgl>     <int>\n## 1 FALSE       809\n## 2 TRUE        500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = m())## Error in `summarize()`:\n## ! Problem while computing `n = m()`.\n## ℹ The error occurred in group 1: survived = FALSE.\n## Caused by error in `m()`:\n## ! could not find function \"m\"\ntitanic %>%\n  group_by(class) %>%\n  summarize(n = n())## # A tibble: 3 × 2\n##   class     n\n##   <dbl> <int>\n## 1     1   323\n## 2     2   277\n## 3     3   709\ntitanic %>%\n  group_by(degree) %>%\n  summarize(n = n())\ntitanic %>%\n  group_by(residence, sex) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'residence'. You can override using the\n## `.groups` argument.## # A tibble: 6 × 3\n## # Groups:   residence [3]\n##   residence sex        n\n##   <chr>     <chr>  <int>\n## 1 American  Female   108\n## 2 American  Male     150\n## 3 British   Female    94\n## 4 British   Male     208\n## 5 Other     Female   264\n## 6 Other     Male     485"},{"path":"lab1.html","id":"titanic-props","chapter":"Lab 1 Exploring data with R","heading":"1.4.2 Proportions","text":"may heard , trying evacuate Titanic, rule put “women children first” onto lifeboats. suggests hypothesis, assuming rule actually followed: female passengers survived often male passengers.see data consistent hypothesis, can begin using code similar ’ve using count number male female passengers either survive. Notice can group_by one variable using list variable names separated commas:table contains information need test hypothesis, hard read different numbers male female passengers. want know whether greater proportion female passengers survived, compared proportion male passengers survived.find proportions taking count dividing sum counts. Specifically, group “” want find proportion elements group “” characteristic “B”, find\\[\n\\text{Proportion B} = \\frac{\\text{Number B}}{\\text{Total number 's}}\n\\]","code":"\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.## # A tibble: 4 × 3\n## # Groups:   sex [2]\n##   sex    survived     n\n##   <chr>  <lgl>    <int>\n## 1 Female FALSE      127\n## 2 Female TRUE       339\n## 3 Male   FALSE      682\n## 4 Male   TRUE       161"},{"path":"lab1.html","id":"r-is-a-fancy-calculator","chapter":"Lab 1 Exploring data with R","heading":"1.4.2.1 R is a fancy calculator","text":"Let’s use numbers frequency table find proportion male passengers survived proportion female passengers survived. illustrate , although R quite powerful, many ways just fancy calculator. calculator still handy!table just made, see 339 female passengers survived. \\(339 + 127\\) total female passengers. can use R find proportion female passengers survived using line code :Notice / stands “division” put \\(339 + 127\\) parentheses tell R whole sum denominator.Exercise 1.3  Find proportion male passengers survived (Hint: numbers need come table showed number female passengers survive.). Compare proportion survivors among male female passengers—proportions consistent “women children first” policy? Explain reasoning.","code":"\n339 / (339 + 127)## [1] 0.7274678"},{"path":"lab1.html","id":"another-way-to-get-proportions","chapter":"Lab 1 Exploring data with R","heading":"1.4.2.2 Another way to get proportions","text":"One thing notice R many ways thing. Instead calculating proportions hand, can add line code used make frequency table get R give us proportions female male passengers survive:new column p proportion represents proportion people group (either male female) either survive. Notice numbers p column male female survivors ones found preceding section.Exercise 1.4  Write run code gives us table, like one , shows proportion people class (1, 2, 3) survived died. , find helpful start code already used try filling blanks:code use? Class highest proportion survivors?","code":"\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.## # A tibble: 4 × 4\n## # Groups:   sex [2]\n##   sex    survived     n     p\n##   <chr>  <lgl>    <int> <dbl>\n## 1 Female FALSE      127 0.273\n## 2 Female TRUE       339 0.727\n## 3 Male   FALSE      682 0.809\n## 4 Male   TRUE       161 0.191titanic %>%\n  group_by(___, survived) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))"},{"path":"lab1.html","id":"titanic-bar","chapter":"Lab 1 Exploring data with R","heading":"1.4.3 Bar charts","text":"looking patterns trends data, often easier see visualization rather table numbers. Bar charts make numerical relationships easy see visually, don’t need compare bunch numbers.example, made table count number passengers class. bar chart conveys information terms height bars.Pretty neat! now easy see many 3rd class passengers 1st 2nd, interestingly, fewer 2nd class 1st class passengers.code used similar ’ve using, differs important ways:\n* first line , telling R dataset using (titanic).\n* second line tells R want make plot want put variable class along horizontal axis plot (x axis). “gg” front “plot” refers “grammar graphics”, language R uses describe plots. language, different parts plot called “aesthetics”, x = class falls inside parenthetical labeled aes(thetic).\n* final line just tells R want make bar chart. grammar graphics, different types charts called geoms.\n* Notice second 2 lines connected + rather %>% symbol. historical accident, meaning two symbols basically . telling R order follow instructions.put bar charts side--side, can use compare groups. R, putting multiple graphs together called faceting. graph “facet”. can tell R make facets based specific variable adding line code, like :line end splits plot different “facets”, one level residence variable. Note put “faceting” variable name quotes (reason). result makes easy see distribution passengers across classes different depending —Americans Titanic tended wealthier first class passengers, relative passengers Britain elsewhere.Exercise 1.5  Make bar chart shows number people either survive depending country residence. , fill blanks code “facet” corresponds country residence “facet” two bars , one bar survivors one bar non-survivors.code use? possible reason relative number survivors different depending passengers ?","code":"\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar()\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar() +\n  facet_wrap(\"residence\")titanic %>%\n  ggplot(aes(x = ___)) +\n  geom_bar() +\n  facet_wrap(\"___\")"},{"path":"lab1.html","id":"histograms","chapter":"Lab 1 Exploring data with R","heading":"1.4.4 Histograms","text":"far, summarizing categorical variables. also numerical variables data, example age passenger well much paid tickets. Let’s try making frequency table figure many people different ages sailed Titanic:Well ’s helpful! R didn’t even bother show us whole thing. Though can see something interesting: Age measured years, passengers least one year old, age whole number. fractions years passengers less year old—ages measured months rather years.main point even though age can measured less fine-grained manner, age effectively continuous. don’t want know many passengers exactly 31.3491 years old, want get general sense distribution ages across passengers.can construct summary conveys information using histogram. histogram similar bar chart; difference bar charts categorical variables histograms numerical variables. code constructs histogram summarize passenger age:resulting histogram shows bunch bars, height indicate number passengers within particular age range. Notice got couple messages R addition plot, one “non-finite values” another “picking better value”. R says, “non-finite values”, talking people age recorded. unfortunate thing real data: sometimes missing pieces. didn’t stop R making plot wanted using non-missing data, R wanted warn us just case.message “picking better value” important: make histogram, looking many things fall within particular range values, say, ages 4 8. decide ranges? don’t tell R , decide divide range values 30 “bins”, corresponds range values width. usually want.Instead, decide big small want ranges . following code tells R make histogram using “bins” 2 years “wide” (0-1, 2-3, etc.):just like bar charts, can split histogram different “facets”. pair histograms shows distribution ages passengers either survive:Exercise 1.6  Try making several different histograms passenger age split survival, using different bin widths:bin width believe gives best visual summary ? Describe whether shapes histograms facet consistent “women children first” rule.","code":"\ntitanic %>%\n  group_by(age) %>%\n  summarize(n = n())## # A tibble: 99 × 2\n##      age     n\n##    <dbl> <int>\n##  1 0.167     1\n##  2 0.333     1\n##  3 0.417     1\n##  4 0.667     1\n##  5 0.75      3\n##  6 0.833     3\n##  7 0.917     2\n##  8 1        10\n##  9 2        12\n## 10 3         7\n## # … with 89 more rows\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2)## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(\"survived\")## Warning: Removed 263 rows containing non-finite values (stat_bin).titanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = ___) +\n  facet_wrap(\"survived\")"},{"path":"lab1.html","id":"wrap-up","chapter":"Lab 1 Exploring data with R","heading":"1.5 Wrap-up","text":"Today began adventure using RStudio explore data. saw look data summarize various helpful ways. frequency tables, proportions, bar charts, histograms.Frequency tables count number times particular value particular variable (combination values across multiple variables) occurs dataset.can use counts frequency tables calculate proportions, better conveying relative values.Bar charts display counts categorical variables visual form makes easier compare .Histograms let us visually summarize counts numerical variables putting “bins”, width need decide.","code":""},{"path":"lab2.html","id":"lab2","chapter":"Lab 2 Sampling","heading":"Lab 2 Sampling","text":"previous activity, got exposure can explore data R way can use data help answer questions. activity, get sense data come processes produce data can force us change interpret data.Generally, data come sample larger population. Depending sample selected, may give us biased perspective larger population. particular, non-random samples (observational studies), must think carefully processes cases end sample. processes, might social nature, introduce confounding variables, can systematic differences different groups sample, requiring us interpret data differently.","code":""},{"path":"lab2.html","id":"first-things-first","chapter":"Lab 2 Sampling","heading":"2.1 First things first","text":"First, start RStudio.usual, next thing starting RStudio load tidyverse package R’s library using following line code (run RStudio console, like last time):Now ’s done, can get data.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab2.html","id":"sex-bias-sampling-bias-or-both","chapter":"Lab 2 Sampling","heading":"2.2 Sex Bias, Sampling Bias, or Both?","text":"1973, University California Berkeley noted disturbing fact graduate school admissions: considerably male applicants admitted female applicants. University, fearing lawsuit, wanted know reflected systematic sex discrimination admissions offices. shall see story turned complex (full details, see Bickel et al. (1973)).","code":""},{"path":"lab2.html","id":"load-the-data-1","chapter":"Lab 2 Sampling","heading":"2.2.1 Load the data","text":"First, run code load 1973 Berkeley admissions data workspace:berkeley dataset now visible “Environment” pane upper right corner RStudio. Click look raw data.applicant selected sample, clearly random. row data refers specific applicant 1973. applicant, three observed variables:Admit: Either “Admitted” “Rejected”, depending whether applicant admitted rejected.Gender: Either “Male” “Female”.Department: letter “” “F” identifies academic department applicant applied. actual department names obscured privacy.Exercise 2.1  type (nominal categorical, ordinal categorical, numerical) variables dataset?","code":"\nberkeley <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/berkeley.csv\")## Rows: 4526 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): Admit, Gender, Department\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab2.html","id":"are-more-males-than-females-admitted","chapter":"Lab 2 Sampling","heading":"2.2.2 Are more males than females admitted?","text":"noted , Berkeley concerned greater proportion male applicants admitted female applicants. verify whether true, let’s find proportions running following chunk code:rightmost column table (labeled p) gives proportion applicants gender either admitted rejected. Based table, looks like Berkeley right concerned—proportion male applicants admitted higher proportion female applicants admitted.Exercise 2.2  Compare chunk code just ran code used last activity section. note four lines following structure:four lines code , describe words line .","code":"\nberkeley %>%\n  group_by(Gender, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Gender'. You can override using the\n## `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   Gender [2]\n##   Gender Admit        n     p\n##   <chr>  <chr>    <int> <dbl>\n## 1 Female Admitted   557 0.304\n## 2 Female Rejected  1278 0.696\n## 3 Male   Admitted  1198 0.445\n## 4 Male   Rejected  1493 0.555___ %>%\n  group_by(___) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))"},{"path":"lab2.html","id":"is-there-a-bias-in-all-departments","chapter":"Lab 2 Sampling","heading":"2.2.3 Is there a bias in all departments?","text":"Even overall bias, case departments, just ?Exercise 2.3  Make new table gives proportion male female applicants admitted department. may find helpful fill blanks following chunk code (note new line end “filters” redundant rows table make long):departments admit higher proportion female applicants admit higher proportion male applicants?","code":"berkeley %>%\n  group_by(___, ___, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n)) %>%\n  filter(Admit == \"Admitted\")"},{"path":"lab2.html","id":"resolving-the-paradox","chapter":"Lab 2 Sampling","heading":"2.2.4 Resolving the paradox","text":"appear paradox hands: ignore differences departments, likely male applicant gets admitted female applicant. within departments, opposite true: greater proportion female applicants admitted male applicants. words, seems confounding variable work, related differences departments.try figure confound might , look two additional issues. first issue , department, proportion applicants female, regardless whether admitted ? can address question running following chunk code:Exercise 2.4  Based table just produced, departments male female applicants? overlap departments identified previous exercise admitting higher proportion female male applicants?second issue consider overall rate admission department.Exercise 2.5  Make table shows proportion applicants admitted department, regardless gender. may find useful modify chunk code just used.code use? Put departments order highest lowest admission rate.Finally, let’s create visualization may help us put pieces together. chunk code produces set bar charts, one department. similar made previous activity, new “aesthetic”, namely, use variable (Gender) fill bars different colors:Exercise 2.6  Compare code just ran create colored bar chart kind code used make bar charts previous activity. seems different?plot just made, total height bar represents total number applicants department either admitted rejected. bar divided two parts different colors, representing numbers male female applicants bar. can use total height bars see relative number people admitted vs. rejected department; can use amount red vs. teal panel see relative number female vs. male applicants department.Putting pieces together, found following:Across departments, proportion female applicants admitted lower proportion male applicants admitted.Within departments, proportion female applicants admitted higher proportion male applicants admitted.Departments high overall admission rates receive fewer female applicants departments low admission rates.Finally, note , although names specific departments removed, departments B (easy get received predominantly male applicants) physical sciences engineering whereas departments E F (hard get received considerably female applicants) social sciences humanities.Exercise 2.7  Even though results suggest sex bias admissions level individual departments, kinds bias results suggest might going ? potential biases related types sampling biases ’ve discussed class book?","code":"\nberkeley %>%\n  group_by(Department, Gender) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Department'. You can override using the\n## `.groups` argument.## # A tibble: 12 × 4\n## # Groups:   Department [6]\n##    Department Gender     n      p\n##    <chr>      <chr>  <int>  <dbl>\n##  1 A          Female   108 0.116 \n##  2 A          Male     825 0.884 \n##  3 B          Female    25 0.0427\n##  4 B          Male     560 0.957 \n##  5 C          Female   593 0.646 \n##  6 C          Male     325 0.354 \n##  7 D          Female   375 0.473 \n##  8 D          Male     417 0.527 \n##  9 E          Female   393 0.673 \n## 10 E          Male     191 0.327 \n## 11 F          Female   341 0.478 \n## 12 F          Male     373 0.522\nberkeley %>%\n  ggplot(aes(x = Admit, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(\"Department\")"},{"path":"lab2.html","id":"wrap-up-1","chapter":"Lab 2 Sampling","heading":"2.3 Wrap-up","text":"paradox confronted activity name: “Simpson’s Paradox”. Check good video demonstration paradox .Simpson’s Paradox occurs whenever pattern appears aggregate (like bias female applicants departments) disappears reverses look different subgroups (like apparent bias favor female applicants within departments). done activity, Simpson’s Paradox can resolved careful exploration data consideration potential confounding variables associated different subgroups.","code":""},{"path":"lab3.html","id":"lab3","chapter":"Lab 3 Describing data","heading":"Lab 3 Describing data","text":"session divided two parts. first, use R understand two measures variability work: standard deviation close cousin, variance. see valuable computer can quickly produce numerical visual summaries data.computers great mindless computation, means us people better mindful stuff. need think numbers mean may important understanding world.second part session analyze data experiment. use visual numerical summaries help draw conclusions going experiment.","code":""},{"path":"lab3.html","id":"load-the-tidyverse-library","chapter":"Lab 3 Describing data","heading":"3.1 Load the tidyverse library","text":"starting RStudio, begin making sure ’ve got tidyverse package loaded R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab3.html","id":"variance-and-standard-deviation","chapter":"Lab 3 Describing data","heading":"3.2 Variance and standard deviation","text":"Two important widely-used numerical summaries variability variance standard deviation. Although two different names, really two ways looking thing. standard deviation often used summary variability, although shall see variance useful purposes later course.summaries involve somewhat long chain calculations. important see chain understand numbers telling us. part session, use R find variance standard deviation simple simulated data. First, follow long chain. , see find summaries quickly efficiently.","code":""},{"path":"lab3.html","id":"the-big-picture","chapter":"Lab 3 Describing data","heading":"3.2.1 The big picture","text":"good numerical summary variability tell us much expect observed values variable differ “typical”. “standard deviation” means: summary typical (“standard”) amount observed values variable tend differ (“deviate”) central tendency. variance standard deviation based using mean describe central tendency.","code":""},{"path":"lab3.html","id":"load-the-data-2","chapter":"Lab 3 Describing data","heading":"3.2.2 Load the data","text":"see variance standard deviation calculated, let’s create small fake data set, creatively call my_data. contains 5 observations single variable called “X”.Although data real, similar one might observe random sample IQ scores (mean 100 standard deviation 15 typical population).","code":"\nmy_data <- tibble(X = c(91, 102, 107, 109, 111))"},{"path":"lab3.html","id":"step-by-step","chapter":"Lab 3 Describing data","heading":"3.2.3 Step by step","text":"now follow chain calculations, step step, results finding variance standard deviation set observed values like “X” variable simulated data.","code":""},{"path":"lab3.html","id":"find-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.1 Find the mean","text":"First, find mean observed values. R , many ways, fancy calculator. find mean observed values X variable plugging formula mean already know:using R calculator advantages hand, downside still type/copy numbers. Moreover, wanted use result something else (), ’d copy , , greater chances make mistake.Rather typing numbers manually, instead use R’s mean function within “summarize” line ’ve used :Exercise 3.1  ’ve used “summarize” function previous sessions, always preceded group_by line. Say words didn’t use group_by line find mean chunk code just ran.","code":"\n(91 + 102 + 107 + 109 + 111) / 5## [1] 104\nmy_data %>%\n    summarize(mean(X))## # A tibble: 1 × 1\n##   `mean(X)`\n##       <dbl>\n## 1       104"},{"path":"lab3.html","id":"find-the-deviations-from-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.2 Find the deviations from the mean","text":"next step find, observed value data, deviation mean. mathematical notation, deviation can written \\((x_i - \\bar{x})\\) \\(x_i\\) shorthand observed value \\(\\bar{x}\\) mean.previous sessions, ’ve used mutate line transform counts proportions. Now use find deviations:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X))## # A tibble: 5 × 2\n##       X deviation\n##   <dbl>     <dbl>\n## 1    91       -13\n## 2   102        -2\n## 3   107         3\n## 4   109         5\n## 5   111         7"},{"path":"lab3.html","id":"square-the-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.3 Square the deviations","text":"Now ’ve found deviations, square , using mutate line:Note , R, caret symbol ^ used exponentiation.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2)## # A tibble: 5 × 3\n##       X deviation squared_deviation\n##   <dbl>     <dbl>             <dbl>\n## 1    91       -13               169\n## 2   102        -2                 4\n## 3   107         3                 9\n## 4   109         5                25\n## 5   111         7                49"},{"path":"lab3.html","id":"add-up-the-squared-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.4 Add up the squared deviations","text":"Now, add squared deviations using sum function part summarize line:Exercise 3.2  Modify chunk code just ran just add deviations instead squared_deviations. (Hint: think variable name put inside parentheses “sum” last line.)sum deviations?think sum deviations might relate idea mean “balance point” distribution numbers?","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation))## # A tibble: 1 × 1\n##   sum_squared_deviation\n##                   <dbl>\n## 1                   256"},{"path":"lab3.html","id":"variance-divide-by-n---1","chapter":"Lab 3 Describing data","heading":"3.2.3.5 Variance: Divide by \\(n - 1\\)","text":"variance sum squared deviations divided \\(n - 1\\), \\(n\\) number observed values, “sample size”. get \\(n\\), can include n = n() summarize line use another mutate line get variance:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1))## # A tibble: 1 × 3\n##   sum_squared_deviation     n variance\n##                   <dbl> <int>    <dbl>\n## 1                   256     5       64"},{"path":"lab3.html","id":"standard-deviation-take-the-square-root","chapter":"Lab 3 Describing data","heading":"3.2.3.6 Standard deviation: Take the square root","text":"Finally, get standard deviation taking square root variance, final mutate line. R, sqrt function stands square root.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1)) %>%\n    mutate(standard_deviation = sqrt(variance))## # A tibble: 1 × 4\n##   sum_squared_deviation     n variance standard_deviation\n##                   <dbl> <int>    <dbl>              <dbl>\n## 1                   256     5       64                  8"},{"path":"lab3.html","id":"all-at-once","chapter":"Lab 3 Describing data","heading":"3.2.4 All at once","text":"just saw complete chain calculations involved finding variance standard deviation. might guessed fact R mean function, also functions directly calculate variances standard deviations. functions called var sd, respectively, can use summary line, like :code just ran much compact, readable, less liable lead typos, naturally use built-var sd functions pretty often!Exercise 3.3  Take look used mutate summarize lines calculating variance standard deviation step--step. words, describe basic difference mutate line used summarize line used .Hint: Think result got adding either mutate summarize line affected number rows /columns result.","code":"\nmy_data %>%\n    summarize(mean = mean(X), variance = var(X), standard_deviation = sd(X))## # A tibble: 1 × 3\n##    mean variance standard_deviation\n##   <dbl>    <dbl>              <dbl>\n## 1   104       64                  8"},{"path":"lab3.html","id":"diet-and-lifespan","chapter":"Lab 3 Describing data","heading":"3.3 Diet and Lifespan","text":"seen use R calculate numerical summaries central tendency variability, let’s see operate real data. data looking come study Yu et al. (1982). studied lifespan sample rats randomly assigned one two different diets: One group rats allowed eat freely, however wanted; another group fed restricted diet 60% calories free-eating rats .","code":""},{"path":"lab3.html","id":"load-the-data-3","chapter":"Lab 3 Describing data","heading":"3.3.1 Load the data","text":"First, need get data R. following code download import data current R session:“Environment” panel upper right, ’ll see new entry called “rats”. data just imported. Click look data, appear upper left. just two variables dataset, Diet Lifespan (measured days). Diet explanatory variable Lifespan response variable.","code":"\nrats <- read_csv('https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv')## Rows: 195 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Diet\n## dbl (1): Lifespan\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab3.html","id":"summarizing-the-data-visually","chapter":"Lab 3 Describing data","heading":"3.3.2 Summarizing the data visually","text":"Although numerical summaries useful, always accompanied visual summaries order get complete understanding data.","code":""},{"path":"lab3.html","id":"histograms-1","chapter":"Lab 3 Describing data","heading":"3.3.2.1 Histograms","text":"Let’s make facetted histogram shows distribution lifespans rats diet. use option facet_wrap lets us set number columns (ncol) can see facets stacked atop one another.Exercise 3.4  First, let’s describe can see histograms (may also try bin widths histograms like).Describe shape two distributions. sure note number modes, skewness, whether may outliers either group.Compare two distributions. sure note whether seem differences central tendency variability two groups.","code":"\nrats %>%\n    ggplot(aes(x=Lifespan)) +\n    geom_histogram(binwidth=90) +\n    facet_wrap(\"Diet\", ncol = 1)"},{"path":"lab3.html","id":"boxplots","chapter":"Lab 3 Describing data","heading":"3.3.2.2 Boxplots","text":"’ve also seen examples boxplots useful visual summaries data. “box” boxplot encloses “inter-quartile range” (IQR), , middle 50% data falls 25th 75th percentiles. line middle box shows median . Boxplots easy produce R using geom_boxplot instead geom_histogram:Exercise 3.5  Compare boxplots histograms just made.sorts things seem easier boxplots opposed histograms? Think features data might easier see boxplot comparisons might easier make using boxplot.lose anything using boxplot instead histogram? important features data harder see boxplot?","code":"\nrats %>%\n    ggplot(aes(x=Lifespan, y=Diet)) +\n    geom_boxplot()"},{"path":"lab3.html","id":"summarizing-the-data-numerically","chapter":"Lab 3 Describing data","heading":"3.3.3 Summarizing the data numerically","text":"Finally, let’s use R get numerical summary lifespans rats two diets. first part session, focused mean standard deviation (variance), also seen median inter-quartile range (IQR) can also used numerical summaries central tendency variability.R can find median IQR us:Exercise 3.6  Fill blanks following chunk code find mean standard deviation lifespans rats diet. Hint: recall names functions used find quantities earlier.code use?Compare means standard deviations group rats histograms made earlier. differences mean lifespan groups reflected two histograms? differences standard deviation lifespan reflected two histograms?numerical summaries central tendency variability provide concise description lifespans rats two different diets. descriptions help us address research question: restricted diet effect lifespan?Exercise 3.7  Based visual numerical summaries made, let us draw conclusions research question.Briefly describe differences lifespans rats restricted diet versus rats free diet.Based design study, can conclude differences diet probably played role causing observed differences lifespan sample? ?population think generalize results?","code":"\nrats %>%\n    group_by(Diet) %>%\n    summarize(Median = median(Lifespan), IQR = IQR(Lifespan))## # A tibble: 2 × 3\n##   Diet       Median   IQR\n##   <chr>       <dbl> <dbl>\n## 1 Free         710   116 \n## 2 Restricted  1036.  300.___ %>%\n    group_by(___) %>%\n    ___(Mean = ___, SD = ___)"},{"path":"lab3.html","id":"wrap-up-2","chapter":"Lab 3 Describing data","heading":"3.4 Wrap-up","text":"session, saw find two important numerical summaries variability: variance standard deviation. analyzed data experiment studying relationship diet lifespan sample rats, using visual numerical summaries get complete picture relationships explanatory response variables.","code":""},{"path":"lab4.html","id":"lab4","chapter":"Lab 4 Relationships between numerical variables","heading":"Lab 4 Relationships between numerical variables","text":"session, use R help us describe relationships numerical variables. Typically, purpose see well can predict explain differences response variable terms differences explanatory variable. seen, several useful tools work : scatterplots, correlation, linear regression.","code":""},{"path":"lab4.html","id":"load-the-tidyverse","chapter":"Lab 4 Relationships between numerical variables","heading":"4.1 Load the tidyverse","text":"usual, started RStudio, first thing sure load tidyverse package R’s library using line :","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab4.html","id":"always-plot-your-data","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2 Always plot your data","text":"Numerical summaries data extremely useful compact descriptions things like central tendency variability. also seen Pearson correlation coefficient useful summary strength direction relationship numerical variables. numerical summaries can also misleading, shall see following example.","code":""},{"path":"lab4.html","id":"load-the-data-4","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.1 Load the data","text":"First, need import data R using code .data artificial devised Anscombe (1973).","code":"\nanscombe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv\")## Rows: 44 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Group\n## dbl (2): X, Y\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"correlations-for-different-sets-of-measurements","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.2 Correlations for different sets of measurements","text":"four groups observations data. groups labeled “”, “B”, “C”, “D”. Observations made two variables, labeled “X” “Y”. Click anscombe environment panel RStudio (upper right) take look.Using R, can quickly get numerical summaries data. code provides mean standard deviation X Y values group, well correlation X Y group.notice anything funny results? looks like basically differences groups!","code":"\nanscombe %>%\n    group_by(Group) %>%\n    summarize(Mean_X = mean(X), Mean_Y = mean(Y), SD_X = sd(X), SD_Y = sd(Y), r = cor(X, Y))## # A tibble: 4 × 6\n##   Group Mean_X Mean_Y  SD_X  SD_Y     r\n##   <chr>  <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 A          9   7.50  3.32  2.03 0.816\n## 2 B          9   7.50  3.32  2.03 0.816\n## 3 C          9   7.5   3.32  2.03 0.816\n## 4 D          9   7.50  3.32  2.03 0.817"},{"path":"lab4.html","id":"scatterplots","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.3 Scatterplots","text":"Rather numerical summary, now let’s use R visually summarize data using scatterplots. put X variable horizontal (“x”) axis Y variable vertical (“y”) axis. use Group “facetting” variable:Exercise 4.1  scatterplots , respond following questions:scatterplot indicate probably association two variables? , describe type relationship.correlation coefficient (r) provide good summary whether two variables related? ?","code":"\nanscombe %>%\n    ggplot(aes(x=X, y=Y)) +\n    geom_point() +\n    facet_wrap(\"Group\")"},{"path":"lab4.html","id":"and-now-for-a-word","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3 And now for a word","text":"Psycholinguistics study perceptual cognitive processes involved learning, understanding, producing language. One ways psycholinguists study language processing using “lexical decision task”. lexical decision task, participants shown strings letters; sometimes, make real words (like “AUTHOR”) sometimes don’t (like “AWBLOR”). time someone takes decide string letters real word (AUTHOR) measure easily knowledge word can accessed. looking relationships lexical decision time (LDT) different properties word, can begin understand processes organize access knowledge language. words, lexical decision time tells us “mental dictionary” structured.English Lexicon Project collecting kind data lot people many different words English language. report mean lexical decision time word, along number properties word. treat mean lexical decision time response variable examine relationships number explanatory variables.","code":""},{"path":"lab4.html","id":"load-the-data-5","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.1 Load the data","text":"Run following line code download subset data English Lexicon Project.case/observation dataset particular word. addition lexical decision time, several potential explanatory variables measured word.","code":"\nelp <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/elp.csv\")## Rows: 31433 Columns: 25\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): Word, POS\n## dbl (23): Length, Freq_KF, Freq_HAL, SUBTLWF, SUBTLCD, Ortho_N, Phono_N, OLD...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"examine-the-distribution-of-lexical-decision-times","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.2 Examine the distribution of lexical decision times","text":"LDT variable contains lexical decision time word dataset. LDT measured milliseconds. Let us first examine distribution lexical decision times using code :Exercise 4.2  Describe distribution lexical decision times across words (can re-run code look binwidths like). sure note number modes, skewness, whether potential outliers.","code":"\nelp %>%\n  ggplot(aes(x = LDT)) +\n  geom_histogram(binwidth=20)"},{"path":"lab4.html","id":"word-length-as-an-explanatory-variable","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3 Word length as an explanatory variable","text":"natural research question ask point whether LDT can explained word length. words, number letters word affect easy recognize?","code":""},{"path":"lab4.html","id":"scatterplot","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.1 Scatterplot","text":"First, make scatterplot two variables. Word length recorded variable named Length.scatterplot suggests positive relationship described line.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point()"},{"path":"lab4.html","id":"overlaying-a-line","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.2 Overlaying a line","text":"can easily put best-fitting linear regression line top scatterplot get visual sense well linear model describe relationship. , add line called geom_smooth(method = \"lm\"). geom_smooth puts “smooth” lines curves plot, including method = \"lm\" parentheses tells R specifically want linear model.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"lab4.html","id":"finding-the-correlation","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.3 Finding the correlation","text":"can get correlation coefficient using cor function, like :","code":"\nelp %>%\n    summarize(r = cor(LDT, Length))## # A tibble: 1 × 1\n##       r\n##   <dbl>\n## 1 0.543"},{"path":"lab4.html","id":"finding-the-slope-and-intercept","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.4 Finding the slope and intercept","text":"Finally, can get slope intercept best-fitting line. line code looks little different ’ve done things now. just single line called “lm” “linear model”. parentheses, two instructions separated comma. first says explanatory response variables using funky format [response variable name] ~ [explanatory variable name]. second instruction parentheses tells R data find variables .’s looks together:result line gives us intercept best fitting line, well slope. slope labeled terms name explanatory variable.Exercise 4.3  Based result got running code , additional letter word , much longer take recognize word?","code":"\nlm(LDT ~ Length, data = elp)## \n## Call:\n## lm(formula = LDT ~ Length, data = elp)\n## \n## Coefficients:\n## (Intercept)       Length  \n##      546.33        28.15"},{"path":"lab4.html","id":"age-of-acquisition-as-an-explanatory-variable","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4 Age of Acquisition as an explanatory variable","text":"Although makes sense longer word take longer recognize, another important aspect word learned. reasonable think word learn early life easier access one learned recently.English Lexicon Project also records mean “age acquisition” word. mean age (years) someone first learns word. variable labeled Age_Of_Acquisition elp data. Now, follow steps looking relationship Length LDT, instead Length explanatory variable, use “age acquisition”.following exercises, sure refer code used previous section.","code":""},{"path":"lab4.html","id":"scatterplot-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.1 Scatterplot","text":", let’s first make scatterplot visualize relationship LDT age acquisition.Exercise 4.4  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis):code use?describe relationship LDT age acquisition?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___()"},{"path":"lab4.html","id":"overlaying-a-line-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.2 Overlaying a line","text":"Now, let’s add line scatterplot.Exercise 4.5  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis) best-fitting line overlaid top.code use?line seem good fit data? areas line seems -shoot -shoot data?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___() +\n  ___(method = \"lm\")"},{"path":"lab4.html","id":"finding-the-correlation-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.3 Finding the correlation","text":"Now let’s find correlation (, sure look code used previous section guide):Exercise 4.6  Fill blanks find correlation LDT Age_Of_Acquisition:code use?correlation LDT Age Acquisition stronger weaker correlation LDT Length?ELP dataset, Age Acquisition measured years LDT measured milliseconds. correlation LDT Age Acquisition change Age Acquisition measured months LDT measured seconds? ?","code":"___ %>%\n    summarize(r = cor(___, ___))"},{"path":"lab4.html","id":"finding-the-slope-and-intercept-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.4 Finding the slope and intercept","text":"Finally, let’s find slope intercept best-fitting regression line Age Acquisition LDT:Exercise 4.7  Fill blanks code find intercept slope line using Age Acquisition explanatory variable LDT response variable:code use?According linear model just found, much longer take recognize word learned age 10, relative word learned age 9?make sense try extend relationship young ages (e.g., 6 months old)? Explain reasoning.expect relationship continue words learned relatively later life, like technical words learn college work? , shape expect relationship words learned later life ?","code":"lm(___ ~ ___, data = ___)"},{"path":"lab4.html","id":"summary","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.5 Summary","text":"seen can predict long takes recognize word terms either length (number letters) age learned (Age Acquisition). results tell us “mental dictionary” organized just things like spelling, also life experience.","code":""},{"path":"lab4.html","id":"wrap-up-3","chapter":"Lab 4 Relationships between numerical variables","heading":"4.4 Wrap-up","text":"seen scatterplots, correlation, linear regression valuable tools describing relationships numerical variables. tools help us explain details behavior reveal structure memory. tools always used carefully always visualization, since numerical summaries alone can misleading.","code":""},{"path":"lab5.html","id":"lab5","chapter":"Lab 5 Hypothesis testing with randomization","heading":"Lab 5 Hypothesis testing with randomization","text":"session, get initial practice testing hypotheses randomization. practice cover nuances hypothesis testing statistics, touch many key ideas see different forms throughout rest course.first part, get sense permutation works, helps us simulate sample might look like null hypothesis true. Permutation one way can use “randomization” test null hypothesis. second part, use permutation test serious hypotheses different types people might make decisions differently.Generally, hypothesis testing requires us follow set steps can outline like :Translate research question null alternative hypotheses, framed terms population parametersUse data sample calculate summary statistics corresponding population parametersModel summary statistics null hypothesis trueFind \\(p\\) valueForm conclusionTo help work exercises activity, sure download worksheet open RStudio.","code":""},{"path":"lab5.html","id":"did-kobe-have-a-hot-hand","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1 Did Kobe have a hot hand?","text":"get handle big ideas hypothesis testing general, permutation particular, let’s first look special dataset. data pertain Kobe Bryant LA Lakers playing Orlando Magic 2009 NBA finals. Commentators time remarked Kobe seemed “hot hand”. words, claiming Kobe made basket, likely make basket next shot.","code":""},{"path":"lab5.html","id":"check-out-the-data","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.1 Check out the data","text":"loaded R dataset called kobe consists every shooting attempt Kobe made game, including whether went (.e., shot “Hit” “Miss”). ’s first rows:","code":""},{"path":"lab5.html","id":"framing-the-hypotheses","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.2 Framing the hypotheses","text":"can translate claim hot hand null hypothesis alternative hypothesis. us believe “hot hand” claim, first rule possibility Kobe’s hit proportion regardless whether previous shot went . possibility null hypothesis. alternative hypothesis Kobe really hot hand made greater proportion hits already made hit missing.","code":""},{"path":"lab5.html","id":"summarize-the-data","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.3 Summarize the data","text":"Now ’ve framed hypotheses, let’s see whether data suggest reject null hypothesis. ’ve already seen using frequency table, like :, prev_shot refers whether Kobe’s previous shot hit (“prev_H”) miss (“prev_M”). final column p gives us proportions Hits (“H”) Misses (“M”) following either Previous Hit Previous Miss.use proportions final column calculate hand difference proportion Kobe’s made shots (Hits) following either hit (prev_H) miss (prev_M). can use R work us using chunk like one:chunk works like :Tell R name dataset working (kobe).specify variable explanatory variable (prev_shot) variable response variable (shot). using squiggly thing used linear regression ([response variable] ~ [explanatory variable]). response variable binary (.e., two-level) categorical variable, also need tell R “success” Hit (abbreviated “H”).Tell R calculate particular summary statistic. saying stat = \"diff props\" telling R want calculate difference proportions. already told R response variable shot success Hit, R knows want calculate difference proportion shots hits. saying order = c(\"prev_H\", \"prev_M\"), tell R want difference calculated specific order. math shorthand, write \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\).may seem like lot, remember computers needs told everything detail. can’t trust computer figure anything (yet?). Moreover, see lines code can “remixed” various ways convenient us go.Exercise 5.1  –following chunk code changes success setting \"H\" \"M\". words, describe number means get code.following chunk code changes order setting, puts success setting back \"H\". words, describe number means get code (different one part [] exercise!).","code":"\nkobe %>%\n    group_by(prev_shot, shot) %>%\n    summarize(n = n()) %>%\n    mutate(p = n / sum(n))## `summarise()` has grouped output by 'prev_shot'. You can override using the\n## `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   prev_shot [2]\n##   prev_shot shot      n     p\n##   <chr>     <chr> <int> <dbl>\n## 1 prev_H    H        18 0.36 \n## 2 prev_H    M        32 0.64 \n## 3 prev_M    H        25 0.410\n## 4 prev_M    M        36 0.590\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##      stat\n##     <dbl>\n## 1 -0.0498\nkobe %>%\n    specify(shot ~ prev_shot, success = \"M\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##     stat\n##    <dbl>\n## 1 0.0498\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_M\", \"prev_H\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##     stat\n##    <dbl>\n## 1 0.0498"},{"path":"lab5.html","id":"simulating-a-possible-dataset-if-the-null-were-true","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.4 Simulating a possible dataset if the null were true","text":"null hypothesis true, whether Kobe make shot depend whether make previous shot. can model Kobe’s performance look like null hypothesis true. , treat Kobe’s shooting history random. null hypothesis true, able randomly permute Kobe’s shooting history without changing overall relationship Kobe’s previous shots current shot. “overall relationship”, mean difference proportions just calculated. null hypothesis true, difference proportions permuted data different difference proportions actual data. hand, null hypothesis false, difference proportions permuted data different actually saw.can use R permute Kobe’s shot history thereby model Kobe’s shots might gone null hypothesis true. , need specify relevant variables well hypothesis. Let’s see , unpack :first second lines just like used already.third line tell R null hypothesis , namely, explanatory response variables independent (associated).Finally, fourth line generates simulated dataset randomly permuting—, shuffling—columns original data containing explanatory response variables. ’ll see function reps = 1 setting momentarily.Exercise 5.2  following chunk code simulates one way Kobe’s shots gone null hypothesis (previous shot current shot independent) true. fifth line (last line one chunks ) calculates difference proportions based simulated data.words, describe difference proportions simulated data different actual data.","code":"\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## Null Hypothesis: independence\n## # A tibble: 111 × 3\n## # Groups:   replicate [1]\n##    shot  prev_shot replicate\n##    <fct> <fct>         <int>\n##  1 M     prev_H            1\n##  2 M     prev_M            1\n##  3 H     prev_M            1\n##  4 M     prev_H            1\n##  5 M     prev_H            1\n##  6 M     prev_M            1\n##  7 H     prev_M            1\n##  8 M     prev_M            1\n##  9 M     prev_H            1\n## 10 M     prev_H            1\n## # … with 101 more rows\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))"},{"path":"lab5.html","id":"simulating-many-possible-datasets-if-the-null-were-true","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.5 Simulating many possible datasets if the null were true","text":"simulate one possible way Kobe’s shots turned null hypothesis true. many possible ways Kobe’s shots turned , corresponds different random permutation Kobe’s shot history. need keep track differences proportions get permutations. ’s good thing computers good repetitive tasks, can use computer repeat random permutation process many times simulated many possible datasets.next chunk code except changed reps generate line 1 1000. way, can simulate 1000 different datasets, different random permutation Kobe’s shot history. tell R remember name kobe_null_distribution represents distribution differences expect see null hypothesis true.first rows result look like :“replicate” column label simulated dataset, new “stat” column difference \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\) simulated dataset.can now make histogram examine distribution differences proportions result null hypothesis true:","code":"\nkobe_null_distribution <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1000, type = \"permute\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))\nkobe_null_distribution %>%\n    visualize()"},{"path":"lab5.html","id":"find-the-p-value","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.6 Find the \\(p\\) value","text":"Remember \\(p\\) value proportion simulated datasets least extreme actual data. case, means proportion simulated datasets difference proportions bigger observed. Let’s tell R remember difference running chunk code :Now, let’s see many simulated datasets bigger . following line code, tell R observed “statistic” (obs_stat) difference just told remember (kobe_obs_diff) well fact interested many simulations produced results “greater” observed:Finally, help visualize observed difference falls relative distribution differences simulated data (note uses shade_p_value rather get_p_value):red line value actually observed parts histogram shaded pink represent simulated datasets “extreme” observed.","code":"\nkobe_obs_diff <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))\nkobe_null_distribution %>%\n    get_p_value(obs_stat = kobe_obs_diff, direction = \"greater\")## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.757\nkobe_null_distribution %>%\n    visualize() +\n    shade_p_value(obs_stat = kobe_obs_diff, direction = \"greater\")"},{"path":"lab5.html","id":"form-a-conclusion","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.7 Form a conclusion","text":"Remember testing null hypothesis Kobe hot hand. reject null hypothesis data unlikely null hypothesis true, .e., \\(p\\) value low. Assume adopt significance level 0.05, reject null hypothesis \\(p\\) value less level.Exercise 5.3  reject null hypothesis Kobe’s previous shot influence current shot? Explain reasoning. conclusion say whether Kobe “hot hand”?","code":""},{"path":"lab5.html","id":"do-people-on-the-autism-spectrum-make-more-consistent-choices","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2 Do people on the autism spectrum make more consistent choices?","text":"kind hypothesis testing just fun case Kobe’s hot hand use answer serious research questions.Autism condition many facets. Individuals autism cognitive impairments often different cognitive “styles”. particular, thought people autism “detail-oriented”. can detriment trying find general pattern, might benefit situations many irrelevant distractions.potential benefit studied Farmer et al. (2017). experiment included group participants diagnosed autism spectrum (cognitive impairments) well group neuro-typical controls. looked participants’ choices pairs consumer products presented alongside third, less desirable “decoy” option. “rational” choice affected presence decoy, fact people often swayed irrelevant options. Might people autism make choices consistent—“rational”—ignore irrelevant decoy?rest lab, conduct hypothesis test address question. follow basic outline procedure followed address “hot hand” question, sure refer previous section guidance.","code":""},{"path":"lab5.html","id":"check-out-the-data-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.1 Check out the data","text":"relevant data stored R name asc_choice. ’s first rows look like.response variable Choice, either “Consistent” (participant’s choice affected decoy) “Inconsistent” (participant’s choice affected decoy).explanatory variable Group, either “ASC” (“Autism Spectrum Condition”) “NT” (“Neuro-Typical” control).","code":""},{"path":"lab5.html","id":"framing-the-hypotheses-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.2 Framing the hypotheses","text":"Exercise 5.4  research question , “proportion consistent choices higher participants ASC group NT group?” null hypothesis alternative hypothesis correspond research question?","code":""},{"path":"lab5.html","id":"summarize-the-data-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.3 Summarize the data","text":"Exercise 5.5  Find difference proportions consistent choices autism spectrum group (ASC) neuro-typical control group (NT). guidance, check Kobe example . sure note:name relevant dataset?names explanatory response variables?“Consistent” choice counts “success”.difference proportions, want look ASC minus NT.difference found consistent null hypothesis alternative hypothesis? Explain reasoning.","code":"___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))"},{"path":"lab5.html","id":"model-the-null-hypothesis","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.4 Model the null hypothesis","text":"null hypothesis true, participant’s choice shouldn’t depend group . Kobe example, modeled null hypothesis randomly permuting Kobe’s shot history, since explanatory variable. research scenario, group explanatory variable. randomly shuffling participants groups, can simulate data look null hypothesis true.Exercise 5.6  Fill blanks code simulate 1000 datasets assuming null hypothesis true, produce histogram (using visualize function) resulting simulated differences proportions. ’ll re-use lot last exercise!name relevant dataset?names explanatory response variables?“Consistent” choice counts “success”.want 1000 simulations.difference proportions, want look ASC minus NT.code working correctly, try running times.Explain words histogram get look time run code.Based just looking histogram (need calculations), describe aspects distribution seem stay seem differ time run code. Note things like shape distribution (number modes skewness), central tendency, variability.","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    visualize()"},{"path":"lab5.html","id":"find-the-p-value-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.5 Find the \\(p\\) value","text":"find \\(p\\) value, first need get observed difference proportions tell R remember label asc_obs_diff.Exercise 5.7  Fill blanks find observed difference proportions actual data tell R remember label asc_obs_diff. value used calculate \\(p\\) value. Hint 1: direction, remember correspond alternative hypothesis (“less”, “greater”, “two-sided”). Hint 2: sure reuse code previous exercises!\\(p\\) value got?","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_obs_diff <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    get_p_value(obs_stat = asc_obs_diff, direction = \"___\")"},{"path":"lab5.html","id":"form-a-conclusion-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.6 Form a conclusion","text":"Finally, position visualize observed difference proportions falls relative differences expected null hypothesis true. enable us form conclusion data tell us research question.Exercise 5.8  Fill blanks code visualize observed difference proportions falls distribution differences expected null hypothesis true (hint: look back previous exercises, ’ll able re-use lot!)Based analysis, reject null hypothesis? Explain reasoning, sure state reasonable significance level. conclusion say whether participants diagnosis autism made consistent choices neuro-typical participants?","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_obs_diff <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    visualize() +\n    shade_p_value(obs_stat = asc_obs_diff, direction = \"___\")"},{"path":"lab5.html","id":"wrap-up-4","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3 Wrap-up","text":"session, got practice using R perform hypothesis tests using randomization. Specifically, used type shuffling called permutation. Shuffling allows us simulate various ways particular dataset look like null hypothesis true. found \\(p\\) value visualized null distribution order get sense whether actual data unlikely null hypothesis true.","code":""},{"path":"lab6.html","id":"lab6","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"Lab 6 Confidence intervals with bootstrapping","text":"session, get practice using R find confidence intervals via bootstrapping. initial exposure hypothesis testing previous session, meant “first contact” basics techniques. first part, revisit Kobe data get view resampling works, since heart bootstrapping technique. second part, use bootstrapping create confidence intervals representing people’s tendencies making moral decisions.important keep mind hypothesis testing confidence intervals involve imagining things turned differently. Hypothesis testing imagining ways comparison two groups turned differently null hypothesis (group difference) true. Confidence intervals imagining ways sample look current sample representative population. hypothesis testing, use simulation help us imagine different possible outcomes. ultimate purpose constructing confidence interval simulation express values population parameter “plausible” light sample got.help later exercises session, sure download worksheet session right-clicking following link selecting “Save link …”: Worksheet Lab 6. Open saved file (default called “ws_lab06.Rmd”) RStudio.","code":""},{"path":"lab6.html","id":"what-is-kobes-field-goal-percentage","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1 What is Kobe’s field goal percentage?","text":"Last time, asked whether Kobe Bryant “hot hand” 2009 NBA finals. used randomization test null hypothesis Kobe’s chance making hit regardless whether previous shot hit .time just interested proportion Kobe’s shots actually make basket. traditional basketball term Kobe’s “field goal percentage”. constitutes population parameter label mathematically \\(\\pi_{\\text{Kobe}}\\). “population” shots Kobe ever attempted across career. sample shots Kobe attempted 2009 NBA finals. sample yields point estimate \\(\\hat{p}_{\\text{Kobe}}\\) population parameter \\(\\pi_{\\text{Kobe}}\\).","code":""},{"path":"lab6.html","id":"what-is-the-point-estimate","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.1 What is the point estimate?","text":"interested time just proportion Kobe’s shots hits rather misses. today, don’t care outcome previous shot .get point estimate proportion Kobe’s shots hits rather misses, can use following chunk code:Exercise 6.1  reminder, chunk code used find observed difference hit proportions last time, function whether Kobe’s previous shot hit :need mention prev_shot calculating proportion Kobe’s hits?need set order calculating proportion Kobe’s hits?","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.387\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))"},{"path":"lab6.html","id":"model-the-randomness","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.2 Model the randomness","text":"Although sample gives us point estimate (\\(\\hat{p}_{\\text{Kobe}}\\)) Kobe’s true field goal percentage (\\(\\pi_{\\text{Kobe}}\\)), know point estimate unlikely perfect estimate. know sample population subject sampling variability can treated effectively “random”. confidence interval based modeling sampling variability produced actual sample got, can know range values population parameter remain plausible.Bootstrapping models randomness using sample estimate population repeatedly resampling estimated population. Let’s see works.","code":""},{"path":"lab6.html","id":"a-single-resample","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.2.1 A single resample","text":"’ve seen, “resampling” treats original data whole population. sampling replacement imaginary population, can simulate new sample population might look like. following chunk code resamples Kobe’s shots produce new sample shots:Exercise 6.2  chunk code used last time generate single “shuffle” shots:words, explain need include hypothesize line bootstrapping, like using permutation test null hypothesis?final step bootstrapping calculate sample statistic—case, proportion hits—randomly generated re-sample. can adding calculate line end code used make resample:","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\")## Response: shot (factor)\n## # A tibble: 111 × 2\n## # Groups:   replicate [1]\n##    replicate shot \n##        <int> <fct>\n##  1         1 M    \n##  2         1 M    \n##  3         1 H    \n##  4         1 M    \n##  5         1 M    \n##  6         1 M    \n##  7         1 H    \n##  8         1 M    \n##  9         1 M    \n## 10         1 M    \n## # … with 101 more rows\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.342"},{"path":"lab6.html","id":"many-resamples","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.2.2 Many resamples","text":"whole point bootstrapping get range plausible values need one resample! like last time, want get R remember proportions resample can use later; call kobe_boot_dist.following chunk code uses bootstrapping produce distribution sample proportions, variability mimics sampling variability work generating original observed sample:like last time, can use histogram get nice visual summary bootstrap distribution:","code":"\nkobe_boot_dist <- kobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")\nkobe_boot_dist %>%\n    visualize()"},{"path":"lab6.html","id":"create-the-interval","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.3 Create the interval","text":"find confidence interval, need find “middle” bootstrap distribution. also need decide wide “middle” . example, create 95% confidence interval, need find middle 95% distribution .middle 95% two different quantiles bootstrap distribution. Specifically, 2.5% 97.5% quantiles define boundaries 95% confidence interval. values 2.5% simulated proportions interval 2.5% simulated proportions interval. , total, 5% values outside interval meaning remainder (95%) inside .many things, R shortcuts! following code finds 95% confidence interval, use level set wide want interval. Note putting result label kobe_boot_ci can use help us visualize interval:Now can add 95% confidence interval saved kobe_boot_ci visualization:","code":"\nkobe_boot_ci <- kobe_boot_dist %>%\n    get_confidence_interval(level = 0.95)\nkobe_boot_dist %>%\n    visualize() +\n    shade_confidence_interval(endpoints = kobe_boot_ci)"},{"path":"lab6.html","id":"form-a-conclusion-2","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1.4 Form a conclusion","text":"Now position form conclusion Kobe’s true field goal percentage based sample .Exercise 6.3  Answer following based 95% confidence interval just constructed Kobe’s field goal percentage. (Hint: visualization alone enough answer questions.)plausible Kobe makes half shots attempts? Explain reasoning.Kobe’s actual career field goal percentage 44.7%. words, true value population parameter \\(\\pi_{\\text{Kobe}} = 0.447\\). Based 95% CI, surprised ? Explain reasoning.","code":""},{"path":"lab6.html","id":"how-willing-are-people-to-obey-commands-that-endanger-someones-life","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2 How willing are people to obey commands that endanger someone’s life?","text":"Stanley Milgram conducted now ()famous experiment 1963 nature obedience. experiment part line research motivated experiences Holocaust. Holocaust, began 1930s continued end second world war 1945, Nazi Germany allies engaged systematic extermination millions people variety ethnic, cultural, sexual minorities. vast bureaucracy set coordinate murder massive scale. Milgram (1963) said,Gas chambers built, death camps guarded, daily quotas corpses produced efficiency manufacture appliances. inhumane policies may originated mind single person, carried massive scale large number persons obeyed orders.question laid heavy Milgram’s mind, well contemporaries, many otherwise ordinary people participate murdering people , circumstances, might neighbors. Milgram’s experiment focused one potential explanation: people may strong tendency obey authority figures, even violates personal morality.Milgram (1963) studied using experiment participant, calls “naive subject” (“naive” means “aware purpose experiment”) ordered “administer electric shock victim.” describes experimental setup:simulated shock generator used, 30 clearly marked voltage levels range 15 450 volts. instrument bears verbal designations range Slight Shock Danger: Severe Shock. responses victim, trained confederate experimenter, standardized. orders administer shocks given naive subject context “learning experiment” ostensibly set study effects punishment memory. experiment proceeds naive subject commanded administer increasingly intense shocks victim, even point reaching level marked Danger: Severe Shock.setup experiment shown title image today’s session, “T” participant, “E” Experimenter, “L” “victim” supposedly shocked.session, examine data one Milgram’s original experiments well recent replication experiment. cases, response variable whether participant continued obey commands shock “victim” past “danger” mark.","code":""},{"path":"lab6.html","id":"milgrams-original-study","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.1 Milgram’s original study","text":"data Milgram’s original study loaded R’s environment name milgram.Exercise 6.4  Use chunk code generate 1000 simulated datasets based Milgram’s study visualize 95% confidence interval.Based confidence interval found (can see exact numbers click milgram_ci R’s Environment window running code), can say proportion people general willing risk someone’s life obedience?","code":"milgram_boot_dist <- ___ %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nmilgram_ci <- milgram_boot_dist %>%\n    get_confidence_interval(level = ___)\n\nmilgram_boot_dist %>%\n    visualize() +\n    shade_confidence_interval(endpoints = milgram_ci)"},{"path":"lab6.html","id":"a-more-recent-replication","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.2 A more recent replication","text":"reasonable believe societal attitudes toward authority changed since 1960’s. One reason shift attitudes people became aware Milgram’s work. , interesting replicate Milgram’s experiment see whether shift attitudes reduced proportion people continue obey.participants Milgram’s experiments—.e., people commanded issue shocks, actor played “victim”—experienced considerable stress. example, Milgram (1963) says , “[f]ull-blown, uncontrollable seizures observed 3 subjects. one occasion observed seizure violently convulsive necessary call halt experiment.” Despite horrible reactions, participants still obeyed command increase level shock.horrific effects participants, impossible replicate Milgram’s procedure exactly ethical way. Burger (2009) got close one get. First, Burger screened participants beforehand ensure risk factors lead kind extreme reactions described . Second, although Burger’s fake electric shock machine looked Milgram’s, experiment stop reaching dangerous setting. Even participant obeyed command increase voltage past certain point, experiment stop “shock” applied. result, participants never thought inflicting mild discomfort. Even , Burger’s experiment allows us measure whether someone willing continue shocks.data Burger’s replication loaded R name milgram_replication.Exercise 6.5  Fill blanks following chunk calculate proportion participants Burger’s replication continue obey experimenter.proportion fall inside outside 95% confidence interval found Milgram’s original experiment (previous exercise)?answer part [] suggest whether attitudes toward obedience changed 1963 2009? Explain reasoning.Exercise 6.6  Fill blanks chunk code repeat analysis performed Milgram’s original data (called milgram) recent replication (called milgram_replication). Hint: usual, able re-use lot code!95% confidence interval found? (can see raw numbers clicking milgram_replication_ci running code.)Milgram described sample: “subjects 40 males ages 20 50, drawn New Haven [Connecticut] surrounding communities…subjects postal clerks, high school teachers, salesmen, engineers, laborers. Subjects ranged educational level one finished elementary school, doctorate professional degrees.” Burger’s sample also includes range educational experiences, comprised 18 men 22 women range age 20 81 years old. think 95% CI Milgram’s original study one Burger’s replication gives better description true population proportion probably ? Explain reasoning.","code":"milgram_replication %>%\n    specify(response = ___, success = \"___\") %>%\n    calculate(stat = \"___\")milgram_replication_boot_dist <- ___ %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nmilgram_replication_ci <- milgram_replication_boot_dist %>%\n    get_confidence_interval(level = ___)\n\nmilgram_replication_boot_dist %>%\n    visualize() +\n    shade_confidence_interval(endpoints = milgram_replication_ci)"},{"path":"lab6.html","id":"do-men-and-women-differ-in-their-proportions-of-obedience","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3 Do men and women differ in their proportions of obedience?","text":"major difference Milgram’s original study Burger’s replication Burger’s sample contains men women. allows us address research question: men women differ tendency toward obedience situation?Exercise 6.7  Fill blanks code use random permutation test null hypothesis men women differ proportion obey.specify lines: response variable still obeyed, now explanatory variable labeled gender.blanks hypothesize, generate, calculate lines: look set previous activity.direction lines: three possible settings: direction = \"less\", direction = \"greater\", direction = \"two-sided\". Set direction results considered “extreme” null hypothesis true.Based results obtained, reject null hypothesis? (can check \\(p\\) value clicking milgram_replication_p_value R’s Environment window running code.) result hypothesis test say whether men women differ tendency obey commands potentially endanger someone’s life?","code":"milgram_replication_null_dist <- milgram_replication %>%\n    specify(___ ~ ___, success = \"Yes\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"Male\", \"Female\"))\n\nmilgram_replication_obs_diff <- milgram_replication %>%\n    specify(___ ~ ___, success = \"Yes\") %>%\n    calculate(stat = \"___\", order = c(\"Male\", \"Female\"))\n\nmilgram_replication_p_value <- milgram_replication_null_dist %>%\n    get_p_value(obs_stat = milgram_replication_obs_diff, direction = \"___\")\n\nmilgram_replication_null_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = milgram_replication_obs_diff, direction = \"___\")"},{"path":"lab6.html","id":"wrap-up-5","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3 Wrap-up","text":"session, got practice using R construct confidence intervals using bootstrapping. used R generate many “resampled” dataset simulate kinds samples seen due sampling variability. allows us say values population parameter plausible given sample population.","code":""},{"path":"lab7.html","id":"lab7","chapter":"Lab 7 The normal distribution","heading":"Lab 7 The normal distribution","text":"venerable normal distribution “bell curve” almost like mascot statistics. Although variables real life distributed according normal distribution, real value describing sampling distributions. seen sampling distributions hypothesis testing confidence intervals: Sampling distributions represent variability point estimate like proportion mean due randomness involved selecting samples population. According central limit theorem, much time, sampling distributions approximately normal shape.session, first use normal distribution model population distributions, distribution values particular variable across whole population. Along way, get hang using R find proportions intervals based normal distribution. , second part, use normal distribution model sampling distributions. give us insight standard error relates things like sample size.help later exercises session, sure download worksheet session right-clicking following link selecting “Save link …”: Worksheet Lab 7. Open saved file (default called “ws_lab07.Rmd”) RStudio.","code":""},{"path":"lab7.html","id":"the-data-national-health-and-nutrition-examination-surveys-nhanes","chapter":"Lab 7 The normal distribution","heading":"7.1 The data: National Health and Nutrition Examination Surveys (NHANES)","text":"data using lab—labeled nhanes R environment—originally collected US National Center Heath Statistics 2009 2012. subset effectively simple random sample entire US population, though use observations many collected.","code":""},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-population-distribution","chapter":"Lab 7 The normal distribution","heading":"7.2 The normal distribution as a model for a population distribution","text":"variable focus first number hours people report sleeping night, measured hours. recorded individual sample variable SleepHrsNight.","code":""},{"path":"lab7.html","id":"visualize-the-distribution","chapter":"Lab 7 The normal distribution","heading":"7.2.1 Visualize the distribution","text":"usual, begin visualizing actual data. reminder, assume time data constitute entire population interest. histogram shows population distribution nightly hours sleep:","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(binwidth = 1)"},{"path":"lab7.html","id":"is-the-normal-distribution-a-good-model","chapter":"Lab 7 The normal distribution","heading":"7.2.2 Is the normal distribution a good model?","text":"Next, want know whether normal distribution good model population distribution just visualized. , draw curve representing normal distribution top histogram just made.remember normal distribution just shape. center spread determined two population parameters: mean (\\(\\mu\\)) standard deviation (\\(\\sigma\\)). parameters ?Let’s first take wild guess say \\(\\mu = 8\\) \\(\\sigma = 1\\). can draw distribution using chunk code :Notice needed add aes(y = ..density..) histogram line, reasons clear next paragraph. final line lets us draw function graph. function called dnorm “density normal distribution”. say arguments function , list say mean sd normal distribution .whole “density” thing comes : histogram shows absolute counts. normal distribution specifies relative frequency different values. relative frequency called “density”. dnorm function gives us normal distribution. adding aes(y = ..density..) histogram line told R show density (relative frequency) rather absolute counts.Anyway, clear mean 8 hours standard deviation 1 hour make good fit.Exercise 7.1  Play around different values “mean” “sd” chunk code . Try find values result normal distribution good “fit” data. can start setting mean = 8 sd = 1 adjust .Describe strategy used find values mean sd seemed make good fit.values find?Try setting mean = 6.86 sd = 1.32 run chunk. close actual mean standard deviation data sample. Compared settings mean sd found, think make better worse “fit”?","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = 8, sd = 1), color = 'darkred')nhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"bad-days-bad-models","chapter":"Lab 7 The normal distribution","heading":"7.2.3 Bad days, bad models?","text":"part survey, people asked many days last 30 consider mental health poor. recorded variable named DaysMentHlthBad.Exercise 7.2  Use chunk code help make histogram distribution bad mental health days per month (remember recorded nhanes dataset variable named DaysMentHlthBad; can also try different bin widths like).think normal distribution able fit data? ?Make reasonable guess population distribution might shape , taking note fact respondents give number days 30.","code":"___ %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 1)"},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-sampling-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3 The normal distribution as a model for a sampling distribution","text":"previous section, got sense can sometimes use normal distribution approximate distribution observed values population. often, normal distribution used model sampling distribution.section, assume nhanes dataset represents entire population interest. simulate drawing many different samples different sizes population calculate summary statistic sample. sampling distribution distribution summary statistics see well can approximate normal distribution.focus just one questions asked NHANES survey. question asks whether someone ever tried using marijuana. information resides variable Marijuana person’s response either “Yes” “”.Exercise 7.3  Without even looking data, make sense try use normal distribution approximate population distribution responses Marijuana question? ?","code":""},{"path":"lab7.html","id":"the-true-proportion","chapter":"Lab 7 The normal distribution","heading":"7.3.1 The “true” proportion","text":"assuming nhanes data represent entire population interested , can find “true” value population parameter \\(\\pi_{\\text{Marijuana}}\\), , proportion people ever tried marijuana. can use code :","code":"\nnhanes %>%\n    specify(response = Marijuana, success = \"Yes\") %>%\n    calculate(stat = \"prop\")## Response: Marijuana (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.585"},{"path":"lab7.html","id":"simulating-many-samples","chapter":"Lab 7 The normal distribution","heading":"7.3.2 Simulating many samples","text":"Last session, used bootstrapping simulate happen collected many samples population. using sample estimate whole population. Now, access whole population. can therefore draw many samples given size want!following chunk code randomly samples 5 people population gets R remember sample name sample_size5.sample looks like:Notice still people’s responses question NHANES survey. means can get summary statistic Marijuana variable sample, just like population.Exercise 7.4  Fill blanks chunk code draw sample size 5 NHANES population get proportion people sample used marijuana. Hint: see calculated proportion whole NHANES population, remember now calculating new sample_size5.proportion people tried marijuana sample?Try running chunk code times. Explain words calculated proportion always .’ve seen, great thing computers can boring repetitive things us quickly. Now, draw 1000 samples size 5 population get R remember name samples_size5.first three samples new samples_size5:","code":"\nsample_size5 <- nhanes %>%\n    slice_sample(n = 5)sample_size5 <- nhanes %>%\n    slice_sample(n = 5)\n\n___ %>%\n    specify(response = ___, success = \"___\") %>%\n    calculate(stat = \"___\")\nsamples_size5 <- nhanes %>%\n    rep_slice_sample(n = 5, reps = 1000)"},{"path":"lab7.html","id":"summary-statistics-for-each-sample","chapter":"Lab 7 The normal distribution","heading":"7.3.3 Summary statistics for each sample","text":"Notice different samples indexed using new variable replicate. can use get proportion marijuana triers sample using old group_by routine. tell R remember sample proportions name sample_props_mari_size5:Notice proportions calculated sample tend vary. can visualize using histogram:","code":"\nsample_props_mari_size5 <- samples_size5 %>%\n    group_by(replicate) %>%\n    summarize(stat = mean(Marijuana == \"Yes\"))\nsample_props_mari_size5 %>%\n    ggplot(aes(x = stat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2)"},{"path":"lab7.html","id":"approximating-with-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.4 Approximating with a normal distribution","text":"see whether can approximate sampling distribution normal distribution, need find mean standard deviation. Recall, standard deviation sampling distribution special name: standard error (SE).Exercise 7.5  First, run chunk code ’ll sampling distribution work (just ran prior exercise):see sample_props_mari_size5 R environment. contains 1000 proportions 1000 samples size 5 NHANES population.Now, just like hours sleep, play around different mean sd settings final line following chunk find normal distribution seems good “fit” distribution sample proportions.values mean sd seemed best ?seem like normal distribution good “fit” sampling distribution? (Hint: happens tails normal distribution try fit sampling distribution?)","code":"\nsamples_size5 <- nhanes %>%\n    rep_slice_sample(n = 5, reps = 1000)\n\nsample_props_mari_size5 <- samples_size5 %>%\n    group_by(replicate) %>%\n    summarize(stat = mean(Marijuana == \"Yes\"))sample_props_mari_size5 %>%\n    ggplot(aes(x = stat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"larger-samples-and-the-central-limit-theorem","chapter":"Lab 7 The normal distribution","heading":"7.3.5 Larger samples and the central limit theorem","text":"Recall central limit theorem says normal distribution probably good model samples go sampling distribution sufficiently large (observation sample independent others, guaranteed random sampling). , mean sampling distribution close population proportion \\(\\pi\\) standard error close \\[\nSE = \\sqrt{\\frac{\\pi \\left(1 - \\pi \\right)}{n}}\n\\]Let’s see happens use fit normal distribution sampling distribution. Remember found population proportion earlier (\\(0.585\\)).Exercise 7.6  Now, run following chunk code generate 1000 samples size 50, instead size 5:now see sample_props_mari_size50 R environment.standard error according central limit theorem (remember population proportion \\(\\pi = 0.585\\) samples size \\(n = 50\\))?chunk code , set mean population proportion (0.585) sd standard error found part []. Compare fit normal distribution exercise one previous exercise—normal distribution make better “fit” use bigger samples?standard error get bigger, smaller, stay used bigger samples?mean sampling distribution get bigger, smaller, stay used bigger samples?","code":"\nsamples_size50 <- nhanes %>%\n    rep_sample_n(size = 50, reps = 1000)\n\nsample_props_mari_size50 <- samples_size50 %>%\n    group_by(replicate) %>%\n    summarize(stat = mean(Marijuana == \"Yes\"))sample_props_mari_size50 %>%\n    ggplot(aes(x = stat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.02) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"what-if-we-dont-know-the-true-population","chapter":"Lab 7 The normal distribution","heading":"7.3.6 What if we don’t know the “true” population?","text":"Unlike previous example, practice usually single sample don’t know true value population parameter . can still use sample learn something population general. ’ve seen use bootstrapping construct 95% confidence interval population proportion based sample. Let’s normal distribution., following:Draw random sample size 50 complete NHANES dataset.Use R calculate proportion people tried marijuana; sample statistic \\(\\hat{p}\\) p_hat.Use \\(\\hat{p}\\) sample calculate standard error according formula :\n\\[\nSE = \\sqrt{\\frac{\\hat{p} \\left(1 - \\hat{p} \\right)}{n}}\n\\]Make 95% confidence interval using knowledge 95% normal distribution within 1.96 standard deviations mean; 95% CI \\(\\left( \\hat{p} - 1.96 \\times SE, \\hat{p} + 1.96 \\times SE \\right)\\).Exercise 7.7  Run following chunk code draw single random sample 50 people NHANES dataset, calculate sample proportion save result name p_hat.Fill blanks following chunk help calculate standard error based p_hat sample. Hint: translating formula step 3 R code; two blanks filled name value just saved.Now use following chunk help find 95% confidence interval (, translating formula step 4 R, ’ll able use names values ’ve already saved).report 95% confidence interval found real sample (.e., didn’t know true value )?interval contain true population parameter (0.585)? words, explain interval might might contained true value.","code":"\nmy_sample <- nhanes %>%\n    slice_sample(n = 50)\n\np_hat <- my_sample %>%\n    specify(response = Marijuana, success = \"Yes\") %>%\n    calculate(stat = \"prop\") %>%\n    pull()standard_error <- sqrt((___ * (1 - ___)) / ___)c(___ - 1.96 * ___, ___ + 1.96 * ___)"},{"path":"lab7.html","id":"wrap-up-6","chapter":"Lab 7 The normal distribution","heading":"7.4 Wrap-up","text":"session, saw can use normal distribution model either population distribution sampling distribution. Sometimes normal distribution fits well, sometimes , important us check whether provides reasonable approximation . saw use normal distribution find intervals proportions. using normal distribution model sampling distribution, standard deviation (spread) called “standard error” close relationship sample size.","code":""},{"path":"lab8.html","id":"lab8","chapter":"Lab 8 Inference for proportions","heading":"Lab 8 Inference for proportions","text":"session, collect apply techniques encountered inferential statistics proportions:Hypothesis testing deciding whether two proportions significantly different one another.Confidence intervals representing values population proportion plausible based sample.Using normal distribution mathematical model sampling distribution.Many exercises activity may require look back previous sessions hints guidance apply techniques address important questions eyewitness memory.help later exercises session, sure download worksheet session right-clicking following link selecting “Save link …”: Worksheet Lab 8. Open saved file (default called “ws_lab08.Rmd”) RStudio.","code":""},{"path":"lab8.html","id":"get-to-know-the-data","chapter":"Lab 8 Inference for proportions","heading":"8.1 Get to know the data","text":"Eyewitness testimony important tool establishing whether suspect may may committed crime. eyewitness identifies suspect person saw scene crime, usually taken strong evidence suspect’s potential culpability. identifications usually done within lineup, eyewitness shown suspect among several non-suspects known “fillers”. witness identifies suspect person saw, information can used investigate person provide evidence court.Despite importance eyewitness testimony, human memory far perfect. can difficult people () remember details person’s face midst stressful event like experiencing crime; (b) identify whether suspect matches remember even people encountered different settings (e.g., different light conditions, different clothes, different facial hair/makeup, etc.). difficulties, eyewitnesses sometimes make false identifications. Moreover, subtle details police present lineup witness may influence accurately can pick suspect. result, considerable interest understanding accurate eyewitness testimony best conduct police lineups minimize false identifications.data session collected part long-term study use police lineups (Wixted et al., 2016). study conducted field 2013, Robbery Division Houston (Texas) Police Department. participants eyewitnesses various robbery cases. case, investigating officer constructed lineup consisting six photographs. One photograph suspect case, remaining five fillers, defined non-suspects still match physical description suspect. lineups shown either simultaneously (six photos ) sequentially (one photo time).Examples simultaneous sequential photo lineups (Gepshtein et al., 2020).important note investigators showed lineups eyewitnesses blind person lineup suspect. result, investigators able systematically bias identifications witnesses made.first rows data, give sense looks like:row represents response single witness single six-person photo lineup. just two variables:presentation: Whether six photos presented witness “Sequential” “Simultaneous” format.ID: Whether witness identified “Suspect” person saw, whether identified one “Filler” people instead.","code":""},{"path":"lab8.html","id":"confidence-interval-bootstrapping-how-accurate-are-eyewitnesses","chapter":"Lab 8 Inference for proportions","heading":"8.2 Confidence interval (bootstrapping): How accurate are eyewitnesses?","text":"important thing keep mind even witness identified suspect lineup, necessarily mean suspect guilty even present scene crime. hand, identifying one fillers definitely mistake, since people known nothing crime investigated.Exercise 8.1  first task analyzing data estimate proportion suspect ID’s lineups. moment, ignore whether lineup presented simultaneously sequentially. aim build 95% confidence interval describes often, average, witnesses identify suspect lineup.Use following chunk code calculate store proportion suspect ID’s lineup data. Hint: look back wrote specify calculate lines Kobe’s Field Goal Percentage, keep mind focused proportion suspect ID’s, proportion filler ID’s.observed proportion suspect ID’s?Use following chunk code use bootstrapping build sampling distribution proportion suspect ID’s. use distribution find visualize 95% confidence interval. sure generate least 1000 simulated datasets.Report interpret 95% confidence interval found (stored boot_prop_id_ci Environment pane). think interval tells us accuracy eyewitnesses general?","code":"lineup %>%\n    specify(___) %>%\n    calculate(___)boot_prop_id_dist <- lineup %>%\n    specify(___) %>%\n    generate(___) %>%\n    calculate(___)\n\nboot_prop_id_ci <- boot_prop_id_dist %>%\n    get_confidence_interval(level = 0.95)\n\nboot_prop_id_dist %>%\n    visualize() +\n    shade_confidence_interval(boot_prop_id_ci)"},{"path":"lab8.html","id":"hypothesis-test-1-normal-distribution-are-eyewitnesses-better-than-chance","chapter":"Lab 8 Inference for proportions","heading":"8.3 Hypothesis test 1 (normal distribution): Are eyewitnesses better than chance?","text":"previous exercise, constructed 95% confidence interval proportion suspect ID’s photo lineups. important consider whether proportion higher expected chance. eyewitnesses merely picking person random lineup, still pick suspect time just chance. can frame research question can tackled hypothesis test.Exercise 8.2  research question , “proportion suspect ID’s greater expected eyewitnesses picking randomly?”eyewitnesses picking randomly, proportion time expect pick suspect lineup? (Recall lineup six photographs.)null alternative hypotheses corresponding research question?Use chunk code help use normal distribution model sampling distribution assuming null hypothesis true. first blank, answer part []. second blank, see many rows lineup dataset —sample size. third blank, answer part [] previous exercise. final result “Z score”.Z score found? words, describe Z score tells us “extreme” observed proportion relative expected null hypothesis true.Run following chunk find p value (note need completed part [c] work!):Based p value, reject null hypothesis? say whether eyewitnesses might picking suspect just random?","code":"null_prop_id <- ___\nsample_size <- ___\nobs_prop_id <- ___\n\nnull_se <- sqrt(null_prop_id * (1 - null_prop_id) / sample_size)\n\nz_score <- (obs_prop_id - null_prop_id) / null_se\npnorm(z_score, lower.tail = FALSE)"},{"path":"lab8.html","id":"hypothesis-test-2-permutation-is-there-a-difference-between-sequential-and-simultaneous-lineups","chapter":"Lab 8 Inference for proportions","heading":"8.4 Hypothesis test 2 (permutation): Is there a difference between sequential and simultaneous lineups?","text":"Recall lineups study presented one two ways: either simultaneously sequentially. Laboratory studies eyewitness memory (using mock crimes instead real ones) found simultaneous lineups tend produce “correct” identifications suspect. true real lineups analyzing?Exercise 8.3  exercise, research question , “difference proportion suspect ID’s sequential simultaneous lineups?” address question using random permutation conduct hypothesis test.null alternative hypotheses corresponding research question?Use following chunk code calculate observed difference proportion suspect ID’s sequential simultaneous lineups:observed difference proportions (stored obs_diff run code)? Based result, type presentation might produce higher proportion suspect ID’s?Use following chunk code use random permutation simulate least 1000 datasets assuming null hypothesis true, visualize observed difference falls relative distribution. Remember put one “greater”, “less” “two-sided” direction setting last line; appropriate setting depends alternative hypothesis (.e., answer part []). Also, make sure ’ve completed part [b], since code needs obs_diff run!Use following chunk find p value hypothesis test (make sure completed parts [b] [c], since code depends completed successfully):p value got? Based p value, reject null hypothesis? conclusion tell us whether simultaneous sequential presentation differ much produce suspect ID’s?","code":"obs_diff <- lineup %>%\n    specify(___) %>%\n    calculate(___, order = c(\"Sequential\", \"Simultaneous\"))null_diff_dist <- lineup %>%\n    specify(___) %>%\n    hypothesize(___) %>%\n    generate(___) %>%\n    calculate(___, order = c(\"Sequential\", \"Simultaneous\"))\n\nnull_diff_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = ___)null_diff_dist %>%\n    get_p_value(obs_stat = obs_diff, direction = ___)"},{"path":"lab8.html","id":"hypothesis-test-3-permutation-does-it-matter-if-police-know-who-the-suspect-is","chapter":"Lab 8 Inference for proportions","heading":"8.5 Hypothesis test 3 (permutation): Does it matter if police know who the suspect is?","text":"Even though lineups conducted field, conducted well-controlled setting police investigator assembled presented lineup working case. result, investigator “blind” sense know pictures suspect. make sure investigator accidentally intentionally bias witness toward identifying police’s suspect.investigator prepared lineup involved case therefore knew suspect ? photos still shuffled prior presenting witness, investigator still wouldn’t know position suspect . simultaneous presentation, suspect appear location sequential presentation, suspect appear spot order. Even , investigator working case, might choose filler photographs way make suspect “stand ”, biasing witness pick person.study much matters, authors study prepared set “mock” lineups. cases, real photo lineups presented people witnesses crime. result, people guess suspect , since information crime. authors study compared lineups prepared either investigating officer (“blind”) officer unfamiliar investigation (“Blind”—type lineup analyzed ).first rows resulting data look like, stored name fairness:two variables:condition: Whether lineup “Blind” “blind”ID: Whether mock witness picked “Suspect” “Filler”.Exercise 8.4  research question , “mock witnesses make higher proportion suspect ID’s -blind lineups opposed blind lineups?” address question using random permutation conduct hypothesis test.null alternative hypotheses corresponding research question?Use following chunk code calculate observed difference proportion suspect ID’s sequential simultaneous lineups:observed difference proportions (stored obs_fairness_diff run code)? result consistent null hypothesis alternative hypothesis?Use following chunk code use random permutation simulate least 1000 datasets assuming null hypothesis true, visualize observed difference falls relative distribution. Remember put one “greater”, “less” “two-sided” direction setting last line; appropriate setting depends alternative hypothesis (.e., answer part []). Also, make sure ’ve completed part [b], since code needs obs_fairness_diff run!Use following chunk find p value hypothesis test (make sure completed parts [b] [c], since code depends completed successfully):p value got? Based p value, reject null hypothesis? conclusion tell us whether non-blind police lineups “fair” sense bias witness toward particular suspect?","code":"obs_fairness_diff <- fairness %>%\n    specify(___) %>%\n    calculate(___, order = c(\"Not blind\", \"Blind\"))null_fairness_diff_dist <- fairness %>%\n    specify(___) %>%\n    hypothesize(___) %>%\n    generate(___) %>%\n    calculate(___, order = c(\"Not blind\", \"Blind\"))\n\nnull_fairness_diff_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_fairness_diff, direction = ___)null_fairness_diff_dist %>%\n    get_p_value(obs_stat = obs_fairness_diff, direction = ___)"},{"path":"lab8.html","id":"wrap-up-7","chapter":"Lab 8 Inference for proportions","heading":"8.6 Wrap-up","text":"session, revisited many core concepts statistical inference. used simulation normal distribution find confidence intervals individual proportions conduct hypothesis tests comparing proportions.","code":""},{"path":"lab9.html","id":"lab9","chapter":"Lab 9 Inference about a mean","heading":"Lab 9 Inference about a mean","text":"session, learn inference response variable numerical rather categorical. Specifically, focus can use data draw inferences population average might , based sample. similar drawing inferences population proportion. shall see, simulation methods like bootstrapping work just numerical response variable categorical response variable. differ mathematical model proportions normal distribution whereas mathematical model means T distribution.methods also critical estimating testing differences within pair measurements. observation paired another, end just analyzing single number case, difference within pair.help later exercises session, sure download worksheet session right-clicking following link selecting “Save link …”: Worksheet Lab 9. Open saved file (default called “ws_lab09.Rmd”) RStudio.","code":""},{"path":"lab9.html","id":"overworked","chapter":"Lab 9 Inference about a mean","heading":"9.1 Overworked?","text":"begin, first address questions number hours people work per week. relevant data collected part General Social Survey (GSS) 2010. first rows survey results look like :number interesting variables , one interested called HoursWorked. number hours person survey reports working per week.Exercise 9.1  Use following chunk code make histogram number hours worked per week, based GSS data. R, dataset called gss number hours worked per week recorded variable HoursWorked. Play around different binwidths find one seems give good sense shape data.Describe shape distribution HoursWorked, noting whether skewed symmetrical, many modes , whether may outliers.data appear satisfy “normality” condition whether can use mathematical model?","code":"___ %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = ___)"},{"path":"lab9.html","id":"confidence-interval-via-bootstrapping","chapter":"Lab 9 Inference about a mean","heading":"9.1.1 Confidence interval via bootstrapping","text":"’ve noted, can still use bootstrapping construct sampling distribution. now, instead sampling distribution proportion, build sampling distribution mean.Exercise 9.2  Use following chunk code use bootstrapping build sampling distribution mean number hours worked per week, using gss data. Set level give us 95% confidence interval sure generate least 1000 repetitions. Hint: look lot like working proportions, one important difference—instead “prop” statistic calculate simulated dataset, calculate mean.Report 95% confidence interval found interpret context.Based results, plausible people average work around 40 hours per week?words, describe difference histogram made previous exercise one made exercise. histograms show distribution numbers, numbers mean different things.","code":"hours_boot_dist <- ___ %>%\n    specify(response = ___) %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nhours_boot_ci <- hours_boot_dist %>%\n    get_confidence_interval(level = ___)\n\nhours_boot_dist %>%\n    visualize() +\n    shade_confidence_interval(hours_boot_ci)"},{"path":"lab9.html","id":"confidence-interval-via-mathematical-model","chapter":"Lab 9 Inference about a mean","heading":"9.1.2 Confidence interval via mathematical model","text":"addition bootstrapping, conditions met, can also use mathematical model sampling distribution. model T distribution. use , need know standard error mean, can calculate using formula:\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\]\\(s\\) sample standard deviation \\(n\\) sample size.standard error, use construct confidence interval using formula:\\[\n\\bar{x} \\pm t^{\\star}_{df} \\times SE\n\\]Remember \\(t^{\\star}_{df} \\times SE\\) “margin error” represents many standard error’s mean (\\(\\bar{x}\\)) believe true population parameter might .need T distribution find \\(t^{\\star}_{df}\\). ’ve noted, T distribution different shape depending many degrees freedom . number degrees freedom one less sample size:\\[\ndf = n - 1\n\\]Exercise 9.3  Fill blanks code use R carry calculations needed use T distribution find 95% confidence interval. exercise translating mathematical formulas meaningful labels. Notice first part chunk finds stores sample_mean, sample_sd (sample standard deviation), sample_size. Use labels fill appropriate blanks. also need supply number confidence_level (0 1, like previous exercise).95% confidence interval found?Compare 95% confidence interval found exercise one found previous exercise using bootstrapping. similar? might expect least little different?","code":"sample_mean <- gss %>%\n    summarize(mean(HoursWorked)) %>%\n    pull()\n\nsample_sd <- gss %>%\n    summarize(sd(HoursWorked)) %>%\n    pull()\n\nsample_size <- gss %>%\n    summarize(n()) %>%\n    pull()\n\nstandard_error <- ___ / sqrt(___)\n\ndegrees_of_freedom <- ___ - 1\n\nconfidence_level <- ___\n\ntail_probability <- 1 - (1 - confidence_level) / 2\n\ncritical_t <- qt(p = tail_probability, df = degrees_of_freedom)\n\nc(___ - critical_t * standard_error, ___ + critical_t * standard_error)"},{"path":"lab9.html","id":"hypothesis-test-via-mathematical-model","chapter":"Lab 9 Inference about a mean","heading":"9.1.3 Hypothesis test via mathematical model","text":"Finally, can use T distribution conduct hypothesis test. , calculate T score represents far observed result expect null hypothesis true. can use T distribution figure \\(p\\) value, .e., probability observing T score least extreme one observed.Just like Z score, T score defined difference point estimate hypothesized value, relative standard error:\\[\n\\begin{align*}\nT & = \\frac{\\text{point estimate} - \\text{null value}}{\\text{standard error}} \\\\\n& = \\frac{\\bar{x} - \\mu_0}{SE}\n\\end{align*}\n\\]Exercise 9.4  research question , “people average work 40 hours per week?” Fill blanks code find T score. final line calculate \\(p\\) value using T distribution. Hint: Like last exercise, use names stored values (like sample_size) fill blank, exception null_value need specify based research question.Based p value got, reject null hypothesis? conclude ?","code":"sample_mean <- gss %>%\n    summarize(mean(HoursWorked)) %>%\n    pull()\n\nsample_sd <- gss %>%\n    summarize(sd(HoursWorked)) %>%\n    pull()\n\nsample_size <- gss %>%\n    summarize(n()) %>%\n    pull()\n\nnull_value <- ___\n\nstandard_error <- ___ / sqrt(___)\n\ndegrees_of_freedom <- ___ - 1\n\nt_score <- (___ - ___) / ___\n\npt(q = t_score, df = ___, lower.tail = FALSE)"},{"path":"lab9.html","id":"do-infants-prefer-people-who-sing-familiar-songs","chapter":"Lab 9 Inference about a mean","heading":"9.2 Do infants prefer people who sing familiar songs?","text":"Sam Mehr, Lee Ann Song (aptly named), Liz Spelke (Mehr et al., 2016) interested infants use music social cue. infant sees someone first time, likely attracted person sing song infant already knows?Mehr et al. (2016) conducted experiment child’s parents taught new melody instructed sing child home course 1–2 weeks. exposure period, parents brought infant back lab. infant seated front screen showing videos two adults infant never seen . first, two adults just smiled silence. recorded proportion time infant looked stranger “” phase. , one people sang melody parents singing 1–2 weeks, sang totally new song. Finally, “” phase, infant saw videos person silently smiling researchers recorded proportion time spent looking person sang familiar song. infants tend look stranger sang familiar song?","code":""},{"path":"lab9.html","id":"check-out-the-data-2","chapter":"Lab 9 Inference about a mean","heading":"9.2.1 Check out the data","text":"First, let’s take look data, saved R name lullaby. first rows:row data specific infant. three variables dataset:“id”: number identifying infant study.“”: phase infants heard anyone sing, proportion time look person eventually sing familiar melody?“”: phase infants heard two people sing, proportion time look person sang familiar melody?Exercise 9.5  Answer following questions based understanding study designed:infant showed preference either phase, proportion time phase spend looking person sang familiar melody?infant change looked hearing person sing, difference looking proportions phase?","code":""},{"path":"lab9.html","id":"examine-the-distribution-of-differences","chapter":"Lab 9 Inference about a mean","heading":"9.2.2 Examine the distribution of differences","text":"research question whether hearing two people sing affected infant’s viewing preferences. result, interested difference . find infant, use R’s mutate function:Exercise 9.6  Run chunk code examine result:words, describe line code mutate(diff = - ) .Refer description study given . positive value diff say infant’s looking preferences changed? negative value diff say preferences changed?nice thing mutate line can add code uses diff variable created line. example, can make histogram differences looking time :Exercise 9.7  Use following chunk make histogram differences looking times hearing people sing.Describe shape distribution (sure note number modes, skewness, whether may outliers).Based histogram, data appear satisfy conditions required using mathematical model (specifically, T distribution)? ?Based histogram, seem like average preference may changed hearing people sing? , direction?","code":"\nlullaby %>%\n    mutate(diff = After - Before)lullaby %>%\n    mutate(diff = After - Before) %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 0.1)"},{"path":"lab9.html","id":"confidence-interval-by-bootstrapping","chapter":"Lab 9 Inference about a mean","heading":"9.2.3 Confidence interval by bootstrapping","text":"Exercise 9.8  begin, let’s use bootstrapping construct confidence interval difference looking time . confidence interval describe much hearing someone sing familiar melody changes infants’ looking preferences.Fill blanks code construct 95% confidence interval mean difference looking proportion . sure refer code earlier session, well previous labs, guidance. (Hint: ’re looking name response variable, remember inference difference.)Based confidence interval found, evidence significance difference looking preferences hearing people sing? Explain reasoning.","code":"lullaby_boot_dist <- lullaby %>%\n    mutate(diff = After - Before) %>%\n    specify(response = ___) %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nlullaby_boot_ci <- lullaby_boot_dist %>%\n    get_confidence_interval(level = ___)\n\nlullaby_boot_dist %>%\n    visualize() +\n    shade_confidence_interval(lullaby_boot_ci)"},{"path":"lab9.html","id":"hypothesis-test-via-mathematical-model-1","chapter":"Lab 9 Inference about a mean","heading":"9.2.4 Hypothesis test via mathematical model","text":"Exercise 9.9  Next, use mathematical model conduct hypothesis test. hypothesis test address research question, “difference infants’ average looking behavior hearing two strangers sing?”null alternative hypotheses corresponding research question?Fill blanks code calculate observed T score visualize falls relative null distribution. Based visualization made, observed data likely unlikely null hypothesis true? (Hint: mu, null value; last blank, check filled previous labs depending alternative hypothesis .)Fill blanks code get \\(p\\) value (make sure ’ve done part b first!). get \\(p\\) value? reject null hypothesis?Give one-sentence summary outcome hypothesis test tells us infant looking behavior affected hearing someone sing familiar melody.","code":"lullaby_obs_t <- lullaby %>%\n    mutate(diff = After - Before) %>%\n    specify(response = ___) %>%\n    hypothesize(null = \"point\", mu = ___) %>%\n    calculate(stat = \"t\")\n\nlullaby_null_dist <- lullaby %>%\n    mutate(diff = After - Before) %>%\n    specify(response = ___) %>%\n    hypothesize(null = \"point\", mu = ___) %>%\n    assume(\"t\")\n\nlullaby_null_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = lullaby_obs_t, direction = \"___\")\nlullaby_null_dist %>%\n    get_p_value(obs_stat = lullaby_obs_t, direction = \"___\")"},{"path":"lab9.html","id":"wrap-up-8","chapter":"Lab 9 Inference about a mean","heading":"9.3 Wrap-up","text":"session, saw can use kinds computational techniques applied proportions—particularly bootstrapping—construct confidence intervals perform hypothesis tests means. also saw use T distribution find confidence intervals conduct hypothesis tests.second part, used R’s mutate function compute differences within participant. end, able address question whether infants respond favorably (looking ) stranger sings familiar song, giving us insight potential social importance music.","code":""},{"path":"lab10.html","id":"lab10","chapter":"Lab 10 Inference for comparing two independent means","heading":"Lab 10 Inference for comparing two independent means","text":"session, see conduct inferences whether two groups differ average. situations, two separate independent samples two (potentially) different populations. want know observed differences sample means reflect difference means two populations reasonably attributed chance alone. Just like inferences differences proportions, can use simulation mathematical methods address types questions.help later exercises session, sure download worksheet session right-clicking following link selecting “Save link …”: Worksheet Lab 10. Open saved file (default called “ws_lab10.Rmd”) RStudio.","code":""},{"path":"lab10.html","id":"what-if-there-were-two-groups-of-infants","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.1 What if there were two groups of infants?","text":"previous lab, analyzed data experiment infants listened two singers researchers recorded much time infant spent looking person sang familiar song, relative someone sang new song. data previous lab paired. infant, compare amount time looking familiar singer sang vs. sang. , created new variable called diff difference “” “” looking times.first part activity, going see conclusions affected instead compared two independent groups infants, one didn’t hear anyone sing (“” group) another hear two people sing (“” group). look difference mean looking times, now won’t looking difference within infant. looking difference group infants.","code":""},{"path":"lab10.html","id":"paired-vs.-two-group-data","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.1.1 Paired vs. two-group data","text":"Exercise 10.1  Run following chunk code examine first rows original infant data last time:Now look first rows data pretend infants two different groups:words, describe differences data structured original paired data pretend version two groups. Hint: able easily create new diff variable two-group data like original paired data?","code":"\nhead(lullaby_paired)\nhead(lullaby_twogroups)"},{"path":"lab10.html","id":"comparing-inferences","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.1.2 Comparing inferences","text":"Exercise 10.2  Last time, tested null hypothesis difference mean looking times zero. Another way state null hypothesis looking time hearing people sing independent. Now use pretend two-group data test null hypothesis see whether conclusions different.Fill blanks following chunk code use random permutation simulated 1000 datasets assuming null hypothesis true. end chunk, visualize sampling distribution difference means groups across simulations, along red line indicating observed difference falls. guidance, check Kobe’s data Lab 5; important difference stat calculate line (remember looking differences means, proportions). Also, remember pretend two-group data response variable (Looking) explanatory variable (Group).Based visualization just made, conclude? reject null hypothesis ?Run following chunk code perform test using original paired data (last time, mathematical model instead simulation, outcome ). come conclusion original paired data pretend two-group data?think observed difference means (red line) position X axis graphs?think sampling distribution wider part [] (pretend two-group data) part [b] (original paired data)?","code":"lullaby_twogroups_obs_diff <- lullaby_twogroups %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"___\", order = c(\"After\", \"Before\"))\n\nlullaby_twogroups_null_dist <- lullaby_twogroups %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"After\", \"Before\"))\n\nlullaby_twogroups_null_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = lullaby_twogroups_obs_diff, direction = \"two-sided\")\nlullaby_paired_obs_diff <- lullaby_paired %>%\n    mutate(diff = After - Before) %>%\n    specify(response = diff) %>%\n    calculate(stat = \"mean\")\n\nlullaby_paired_null_dist <- lullaby_paired %>%\n    mutate(diff = After - Before) %>%\n    specify(response = diff) %>%\n    hypothesize(null = \"point\", mu = 0) %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"mean\")\n\nlullaby_paired_null_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = lullaby_paired_obs_diff, direction = \"two-sided\")"},{"path":"lab10.html","id":"are-recruiters-more-impressed-by-a-spoken-vs.-written-hiring-pitch","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2 Are recruiters more impressed by a spoken vs. written hiring pitch?","text":"Imagine trying convince recruiter right person job. come better writing can give pitch person? question addressed series experiments Schroeder & Epley (2015). focus one experiments (Experiment 4 original paper).experiment, MBA students University Chicago business school prepared short “elevator pitches” describing hired firm choice. students recorded giving pitch. Finally, sample professional recruiters selected randomly assigned one two groups: One group hear audio recording student’s pitch; another group read transcript audio recording. recruiters evaluated potential applicants number scales, focus one response variable: scale 1–10, recruiters rated likelihood hire student based (spoken written) pitch.research question address data , “difference average hiring ratings recruiter hears reads pitch?”Exercise 10.3  null alternative hypotheses corresponding research question?","code":""},{"path":"lab10.html","id":"check-out-the-data-3","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.1 Check out the data","text":"First, let’s check first rows data:row represents responses particular recruiter. Whether recruiter got spoken written pitch recorded Condition variable, either “audio” (hearing spoken pitch) “transcript” (reading transcript pitch). recruiter’s hiring rating recorded hire variable, number 1 10.Exercise 10.4  Fill blanks code appropriate variable names visualize distribution ratings two groups. Hint: Keep mind names explanatory variable response variable.Describe shape distributions whether seems like may relationship type pitch hiring ratings.Fill blanks code use random permutation simulate one way data look null hypothesis true. Describe , , distributions differ looked original data part ().Exercise 10.5  Fill blanks code using appropriate variable names obtain numerical summaries group’s hiring ratings.Use resulting table answer following questions.two groups similar variability? (sure refer “rule thumb” checking whether two samples similar variability.)sample means suggest advantage either spoken (“audio”) written (“transcript”) hiring pitches?","code":"hiring_study %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(\"___\", ncol = 1)hiring_null_simulation <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1, type = \"___\")\n\nhiring_null_simulation %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(\"___\", ncol=1)hiring_study %>%\n    group_by(___) %>%\n    summarize(sample_mean = mean(___), sample_sd = sd(___), sample_size = n())"},{"path":"lab10.html","id":"hypothesis-testing-via-simulation","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.2 Hypothesis testing via simulation","text":"’ve seen, can use random permutation model data look null hypothesis true. simulated datasets, compute difference mean hiring ratings audio transcript conditions. many times, build sampling distribution difference means. simulated datasets produce difference least extreme actually observed, can reject null hypothesis.Exercise 10.6  Fill blanks code (using appropriate variable names) find observed difference mean hiring ratings conditions, compare differences expect null hypothesis true. build sampling distribution differences means using random permutation. sure make least 1000 simulations. final blank, direction can either less, greater, two-sided; pick one corresponding alternative hypothesis.Based visualization, seems like observed difference unusual null hypothesis true?","code":"hiring_obs_diff <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"___\", order = c(\"audio\", \"transcript\"))\n\nhiring_null_simulation <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"audio\", \"transcript\"))\n\nhiring_null_simulation %>%\n    visualize() +\n    shade_p_value(obs_stat = hiring_obs_diff, direction = \"___\")"},{"path":"lab10.html","id":"hypothesis-testing-using-the-t-distribution","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.3 Hypothesis testing using the T distribution","text":"section, relate simulation approach just used mathematical model approach using T distribution.Exercise 10.7  Fill blanks code (using appropriate variable names) calculate T score associated sample:T score indicate observed difference relatively near far expect null hypothesis true? Explain reasoning., used code line calculate(stat = \"diff means\", order = c(\"audio\", \"transcript\")) get observed difference sample means. line differ calculate line code used Exercise find T score? Describe words stat option seems .Exercise 10.8  Fill blanks code visualize sample falls corresponding T distribution. , sure fill last blank according alternative hypothesis (either two-sided, less, greater).Compare visualization just made using mathematical model visualization made earlier using simulation. Relative null distribution, observed data (red vertical line) similar place? similar proportion null distribution extreme observed?Compare visualization just made using mathematical model visualization made earlier using simulation. Relative null distribution, observed data (red vertical line) similar place? similar proportion null distribution extreme observed?Fill blanks code get \\(p\\) value according mathematical model (, direction correspond alternative hypothesis). reject null hypothesis ? (sure state significance level!)Fill blanks code get \\(p\\) value according mathematical model (, direction correspond alternative hypothesis). reject null hypothesis ? (sure state significance level!)Based hypothesis test (either simulation mathematical approach ), give one sentence summary conclude effectiveness spoken vs. written pitches hiring ratings.Speculate results turned way . expect result generalize hiring decisions real life? ?","code":"hiring_obs_t <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"t\", order = c(\"audio\", \"transcript\"))\n\nhiring_obs_thiring_null_math <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    assume(\"t\")\n\nhiring_null_math %>%\n    visualize() +\n    shade_p_value(obs_stat = hiring_obs_t, direction = \"___\")\nhiring_null_math %>%\n    get_p_value(obs_stat = hiring_obs_t, direction = \"___\")"},{"path":"lab10.html","id":"wrap-up-9","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.3 Wrap-up","text":"session, got practice hypothesis tests compare means two independent groups. carried test using simulation mathematical models. Along way, saw whether evidence spoken written job pitches likely lead hiring ratings, illustrating methods used applied research settings.","code":""},{"path":"lab12.html","id":"lab12","chapter":"Lab 11 Inference for multiple independent groups","heading":"Lab 11 Inference for multiple independent groups","text":"session, use R inference comparing averages multiple independent groups. Specifically, interested hypothesis tests address questions whether multiple groups differ average, called Analysis Variance (ANOVA). see simulation techniques learned comparing proportions means two groups also apply multiple groups. mathematical model ANOVA different, however; “F distribution”. use techniques analyse experiment designed compare different ways minimize intrusions traumatic memories.","code":""},{"path":"lab12.html","id":"required-packages-1","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.1 Required packages","text":"session, going need load standard tidyverse package well infer package R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab12.html","id":"can-we-reduce-intrusions-of-traumatic-memories","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2 Can we reduce intrusions of traumatic memories?","text":"One hallmarks post-traumatic stress disorder memories traumatic event “intrude” everyday life, popping wanted. remember previous event, memory said “active”. According reconsolidation theory, active memories can also changed. example, tell story memory, telling story might change memory tell differently next time. Reconsolidation theory suggests one way reduce intrusions traumatic memories “activate” change less likely intrude everyday life.One way modify intrusive memories studied James et al. (2015). thought playing video game, namely Tetris, memory active help make memory less traumatic. participants first view traumatic film (including scenes real car accidents). next week, participants recorded often memory traumatic film intruded everyday life. week, participants came back lab randomly assigned one four treatment conditions:Reactivation-plus-Tetris: group, participants shown still images traumatic film “reactivate” memories film. played Tetris 12 minutes.Reactivation : group, participants shown still images traumatic film “reactivate” memories film, sat silently 12 minutes play Tetris.Tetris : group, participants shown images film simply played Tetris 12 minutes.task (control): group, participants simply sat quietly 12 minutes.According reconsolidation theory, people reactivation-plus-Tetris condition experience fewer intrusive memories playing Tetris change memories less disruptive. predicted happen group either original traumatic memories reactivated (control Tetris-) changed reactivation (control reactivation-).","code":""},{"path":"lab12.html","id":"load-the-data-6","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.1 Load the data","text":"Load relevant data R now:row shows data single participant. 3 variables dataset:condition: four treatment conditions participant .intrusions_pre: number times participant reported intrusive memory traumatic film prior treatment.intrusions_post: number times participant reported intrusive memory traumatic film treatment.Exercise 11.1  Consider design study address following questions.possible conclude treatment condition plays role causing changes might observe number intrusive memories? ?Fill blanks code use mutate create new variable called effect represents difference number intrusions treatment number intrusions treatment. Write code negative value effect means reduction number intrusive memories treatment. code use?","code":"\ntetris_data <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/tetris.csv\")## Rows: 72 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): condition\n## dbl (2): intrusions_pre, intrusions_post\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntetris_data## # A tibble: 72 × 3\n##    condition         intrusions_pre intrusions_post\n##    <chr>                      <dbl>           <dbl>\n##  1 No task (control)              2               4\n##  2 No task (control)              2               3\n##  3 No task (control)              5               6\n##  4 No task (control)              0               2\n##  5 No task (control)              5               3\n##  6 No task (control)              4               4\n##  7 No task (control)              0               0\n##  8 No task (control)              4               4\n##  9 No task (control)              3               2\n## 10 No task (control)              5              11\n## # … with 62 more rowstetris_data %>%\n    mutate(effect = ___ - ___)"},{"path":"lab12.html","id":"visualize-the-data","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.2 Visualize the data","text":"Exercise 11.2  Fill blanks code make boxplot compares distribution effect group (defined treatment condition):research question , “difference average effectiveness treatment conditions?” null alternative hypotheses corresponding research question?names explanatory variable response variable?Based boxplot just made exercise, data seem consistent null alternative hypothesis? Explain reasoning.","code":"tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    ggplot(aes(x = ___, y = effect)) +\n    geom_boxplot()"},{"path":"lab12.html","id":"hypothesis-testing-by-randomization","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.3 Hypothesis testing by randomization","text":"use data conduct hypothesis test help us address research question, “difference average effectiveness treatment conditions?” ’ve seen, test statistic need F statistic, represents much variability groups, relative amount variability within groups.","code":""},{"path":"lab12.html","id":"the-observed-f-statistic","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.3.1 The observed F statistic","text":"Exercise 11.3  Fill blanks code find F statistic observed data. Hint: specify line, remember put name response variable left ~ name explanatory variable right.F statistic found?F statistic found indicate -group variability greater smaller within-group variability? Explain reasoning.","code":"obs_f <- tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"F\")\n\nobs_f"},{"path":"lab12.html","id":"simulating-the-null-hypothesis","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.3.2 Simulating the null hypothesis","text":"Just like comparing proportions comparing means independent samples, can simulate data look null hypothesis true. Specifically, simulate single dataset, randomly re-assign (permute) values explanatory variable across observations. calculate F statistic simulated dataset. repeat process many times build sampling distribution F statistic. Finally, see whether F statistic actual data large enough reject idea came sampling error alone.Exercise 11.4  Fill blanks code use random permutation conduct hypothesis test. final result histogram simulated F statistics along line indicating observed F statistic (obs_f last exercise) falls distribution. Hint: blanks hypothesize generate lines, consider simulated null hypothesis comparing proportions means independent samples.Based histogram just produced, consider observed F statistic unusually large null hypothesis true?Run chunk code overlay mathematical model null hypothesis—“F distribution”—histogram simulated F statistics. Describe shape F distribution (skewness, number modes) well whether F distribution (smooth curve) make good “fit” histogram simulated F statistics.","code":"null_dist <- tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"F\")\n\nnull_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_f, direction = \"greater\")\nnull_dist %>%\n    visualize(method = \"both\") +\n    shade_p_value(obs_stat = obs_f, direction = \"greater\")"},{"path":"lab12.html","id":"hypothesis-testing-by-mathematical-model","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.2.4 Hypothesis testing by mathematical model","text":"practice, ANOVA often done using mathematical model null hypothesis instead using simulation. relevant calculations carried computer, results often presented table makes easier see sausage made (format used class book).Exercise 11.5  Fill blanks code use R produce ANOVA table. Hint: first two lines just convenience; tell R store version data already effect variable. sure use mutate line ’ve using previous exercises. lm line, recall squiggly ~ used specify response explanatory variables.Find mathematically computed \\(p\\) value table just produced (column headings helpful, may also compare format ANOVA tables class book). \\(p\\) value?Using significance level 0.05, reject null hypothesis ? ?Summarize results hypothesis test tell us whether treatment conditions equally effective reducing intrusive memories.reconsolidation theory predicts reactivation-plus-Tetris significantly effective three treatment conditions. Describe follow-analyses use test prediction well whether follow-analyses require special adjustments.","code":"tetris_data_effect <- tetris_data %>%\n    mutate(effect = ___ - ___)\n\nlm(___ ~ ___, data = tetris_data_effect) %>%\n    anova()"},{"path":"lab12.html","id":"wrap-up-10","chapter":"Lab 11 Inference for multiple independent groups","heading":"11.3 Wrap-up","text":"final technique ’ve learned, Analysis Variance, used address questions whether multiple independent groups differ one another average. F statistic used summarize much variability groups versus amount variability within groups. used random permutation simulate kinds data occur null hypothesis true. used simulation well mathematical model determine whether observed F statistic large enough reject null hypothesis.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
