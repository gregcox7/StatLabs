[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"collection laboratory activities part APSY210.","code":""},{"path":"lab1.html","id":"lab1","chapter":"Lab 1 Exploring data with R","heading":"Lab 1 Exploring data with R","text":"session, learn bit data explore using R/RStudio. point learn bit data variables get feel power tools learning use rest semester.follow along activity, asked run bits code RStudio. code can copied--pasted “Console” lower left RStudio. also asked modify code run , can typing code console.document shows snippet code, also shows typical R output result running code. example, following snippet code adds two numbers; way R prints result shown snippet code.Another point today’s activity illustrate even though statistics dealing data, data meaningful. just numbers names, peek world. offer glimpses someone’s life, workings natural process, social structure, etc. dataset limited wide glimpse gives us, point statistics learn make decisions based glimpse.","code":"\n2 + 3## [1] 5"},{"path":"lab1.html","id":"r-and-rstudio","chapter":"Lab 1 Exploring data with R","heading":"1.1 R and RStudio","text":"labs make use RStudio, graphical interface statistical computing language R. R language represents current state art statistical computing academic industrial research. likely remain relevant many years come free open-source, meaning widely accessible improvements extensions made continuously large community professionals hobbyists. fact, many best features R using extensions made people outside “core” development team R. extensions called “packages”, represent bundles code useful statistics.RStudio interface makes easier work R language, also free can installed computers running modern operating system (Windows, Mac, Linux, etc.). R RStudio already installed computers classroom, well Technology-Enhanced Classrooms Library Public Computing Sites campus. working computer, easier time install RStudio . Installing RStudio requires installing R first.","code":""},{"path":"lab1.html","id":"installation-on-your-own-computer","chapter":"Lab 1 Exploring data with R","heading":"1.1.1 Installation on your own computer","text":"can install R computer following instructions : https://cran.rstudio.com/installing R, can install RStudio Desktop following instructions : https://rstudio.com/products/rstudio/download/","code":""},{"path":"lab1.html","id":"running-r-in-a-browser","chapter":"Lab 1 Exploring data with R","heading":"1.1.2 Running R in a browser","text":"Even don’t access computer RStudio installed locally, can use access internet. can run RStudio online : https://rstudio.cloud/. downside cap amount time can spend using online version, better using local installation whenever possible. may need create account use online version, free .","code":""},{"path":"lab1.html","id":"required-packages","chapter":"Lab 1 Exploring data with R","heading":"1.2 Required packages","text":"RStudio running, run line code grey box copying pasting “>” “Console” lower left pane RStudio window.working computer campus, see messages similar ones shown . messages tell us R loaded helpful “packages” “library” use. may notice messages “conflicts”, nothing need worry .working computer online version RStudio, might gotten error tried run last line. get error, run following line install “tidyverse” package need course.Installing “tidyverse” package takes , ’ll need . installed, can just run library(tidyverse) start RStudio everything need.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\ninstall.packages(\"tidyverse\")"},{"path":"lab1.html","id":"troubleshooting-tip","chapter":"Lab 1 Exploring data with R","heading":"1.2.1 Troubleshooting tip","text":"“tidyverse” package automatically loaded first start RStudio, make sure run linefirst thing time open RStudio. Chances , try running something get error, “tidyverse” package loaded. course, ’s possible thing can cause error! ’s good try first see clears things .","code":"\nlibrary(tidyverse)"},{"path":"lab1.html","id":"meet-your-data","chapter":"Lab 1 Exploring data with R","heading":"1.3 Meet your data","text":"data looking passenger records RMS Titanic, oceanliner famously sank April 15, 1912. Though liner filled capacity, lax safety precautions—including failure carry enough lifeboats—meant many passengers died unable evacuate ship struck iceberg.","code":""},{"path":"lab1.html","id":"load-the-data","chapter":"Lab 1 Exploring data with R","heading":"1.3.1 Load the data","text":"Run following line code console load data RStudio “environment”:now see “titanic” pop upper right hand pane RStudio window. Click “titanic” upper right take look data.","code":"\ntitanic <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/titanic.csv\")## Rows: 1309 Columns: 11\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (7): residence, sex, name, ticket, cabin, embarked, hometown\n## dbl (3): class, age, fare\n## lgl (1): survived\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab1.html","id":"check-out-the-variables","chapter":"Lab 1 Exploring data with R","heading":"1.3.2 Check out the variables","text":"RStudio looks like (helpful colored labels):upper left RStudio screen, ’ll see bunch columns. data “raw” form. row represents specific passenger (“case”) column represents different variable.Exercise 1.1  Find example following types variable dataset. Explain reasoning choice.Numerical (either discrete continuous)Ordinal categoricalNominal categorical","code":""},{"path":"lab1.html","id":"answering-questions-with-data","chapter":"Lab 1 Exploring data with R","heading":"1.4 Answering questions with data","text":"Now ’ve gotten acquainted kind data , can begin using answer questions. involve simplifying data, turning summary form makes easier understand. summaries fall heading “descriptive statistics”, meant describe important aspects data. four types summaries explore today frequency tables, proportions, bar charts, histograms. questions attempt answer survived perished Titanic.","code":""},{"path":"lab1.html","id":"frequency-tables","chapter":"Lab 1 Exploring data with R","heading":"1.4.1 Frequency tables","text":"One way answer literally question, “survived died Titanic?” read names 1300 passengers dataset. problem give us much sense particular people might survived versus . Treating survival response variable, like treat variables data explanatory variables. gives us sense types people less likely survived, tells us Titanic disaster whole.begin, first thing can construct frequency table simply counts number people survived number people died. summary give us sense scale disaster. RStudio window, see big open space just data? called “console” work. Copy paste code “Console”. code appear right “>”. , hit enter run see results.got table counted number people survive. went 1300 rows multiple variables just two numbers. pretty concise summary! ? Let’s break bit code:titanic name dataset.group_by(survived) tells R group cases dataset whether survived (TRUE) (FALSE).summarize(n=n()) tells R take grouped cases summarize counting number people group labeling resulting number “n”.funky symbol %>% connects three steps makes sure R order want. symbol called “pipe”.Let’s try things get sense code . happens change n = n() last line Number = n()?Everything looks except instead column labeled “n”, labeled “Number”. bit equals sign frequency table labeled.Now let’s try something seems like small change: Instead n = n() last line, let’s write n = m(). one letter, surely can’t big difference?R doesn’t like ! reports error doesn’t know m(). ’s n() function, instruction tells R count number something. hand, m() doesn’t mean anything R throws hands.can also get counts based variables. example, let’s ask many passengers “class” changing grouping variable code:Exercise 1.2  Try running chunk code written count number passengers without college degrees:Explain words chunk code work.Finally, let’s construct frequency table using multiple variables. lets us answer complex questions like, many British women aboard Titanic? question involves two variables, residence (people ) sex (whether classified male female). can put variables group_by line find answer.","code":"\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = n())## # A tibble: 2 × 2\n##   survived     n\n##   <lgl>    <int>\n## 1 FALSE      809\n## 2 TRUE       500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(Number = n())## # A tibble: 2 × 2\n##   survived Number\n##   <lgl>     <int>\n## 1 FALSE       809\n## 2 TRUE        500\ntitanic %>%\n  group_by(survived) %>%\n  summarize(n = m())## Error in `summarize()`:\n## ! Problem while computing `n = m()`.\n## ℹ The error occurred in group 1: survived = FALSE.\n## Caused by error in `m()`:\n## ! could not find function \"m\"\ntitanic %>%\n  group_by(class) %>%\n  summarize(n = n())## # A tibble: 3 × 2\n##   class     n\n##   <dbl> <int>\n## 1     1   323\n## 2     2   277\n## 3     3   709\ntitanic %>%\n  group_by(degree) %>%\n  summarize(n = n())\ntitanic %>%\n  group_by(residence, sex) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'residence'. You can override using the\n## `.groups` argument.## # A tibble: 6 × 3\n## # Groups:   residence [3]\n##   residence sex        n\n##   <chr>     <chr>  <int>\n## 1 American  Female   108\n## 2 American  Male     150\n## 3 British   Female    94\n## 4 British   Male     208\n## 5 Other     Female   264\n## 6 Other     Male     485"},{"path":"lab1.html","id":"titanic-props","chapter":"Lab 1 Exploring data with R","heading":"1.4.2 Proportions","text":"may heard , trying evacuate Titanic, rule put “women children first” onto lifeboats. suggests hypothesis, assuming rule actually followed: female passengers survived often male passengers.see data consistent hypothesis, can begin using code similar ’ve using count number male female passengers either survive. Notice can group_by one variable using list variable names separated commas:table contains information need test hypothesis, hard read different numbers male female passengers. want know whether greater proportion female passengers survived, compared proportion male passengers survived.find proportions taking count dividing sum counts. Specifically, group “” want find proportion elements group “” characteristic “B”, find\\[\n\\text{Proportion B} = \\frac{\\text{Number B}}{\\text{Total number 's}}\n\\]","code":"\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n())## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.## # A tibble: 4 × 3\n## # Groups:   sex [2]\n##   sex    survived     n\n##   <chr>  <lgl>    <int>\n## 1 Female FALSE      127\n## 2 Female TRUE       339\n## 3 Male   FALSE      682\n## 4 Male   TRUE       161"},{"path":"lab1.html","id":"r-is-a-fancy-calculator","chapter":"Lab 1 Exploring data with R","heading":"1.4.2.1 R is a fancy calculator","text":"Let’s use numbers frequency table find proportion male passengers survived proportion female passengers survived. illustrate , although R quite powerful, many ways just fancy calculator. calculator still handy!table just made, see 339 female passengers survived. \\(339 + 127\\) total female passengers. can use R find proportion female passengers survived using line code :Notice / stands “division” put \\(339 + 127\\) parentheses tell R whole sum denominator.Exercise 1.3  Find proportion male passengers survived (Hint: numbers need come table showed number female passengers survive.). Compare proportion survivors among male female passengers—proportions consistent “women children first” policy? Explain reasoning.","code":"\n339 / (339 + 127)## [1] 0.7274678"},{"path":"lab1.html","id":"another-way-to-get-proportions","chapter":"Lab 1 Exploring data with R","heading":"1.4.2.2 Another way to get proportions","text":"One thing notice R many ways thing. Instead calculating proportions hand, can add line code used make frequency table get R give us proportions female male passengers survive:new column p proportion represents proportion people group (either male female) either survive. Notice numbers p column male female survivors ones found preceding section.Exercise 1.4  Write run code gives us table, like one , shows proportion people class (1, 2, 3) survived died. , find helpful start code already used try filling blanks:code use? Class highest proportion survivors?","code":"\ntitanic %>%\n  group_by(sex, survived) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.## # A tibble: 4 × 4\n## # Groups:   sex [2]\n##   sex    survived     n     p\n##   <chr>  <lgl>    <int> <dbl>\n## 1 Female FALSE      127 0.273\n## 2 Female TRUE       339 0.727\n## 3 Male   FALSE      682 0.809\n## 4 Male   TRUE       161 0.191titanic %>%\n  group_by(___, survived) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))"},{"path":"lab1.html","id":"titanic-bar","chapter":"Lab 1 Exploring data with R","heading":"1.4.3 Bar charts","text":"looking patterns trends data, often easier see visualization rather table numbers. Bar charts make numerical relationships easy see visually, don’t need compare bunch numbers.example, made table count number passengers class. bar chart conveys information terms height bars.Pretty neat! now easy see many 3rd class passengers 1st 2nd, interestingly, fewer 2nd class 1st class passengers.code used similar ’ve using, differs important ways:\n* first line , telling R dataset using (titanic).\n* second line tells R want make plot want put variable class along horizontal axis plot (x axis). “gg” front “plot” refers “grammar graphics”, language R uses describe plots. language, different parts plot called “aesthetics”, x = class falls inside parenthetical labeled aes(thetic).\n* final line just tells R want make bar chart. grammar graphics, different types charts called geoms.\n* Notice second 2 lines connected + rather %>% symbol. historical accident, meaning two symbols basically . telling R order follow instructions.put bar charts side--side, can use compare groups. R, putting multiple graphs together called faceting. graph “facet”. can tell R make facets based specific variable adding line code, like :line end splits plot different “facets”, one level residence variable. Note put “faceting” variable name quotes (reason). result makes easy see distribution passengers across classes different depending —Americans Titanic tended wealthier first class passengers, relative passengers Britain elsewhere.Exercise 1.5  Make bar chart shows number people either survive depending country residence. , fill blanks code “facet” corresponds country residence “facet” two bars , one bar survivors one bar non-survivors.code use? possible reason relative number survivors different depending passengers ?","code":"\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar()\ntitanic %>%\n  ggplot(aes(x = class)) +\n  geom_bar() +\n  facet_wrap(\"residence\")titanic %>%\n  ggplot(aes(x = ___)) +\n  geom_bar() +\n  facet_wrap(\"___\")"},{"path":"lab1.html","id":"histograms","chapter":"Lab 1 Exploring data with R","heading":"1.4.4 Histograms","text":"far, summarizing categorical variables. also numerical variables data, example age passenger well much paid tickets. Let’s try making frequency table figure many people different ages sailed Titanic:Well ’s helpful! R didn’t even bother show us whole thing. Though can see something interesting: Age measured years, passengers least one year old, age whole number. fractions years passengers less year old—ages measured months rather years.main point even though age can measured less fine-grained manner, age effectively continuous. don’t want know many passengers exactly 31.3491 years old, want get general sense distribution ages across passengers.can construct summary conveys information using histogram. histogram similar bar chart; difference bar charts categorical variables histograms numerical variables. code constructs histogram summarize passenger age:resulting histogram shows bunch bars, height indicate number passengers within particular age range. Notice got couple messages R addition plot, one “non-finite values” another “picking better value”. R says, “non-finite values”, talking people age recorded. unfortunate thing real data: sometimes missing pieces. didn’t stop R making plot wanted using non-missing data, R wanted warn us just case.message “picking better value” important: make histogram, looking many things fall within particular range values, say, ages 4 8. decide ranges? don’t tell R , decide divide range values 30 “bins”, corresponds range values width. usually want.Instead, decide big small want ranges . following code tells R make histogram using “bins” 2 years “wide” (0-1, 2-3, etc.):just like bar charts, can split histogram different “facets”. pair histograms shows distribution ages passengers either survive:Exercise 1.6  Try making several different histograms passenger age split survival, using different bin widths:bin width believe gives best visual summary ? Describe whether shapes histograms facet consistent “women children first” rule.","code":"\ntitanic %>%\n  group_by(age) %>%\n  summarize(n = n())## # A tibble: 99 × 2\n##      age     n\n##    <dbl> <int>\n##  1 0.167     1\n##  2 0.333     1\n##  3 0.417     1\n##  4 0.667     1\n##  5 0.75      3\n##  6 0.833     3\n##  7 0.917     2\n##  8 1        10\n##  9 2        12\n## 10 3         7\n## # … with 89 more rows\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2)## Warning: Removed 263 rows containing non-finite values (stat_bin).\ntitanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(\"survived\")## Warning: Removed 263 rows containing non-finite values (stat_bin).titanic %>%\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = ___) +\n  facet_wrap(\"survived\")"},{"path":"lab1.html","id":"wrap-up","chapter":"Lab 1 Exploring data with R","heading":"1.5 Wrap-up","text":"Today began adventure using RStudio explore data. saw look data summarize various helpful ways. frequency tables, proportions, bar charts, histograms.Frequency tables count number times particular value particular variable (combination values across multiple variables) occurs dataset.can use counts frequency tables calculate proportions, better conveying relative values.Bar charts display counts categorical variables visual form makes easier compare .Histograms let us visually summarize counts numerical variables putting “bins”, width need decide.","code":""},{"path":"lab2.html","id":"lab2","chapter":"Lab 2 Sampling","heading":"Lab 2 Sampling","text":"previous activity, got exposure can explore data R way can use data help answer questions. activity, get sense data come processes produce data can force us change interpret data.Generally, data come sample larger population. Depending sample selected, may give us biased perspective larger population. particular, non-random samples (observational studies), must think carefully processes cases end sample. processes, might social nature, introduce confounding variables, can systematic differences different groups sample, requiring us interpret data differently.","code":""},{"path":"lab2.html","id":"first-things-first","chapter":"Lab 2 Sampling","heading":"2.1 First things first","text":"First, start RStudio.usual, next thing starting RStudio load tidyverse package R’s library using following line code (run RStudio console, like last time):Now ’s done, can get data.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab2.html","id":"sex-bias-sampling-bias-or-both","chapter":"Lab 2 Sampling","heading":"2.2 Sex Bias, Sampling Bias, or Both?","text":"1973, University California Berkeley noted disturbing fact graduate school admissions: considerably male applicants admitted female applicants. University, fearing lawsuit, wanted know reflected systematic sex discrimination admissions offices. shall see story turned complex (full details, see Bickel et al. (1973)).","code":""},{"path":"lab2.html","id":"load-the-data-1","chapter":"Lab 2 Sampling","heading":"2.2.1 Load the data","text":"First, run code load 1973 Berkeley admissions data workspace:berkeley dataset now visible “Environment” pane upper right corner RStudio. Click look raw data.applicant selected sample, clearly random. row data refers specific applicant 1973. applicant, three observed variables:Admit: Either “Admitted” “Rejected”, depending whether applicant admitted rejected.Gender: Either “Male” “Female”.Department: letter “” “F” identifies academic department applicant applied. actual department names obscured privacy.Exercise 2.1  type (nominal categorical, ordinal categorical, numerical) variables dataset?","code":"\nberkeley <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/berkeley.csv\")## Rows: 4526 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): Admit, Gender, Department\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab2.html","id":"are-more-males-than-females-admitted","chapter":"Lab 2 Sampling","heading":"2.2.2 Are more males than females admitted?","text":"noted , Berkeley concerned greater proportion male applicants admitted female applicants. verify whether true, let’s find proportions running following chunk code:rightmost column table (labeled p) gives proportion applicants gender either admitted rejected. Based table, looks like Berkeley right concerned—proportion male applicants admitted higher proportion female applicants admitted.Exercise 2.2  Compare chunk code just ran code used last activity section. note four lines following structure:four lines code , describe words line .","code":"\nberkeley %>%\n  group_by(Gender, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Gender'. You can override using the\n## `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   Gender [2]\n##   Gender Admit        n     p\n##   <chr>  <chr>    <int> <dbl>\n## 1 Female Admitted   557 0.304\n## 2 Female Rejected  1278 0.696\n## 3 Male   Admitted  1198 0.445\n## 4 Male   Rejected  1493 0.555___ %>%\n  group_by(___) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))"},{"path":"lab2.html","id":"is-there-a-bias-in-all-departments","chapter":"Lab 2 Sampling","heading":"2.2.3 Is there a bias in all departments?","text":"Even overall bias, case departments, just ?Exercise 2.3  Make new table gives proportion male female applicants admitted department. may find helpful fill blanks following chunk code (note new line end “filters” redundant rows table make long):departments admit higher proportion female applicants admit higher proportion male applicants?","code":"berkeley %>%\n  group_by(___, ___, Admit) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n)) %>%\n  filter(Admit == \"Admitted\")"},{"path":"lab2.html","id":"resolving-the-paradox","chapter":"Lab 2 Sampling","heading":"2.2.4 Resolving the paradox","text":"appear paradox hands: ignore differences departments, likely male applicant gets admitted female applicant. within departments, opposite true: greater proportion female applicants admitted male applicants. words, seems confounding variable work, related differences departments.try figure confound might , look two additional issues. first issue , department, proportion applicants female, regardless whether admitted ? can address question running following chunk code:Exercise 2.4  Based table just produced, departments male female applicants? overlap departments identified previous exercise admitting higher proportion female male applicants?second issue consider overall rate admission department.Exercise 2.5  Make table shows proportion applicants admitted department, regardless gender. may find useful modify chunk code just used.code use? Put departments order highest lowest admission rate.Finally, let’s create visualization may help us put pieces together. chunk code produces set bar charts, one department. similar made previous activity, new “aesthetic”, namely, use variable (Gender) fill bars different colors:Exercise 2.6  Compare code just ran create colored bar chart kind code used make bar charts previous activity. seems different?plot just made, total height bar represents total number applicants department either admitted rejected. bar divided two parts different colors, representing numbers male female applicants bar. can use total height bars see relative number people admitted vs. rejected department; can use amount red vs. teal panel see relative number female vs. male applicants department.Putting pieces together, found following:Across departments, proportion female applicants admitted lower proportion male applicants admitted.Within departments, proportion female applicants admitted higher proportion male applicants admitted.Departments high overall admission rates receive fewer female applicants departments low admission rates.Finally, note , although names specific departments removed, departments B (easy get received predominantly male applicants) physical sciences engineering whereas departments E F (hard get received considerably female applicants) social sciences humanities.Exercise 2.7  Even though results suggest sex bias admissions level individual departments, kinds bias results suggest might going ? potential biases related types sampling biases ’ve discussed class book?","code":"\nberkeley %>%\n  group_by(Department, Gender) %>%\n  summarize(n = n()) %>%\n  mutate(p = n / sum(n))## `summarise()` has grouped output by 'Department'. You can override using the\n## `.groups` argument.## # A tibble: 12 × 4\n## # Groups:   Department [6]\n##    Department Gender     n      p\n##    <chr>      <chr>  <int>  <dbl>\n##  1 A          Female   108 0.116 \n##  2 A          Male     825 0.884 \n##  3 B          Female    25 0.0427\n##  4 B          Male     560 0.957 \n##  5 C          Female   593 0.646 \n##  6 C          Male     325 0.354 \n##  7 D          Female   375 0.473 \n##  8 D          Male     417 0.527 \n##  9 E          Female   393 0.673 \n## 10 E          Male     191 0.327 \n## 11 F          Female   341 0.478 \n## 12 F          Male     373 0.522\nberkeley %>%\n  ggplot(aes(x = Admit, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(\"Department\")"},{"path":"lab2.html","id":"wrap-up-1","chapter":"Lab 2 Sampling","heading":"2.3 Wrap-up","text":"paradox confronted activity name: “Simpson’s Paradox”. Check good video demonstration paradox .Simpson’s Paradox occurs whenever pattern appears aggregate (like bias female applicants departments) disappears reverses look different subgroups (like apparent bias favor female applicants within departments). done activity, Simpson’s Paradox can resolved careful exploration data consideration potential confounding variables associated different subgroups.","code":""},{"path":"lab3.html","id":"lab3","chapter":"Lab 3 Describing data","heading":"Lab 3 Describing data","text":"session divided two parts. first, use R understand two measures variability work: standard deviation close cousin, variance. see valuable computer can quickly produce numerical visual summaries data.computers great mindless computation, means us people better mindful stuff. need think numbers mean may important understanding world.second part session analyze data experiment. use visual numerical summaries help draw conclusions going experiment.","code":""},{"path":"lab3.html","id":"load-the-tidyverse-library","chapter":"Lab 3 Describing data","heading":"3.1 Load the tidyverse library","text":"starting RStudio, begin making sure ’ve got tidyverse package loaded R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab3.html","id":"variance-and-standard-deviation","chapter":"Lab 3 Describing data","heading":"3.2 Variance and standard deviation","text":"Two important widely-used numerical summaries variability variance standard deviation. Although two different names, really two ways looking thing. standard deviation often used summary variability, although shall see variance useful purposes later course.summaries involve somewhat long chain calculations. important see chain understand numbers telling us. part session, use R find variance standard deviation simple simulated data. First, follow long chain. , see find summaries quickly efficiently.","code":""},{"path":"lab3.html","id":"the-big-picture","chapter":"Lab 3 Describing data","heading":"3.2.1 The big picture","text":"good numerical summary variability tell us much expect observed values variable differ “typical”. “standard deviation” means: summary typical (“standard”) amount observed values variable tend differ (“deviate”) central tendency. variance standard deviation based using mean describe central tendency.","code":""},{"path":"lab3.html","id":"load-the-data-2","chapter":"Lab 3 Describing data","heading":"3.2.2 Load the data","text":"see variance standard deviation calculated, let’s create small fake data set, creatively call my_data. contains 5 observations single variable called “X”.Although data real, similar one might observe random sample IQ scores (mean 100 standard deviation 15 typical population).","code":"\nmy_data <- tibble(X = c(91, 102, 107, 109, 111))"},{"path":"lab3.html","id":"step-by-step","chapter":"Lab 3 Describing data","heading":"3.2.3 Step by step","text":"now follow chain calculations, step step, results finding variance standard deviation set observed values like “X” variable simulated data.","code":""},{"path":"lab3.html","id":"find-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.1 Find the mean","text":"First, find mean observed values. R , many ways, fancy calculator. find mean observed values X variable plugging formula mean already know:using R calculator advantages hand, downside still type/copy numbers. Moreover, wanted use result something else (), ’d copy , , greater chances make mistake.Rather typing numbers manually, instead use R’s mean function within “summarize” line ’ve used :Exercise 3.1  ’ve used “summarize” function previous sessions, always preceded group_by line. Say words didn’t use group_by line find mean chunk code just ran.","code":"\n(91 + 102 + 107 + 109 + 111) / 5## [1] 104\nmy_data %>%\n    summarize(mean(X))## # A tibble: 1 × 1\n##   `mean(X)`\n##       <dbl>\n## 1       104"},{"path":"lab3.html","id":"find-the-deviations-from-the-mean","chapter":"Lab 3 Describing data","heading":"3.2.3.2 Find the deviations from the mean","text":"next step find, observed value data, deviation mean. mathematical notation, deviation can written \\((x_i - \\bar{x})\\) \\(x_i\\) shorthand observed value \\(\\bar{x}\\) mean.previous sessions, ’ve used mutate line transform counts proportions. Now use find deviations:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X))## # A tibble: 5 × 2\n##       X deviation\n##   <dbl>     <dbl>\n## 1    91       -13\n## 2   102        -2\n## 3   107         3\n## 4   109         5\n## 5   111         7"},{"path":"lab3.html","id":"square-the-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.3 Square the deviations","text":"Now ’ve found deviations, square , using mutate line:Note , R, caret symbol ^ used exponentiation.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2)## # A tibble: 5 × 3\n##       X deviation squared_deviation\n##   <dbl>     <dbl>             <dbl>\n## 1    91       -13               169\n## 2   102        -2                 4\n## 3   107         3                 9\n## 4   109         5                25\n## 5   111         7                49"},{"path":"lab3.html","id":"add-up-the-squared-deviations","chapter":"Lab 3 Describing data","heading":"3.2.3.4 Add up the squared deviations","text":"Now, add squared deviations using sum function part summarize line:Exercise 3.2  Modify chunk code just ran just add deviations instead squared_deviations. (Hint: think variable name put inside parentheses “sum” last line.)sum deviations?think sum deviations might relate idea mean “balance point” distribution numbers?","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation))## # A tibble: 1 × 1\n##   sum_squared_deviation\n##                   <dbl>\n## 1                   256"},{"path":"lab3.html","id":"variance-divide-by-n---1","chapter":"Lab 3 Describing data","heading":"3.2.3.5 Variance: Divide by \\(n - 1\\)","text":"variance sum squared deviations divided \\(n - 1\\), \\(n\\) number observed values, “sample size”. get \\(n\\), can include n = n() summarize line use another mutate line get variance:","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1))## # A tibble: 1 × 3\n##   sum_squared_deviation     n variance\n##                   <dbl> <int>    <dbl>\n## 1                   256     5       64"},{"path":"lab3.html","id":"standard-deviation-take-the-square-root","chapter":"Lab 3 Describing data","heading":"3.2.3.6 Standard deviation: Take the square root","text":"Finally, get standard deviation taking square root variance, final mutate line. R, sqrt function stands square root.","code":"\nmy_data %>%\n    mutate(deviation = X - mean(X)) %>%\n    mutate(squared_deviation = deviation^2) %>%\n    summarize(sum_squared_deviation = sum(squared_deviation), n = n()) %>%\n    mutate(variance = sum_squared_deviation / (n - 1)) %>%\n    mutate(standard_deviation = sqrt(variance))## # A tibble: 1 × 4\n##   sum_squared_deviation     n variance standard_deviation\n##                   <dbl> <int>    <dbl>              <dbl>\n## 1                   256     5       64                  8"},{"path":"lab3.html","id":"all-at-once","chapter":"Lab 3 Describing data","heading":"3.2.4 All at once","text":"just saw complete chain calculations involved finding variance standard deviation. might guessed fact R mean function, also functions directly calculate variances standard deviations. functions called var sd, respectively, can use summary line, like :code just ran much compact, readable, less liable lead typos, naturally use built-var sd functions pretty often!Exercise 3.3  Take look used mutate summarize lines calculating variance standard deviation step--step. words, describe basic difference mutate line used summarize line used .Hint: Think result got adding either mutate summarize line affected number rows /columns result.","code":"\nmy_data %>%\n    summarize(mean = mean(X), variance = var(X), standard_deviation = sd(X))## # A tibble: 1 × 3\n##    mean variance standard_deviation\n##   <dbl>    <dbl>              <dbl>\n## 1   104       64                  8"},{"path":"lab3.html","id":"diet-and-lifespan","chapter":"Lab 3 Describing data","heading":"3.3 Diet and Lifespan","text":"seen use R calculate numerical summaries central tendency variability, let’s see operate real data. data looking come study Yu et al. (1982). studied lifespan sample rats randomly assigned one two different diets: One group rats allowed eat freely, however wanted; another group fed restricted diet 60% calories free-eating rats .","code":""},{"path":"lab3.html","id":"load-the-data-3","chapter":"Lab 3 Describing data","heading":"3.3.1 Load the data","text":"First, need get data R. following code download import data current R session:“Environment” panel upper right, ’ll see new entry called “rats”. data just imported. Click look data, appear upper left. just two variables dataset, Diet Lifespan (measured days). Diet explanatory variable Lifespan response variable.","code":"\nrats <- read_csv('https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/ratlives.csv')## Rows: 195 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Diet\n## dbl (1): Lifespan\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab3.html","id":"summarizing-the-data-visually","chapter":"Lab 3 Describing data","heading":"3.3.2 Summarizing the data visually","text":"Although numerical summaries useful, always accompanied visual summaries order get complete understanding data.","code":""},{"path":"lab3.html","id":"histograms-1","chapter":"Lab 3 Describing data","heading":"3.3.2.1 Histograms","text":"Let’s make facetted histogram shows distribution lifespans rats diet. use option facet_wrap lets us set number columns (ncol) can see facets stacked atop one another.Exercise 3.4  First, let’s describe can see histograms (may also try bin widths histograms like).Describe shape two distributions. sure note number modes, skewness, whether may outliers either group.Compare two distributions. sure note whether seem differences central tendency variability two groups.","code":"\nrats %>%\n    ggplot(aes(x=Lifespan)) +\n    geom_histogram(binwidth=90) +\n    facet_wrap(\"Diet\", ncol = 1)"},{"path":"lab3.html","id":"boxplots","chapter":"Lab 3 Describing data","heading":"3.3.2.2 Boxplots","text":"’ve also seen examples boxplots useful visual summaries data. “box” boxplot encloses “inter-quartile range” (IQR), , middle 50% data falls 25th 75th percentiles. line middle box shows median . Boxplots easy produce R using geom_boxplot instead geom_histogram:Exercise 3.5  Compare boxplots histograms just made.sorts things seem easier boxplots opposed histograms? Think features data might easier see boxplot comparisons might easier make using boxplot.lose anything using boxplot instead histogram? important features data harder see boxplot?","code":"\nrats %>%\n    ggplot(aes(x=Lifespan, y=Diet)) +\n    geom_boxplot()"},{"path":"lab3.html","id":"summarizing-the-data-numerically","chapter":"Lab 3 Describing data","heading":"3.3.3 Summarizing the data numerically","text":"Finally, let’s use R get numerical summary lifespans rats two diets. first part session, focused mean standard deviation (variance), also seen median inter-quartile range (IQR) can also used numerical summaries central tendency variability.R can find median IQR us:Exercise 3.6  Fill blanks following chunk code find mean standard deviation lifespans rats diet. Hint: recall names functions used find quantities earlier.code use?Compare means standard deviations group rats histograms made earlier. differences mean lifespan groups reflected two histograms? differences standard deviation lifespan reflected two histograms?numerical summaries central tendency variability provide concise description lifespans rats two different diets. descriptions help us address research question: restricted diet effect lifespan?Exercise 3.7  Based visual numerical summaries made, let us draw conclusions research question.Briefly describe differences lifespans rats restricted diet versus rats free diet.Based design study, can conclude differences diet probably played role causing observed differences lifespan sample? ?population think generalize results?","code":"\nrats %>%\n    group_by(Diet) %>%\n    summarize(Median = median(Lifespan), IQR = IQR(Lifespan))## # A tibble: 2 × 3\n##   Diet       Median   IQR\n##   <chr>       <dbl> <dbl>\n## 1 Free         710   116 \n## 2 Restricted  1036.  300.___ %>%\n    group_by(___) %>%\n    ___(Mean = ___, SD = ___)"},{"path":"lab3.html","id":"wrap-up-2","chapter":"Lab 3 Describing data","heading":"3.4 Wrap-up","text":"session, saw find two important numerical summaries variability: variance standard deviation. analyzed data experiment studying relationship diet lifespan sample rats, using visual numerical summaries get complete picture relationships explanatory response variables.","code":""},{"path":"lab4.html","id":"lab4","chapter":"Lab 4 Relationships between numerical variables","heading":"Lab 4 Relationships between numerical variables","text":"session, use R help us describe relationships numerical variables. Typically, purpose see well can predict explain differences response variable terms differences explanatory variable. seen, several useful tools work : scatterplots, correlation, linear regression.","code":""},{"path":"lab4.html","id":"load-the-tidyverse","chapter":"Lab 4 Relationships between numerical variables","heading":"4.1 Load the tidyverse","text":"usual, started RStudio, first thing sure load tidyverse package R’s library using line :","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()"},{"path":"lab4.html","id":"always-plot-your-data","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2 Always plot your data","text":"Numerical summaries data extremely useful compact descriptions things like central tendency variability. also seen Pearson correlation coefficient useful summary strength direction relationship numerical variables. numerical summaries can also misleading, shall see following example.","code":""},{"path":"lab4.html","id":"load-the-data-4","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.1 Load the data","text":"First, need import data R using code .data artificial devised Anscombe (1973).","code":"\nanscombe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/cor_data.csv\")## Rows: 44 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Group\n## dbl (2): X, Y\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"correlations-for-different-sets-of-measurements","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.2 Correlations for different sets of measurements","text":"four groups observations data. groups labeled “”, “B”, “C”, “D”. Observations made two variables, labeled “X” “Y”. Click anscombe environment panel RStudio (upper right) take look.Using R, can quickly get numerical summaries data. code provides mean standard deviation X Y values group, well correlation X Y group.notice anything funny results? looks like basically differences groups!","code":"\nanscombe %>%\n    group_by(Group) %>%\n    summarize(Mean_X = mean(X), Mean_Y = mean(Y), SD_X = sd(X), SD_Y = sd(Y), r = cor(X, Y))## # A tibble: 4 × 6\n##   Group Mean_X Mean_Y  SD_X  SD_Y     r\n##   <chr>  <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 A          9   7.50  3.32  2.03 0.816\n## 2 B          9   7.50  3.32  2.03 0.816\n## 3 C          9   7.5   3.32  2.03 0.816\n## 4 D          9   7.50  3.32  2.03 0.817"},{"path":"lab4.html","id":"scatterplots","chapter":"Lab 4 Relationships between numerical variables","heading":"4.2.3 Scatterplots","text":"Rather numerical summary, now let’s use R visually summarize data using scatterplots. put X variable horizontal (“x”) axis Y variable vertical (“y”) axis. use Group “facetting” variable:Exercise 4.1  scatterplots , respond following questions:scatterplot indicate probably association two variables? , describe type relationship.correlation coefficient (r) provide good summary whether two variables related? ?","code":"\nanscombe %>%\n    ggplot(aes(x=X, y=Y)) +\n    geom_point() +\n    facet_wrap(\"Group\")"},{"path":"lab4.html","id":"and-now-for-a-word","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3 And now for a word","text":"Psycholinguistics study perceptual cognitive processes involved learning, understanding, producing language. One ways psycholinguists study language processing using “lexical decision task”. lexical decision task, participants shown strings letters; sometimes, make real words (like “AUTHOR”) sometimes don’t (like “AWBLOR”). time someone takes decide string letters real word (AUTHOR) measure easily knowledge word can accessed. looking relationships lexical decision time (LDT) different properties word, can begin understand processes organize access knowledge language. words, lexical decision time tells us “mental dictionary” structured.English Lexicon Project collecting kind data lot people many different words English language. report mean lexical decision time word, along number properties word. treat mean lexical decision time response variable examine relationships number explanatory variables.","code":""},{"path":"lab4.html","id":"load-the-data-5","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.1 Load the data","text":"Run following line code download subset data English Lexicon Project.case/observation dataset particular word. addition lexical decision time, several potential explanatory variables measured word.","code":"\nelp <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/elp.csv\")## Rows: 31433 Columns: 25\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): Word, POS\n## dbl (23): Length, Freq_KF, Freq_HAL, SUBTLWF, SUBTLCD, Ortho_N, Phono_N, OLD...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab4.html","id":"examine-the-distribution-of-lexical-decision-times","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.2 Examine the distribution of lexical decision times","text":"LDT variable contains lexical decision time word dataset. LDT measured milliseconds. Let us first examine distribution lexical decision times using code :Exercise 4.2  Describe distribution lexical decision times across words (can re-run code look binwidths like). sure note number modes, skewness, whether potential outliers.","code":"\nelp %>%\n  ggplot(aes(x = LDT)) +\n  geom_histogram(binwidth=20)"},{"path":"lab4.html","id":"word-length-as-an-explanatory-variable","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3 Word length as an explanatory variable","text":"natural research question ask point whether LDT can explained word length. words, number letters word affect easy recognize?","code":""},{"path":"lab4.html","id":"scatterplot","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.1 Scatterplot","text":"First, make scatterplot two variables. Word length recorded variable named Length.scatterplot suggests positive relationship described line.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point()"},{"path":"lab4.html","id":"overlaying-a-line","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.2 Overlaying a line","text":"can easily put best-fitting linear regression line top scatterplot get visual sense well linear model describe relationship. , add line called geom_smooth(method = \"lm\"). geom_smooth puts “smooth” lines curves plot, including method = \"lm\" parentheses tells R specifically want linear model.","code":"\nelp %>%\n  ggplot(aes(x = Length, y = LDT)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"lab4.html","id":"finding-the-correlation","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.3 Finding the correlation","text":"can get correlation coefficient using cor function, like :","code":"\nelp %>%\n    summarize(r = cor(LDT, Length))## # A tibble: 1 × 1\n##       r\n##   <dbl>\n## 1 0.543"},{"path":"lab4.html","id":"finding-the-slope-and-intercept","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.3.4 Finding the slope and intercept","text":"Finally, can get slope intercept best-fitting line. line code looks little different ’ve done things now. just single line called “lm” “linear model”. parentheses, two instructions separated comma. first says explanatory response variables using funky format [response variable name] ~ [explanatory variable name]. second instruction parentheses tells R data find variables .’s looks together:result line gives us intercept best fitting line, well slope. slope labeled terms name explanatory variable.Exercise 4.3  Based result got running code , additional letter word , much longer take recognize word?","code":"\nlm(LDT ~ Length, data = elp)## \n## Call:\n## lm(formula = LDT ~ Length, data = elp)\n## \n## Coefficients:\n## (Intercept)       Length  \n##      546.33        28.15"},{"path":"lab4.html","id":"age-of-acquisition-as-an-explanatory-variable","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4 Age of Acquisition as an explanatory variable","text":"Although makes sense longer word take longer recognize, another important aspect word learned. reasonable think word learn early life easier access one learned recently.English Lexicon Project also records mean “age acquisition” word. mean age (years) someone first learns word. variable labeled Age_Of_Acquisition elp data. Now, follow steps looking relationship Length LDT, instead Length explanatory variable, use “age acquisition”.following exercises, sure refer code used previous section.","code":""},{"path":"lab4.html","id":"scatterplot-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.1 Scatterplot","text":", let’s first make scatterplot visualize relationship LDT age acquisition.Exercise 4.4  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis):code use?describe relationship LDT age acquisition?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___()"},{"path":"lab4.html","id":"overlaying-a-line-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.2 Overlaying a line","text":"Now, let’s add line scatterplot.Exercise 4.5  Fill blanks make scatterplot LDT response variable (y axis) Age_Of_Acquisition explanatory variable (x axis) best-fitting line overlaid top.code use?line seem good fit data? areas line seems -shoot -shoot data?","code":"___ %>%\n  ggplot(aes(x = ___, y = ___)) +\n  ___() +\n  ___(method = \"lm\")"},{"path":"lab4.html","id":"finding-the-correlation-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.3 Finding the correlation","text":"Now let’s find correlation (, sure look code used previous section guide):Exercise 4.6  Fill blanks find correlation LDT Age_Of_Acquisition:code use?correlation LDT Age Acquisition stronger weaker correlation LDT Length?ELP dataset, Age Acquisition measured years LDT measured milliseconds. correlation LDT Age Acquisition change Age Acquisition measured months LDT measured seconds? ?","code":"___ %>%\n    summarize(r = cor(___, ___))"},{"path":"lab4.html","id":"finding-the-slope-and-intercept-1","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.4.4 Finding the slope and intercept","text":"Finally, let’s find slope intercept best-fitting regression line Age Acquisition LDT:Exercise 4.7  Fill blanks code find intercept slope line using Age Acquisition explanatory variable LDT response variable:code use?According linear model just found, much longer take recognize word learned age 10, relative word learned age 9?make sense try extend relationship young ages (e.g., 6 months old)? Explain reasoning.expect relationship continue words learned relatively later life, like technical words learn college work? , shape expect relationship words learned later life ?","code":"lm(___ ~ ___, data = ___)"},{"path":"lab4.html","id":"summary","chapter":"Lab 4 Relationships between numerical variables","heading":"4.3.5 Summary","text":"seen can predict long takes recognize word terms either length (number letters) age learned (Age Acquisition). results tell us “mental dictionary” organized just things like spelling, also life experience.","code":""},{"path":"lab4.html","id":"wrap-up-3","chapter":"Lab 4 Relationships between numerical variables","heading":"4.4 Wrap-up","text":"seen scatterplots, correlation, linear regression valuable tools describing relationships numerical variables. tools help us explain details behavior reveal structure memory. tools always used carefully always visualization, since numerical summaries alone can misleading.","code":""},{"path":"lab5.html","id":"lab5","chapter":"Lab 5 Hypothesis testing with randomization","heading":"Lab 5 Hypothesis testing with randomization","text":"session, get initial practice testing hypotheses randomization. practice cover nuances hypothesis testing statistics, touch many key ideas see different forms throughout rest course.first part, get sense permutation works, helps us simulate sample might look like null hypothesis true. Permutation one way can use “randomization” test null hypothesis. second part, use permutation test serious hypotheses different types people might make decisions differently.Generally, hypothesis testing requires us follow set steps can outline like :Translate research question null alternative hypotheses, framed terms population parametersUse data sample calculate summary statistics corresponding population parametersModel summary statistics null hypothesis trueFind \\(p\\) valueForm conclusionTo help work exercises activity, sure download worksheet open RStudio.","code":""},{"path":"lab5.html","id":"did-kobe-have-a-hot-hand","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1 Did Kobe have a hot hand?","text":"get handle big ideas hypothesis testing general, permutation particular, let’s first look special dataset. data pertain Kobe Bryant LA Lakers playing Orlando Magic 2009 NBA finals. Commentators time remarked Kobe seemed “hot hand”. words, claiming Kobe made basket, likely make basket next shot.","code":""},{"path":"lab5.html","id":"check-out-the-data","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.1 Check out the data","text":"loaded R dataset called kobe consists every shooting attempt Kobe made game, including whether went (.e., shot “Hit” “Miss”). ’s first rows:","code":""},{"path":"lab5.html","id":"framing-the-hypotheses","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.2 Framing the hypotheses","text":"can translate claim hot hand null hypothesis alternative hypothesis. us believe “hot hand” claim, first rule possibility Kobe’s hit proportion regardless whether previous shot went . possibility null hypothesis. alternative hypothesis Kobe really hot hand made greater proportion hits already made hit missing.","code":""},{"path":"lab5.html","id":"summarize-the-data","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.3 Summarize the data","text":"Now ’ve framed hypotheses, let’s see whether data suggest reject null hypothesis. ’ve already seen using frequency table, like :, prev_shot refers whether Kobe’s previous shot hit (“prev_H”) miss (“prev_M”). final column p gives us proportions Hits (“H”) Misses (“M”) following either Previous Hit Previous Miss.use proportions final column calculate hand difference proportion Kobe’s made shots (Hits) following either hit (prev_H) miss (prev_M). can use R work us using chunk like one:chunk works like :Tell R name dataset working (kobe).specify variable explanatory variable (prev_shot) variable response variable (shot). using squiggly thing used linear regression ([response variable] ~ [explanatory variable]). response variable binary (.e., two-level) categorical variable, also need tell R “success” Hit (abbreviated “H”).Tell R calculate particular summary statistic. saying stat = \"diff props\" telling R want calculate difference proportions. already told R response variable shot success Hit, R knows want calculate difference proportion shots hits. saying order = c(\"prev_H\", \"prev_M\"), tell R want difference calculated specific order. math shorthand, write \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\).may seem like lot, remember computers needs told everything detail. can’t trust computer figure anything (yet?). Moreover, see lines code can “remixed” various ways convenient us go.Exercise 5.1  –following chunk code changes success setting \"H\" \"M\". words, describe number means get code.following chunk code changes order setting, puts success setting back \"H\". words, describe number means get code (different one part [] exercise!).","code":"\nkobe %>%\n    group_by(prev_shot, shot) %>%\n    summarize(n = n()) %>%\n    mutate(p = n / sum(n))## `summarise()` has grouped output by 'prev_shot'. You can override using the\n## `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   prev_shot [2]\n##   prev_shot shot      n     p\n##   <chr>     <chr> <int> <dbl>\n## 1 prev_H    H        18 0.36 \n## 2 prev_H    M        32 0.64 \n## 3 prev_M    H        25 0.410\n## 4 prev_M    M        36 0.590\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##      stat\n##     <dbl>\n## 1 -0.0498\nkobe %>%\n    specify(shot ~ prev_shot, success = \"M\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##     stat\n##    <dbl>\n## 1 0.0498\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_M\", \"prev_H\"))## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## # A tibble: 1 × 1\n##     stat\n##    <dbl>\n## 1 0.0498"},{"path":"lab5.html","id":"simulating-a-possible-dataset-if-the-null-were-true","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.4 Simulating a possible dataset if the null were true","text":"null hypothesis true, whether Kobe make shot depend whether make previous shot. can model Kobe’s performance look like null hypothesis true. , treat Kobe’s shooting history random. null hypothesis true, able randomly permute Kobe’s shooting history without changing overall relationship Kobe’s previous shots current shot. “overall relationship”, mean difference proportions just calculated. null hypothesis true, difference proportions permuted data different difference proportions actual data. hand, null hypothesis false, difference proportions permuted data different actually saw.can use R permute Kobe’s shot history thereby model Kobe’s shots might gone null hypothesis true. , need specify relevant variables well hypothesis. Let’s see , unpack :first second lines just like used already.third line tell R null hypothesis , namely, explanatory response variables independent (associated).Finally, fourth line generates simulated dataset randomly permuting—, shuffling—columns original data containing explanatory response variables. ’ll see function reps = 1 setting momentarily.Exercise 5.2  following chunk code simulates one way Kobe’s shots gone null hypothesis (previous shot current shot independent) true. fifth line (last line one chunks ) calculates difference proportions based simulated data.words, describe difference proportions simulated data different actual data.","code":"\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")## Response: shot (factor)\n## Explanatory: prev_shot (factor)\n## Null Hypothesis: independence\n## # A tibble: 111 × 3\n## # Groups:   replicate [1]\n##    shot  prev_shot replicate\n##    <fct> <fct>         <int>\n##  1 M     prev_H            1\n##  2 M     prev_M            1\n##  3 H     prev_M            1\n##  4 M     prev_H            1\n##  5 M     prev_H            1\n##  6 M     prev_M            1\n##  7 H     prev_M            1\n##  8 M     prev_M            1\n##  9 M     prev_H            1\n## 10 M     prev_H            1\n## # … with 101 more rows\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))"},{"path":"lab5.html","id":"simulating-many-possible-datasets-if-the-null-were-true","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.5 Simulating many possible datasets if the null were true","text":"simulate one possible way Kobe’s shots turned null hypothesis true. many possible ways Kobe’s shots turned , corresponds different random permutation Kobe’s shot history. need keep track differences proportions get permutations. ’s good thing computers good repetitive tasks, can use computer repeat random permutation process many times simulated many possible datasets.next chunk code except changed reps generate line 1 1000. way, can simulate 1000 different datasets, different random permutation Kobe’s shot history. tell R remember name kobe_null_distribution represents distribution differences expect see null hypothesis true.first rows result look like :“replicate” column label simulated dataset, new “stat” column difference \\(\\hat{p}_{\\text{Prev. H}} - \\hat{p}_{\\text{Prev. M}}\\) simulated dataset.can now make histogram examine distribution differences proportions result null hypothesis true:","code":"\nkobe_null_distribution <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1000, type = \"permute\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))\nkobe_null_distribution %>%\n    visualize()"},{"path":"lab5.html","id":"find-the-p-value","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.6 Find the \\(p\\) value","text":"Remember \\(p\\) value proportion simulated datasets least extreme actual data. case, means proportion simulated datasets difference proportions bigger observed. Let’s tell R remember difference running chunk code :Now, let’s see many simulated datasets bigger . following line code, tell R observed “statistic” (obs_stat) difference just told remember (kobe_obs_diff) well fact interested many simulations produced results “greater” observed:Finally, help visualize observed difference falls relative distribution differences simulated data (note uses shade_p_value rather get_p_value):red line value actually observed parts histogram shaded pink represent simulated datasets “extreme” observed.","code":"\nkobe_obs_diff <- kobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"prev_H\", \"prev_M\"))\nkobe_null_distribution %>%\n    get_p_value(obs_stat = kobe_obs_diff, direction = \"greater\")## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.757\nkobe_null_distribution %>%\n    visualize() +\n    shade_p_value(obs_stat = kobe_obs_diff, direction = \"greater\")"},{"path":"lab5.html","id":"form-a-conclusion","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.1.7 Form a conclusion","text":"Remember testing null hypothesis Kobe hot hand. reject null hypothesis data unlikely null hypothesis true, .e., \\(p\\) value low. Assume adopt significance level 0.05, reject null hypothesis \\(p\\) value less level.Exercise 5.3  reject null hypothesis Kobe’s previous shot influence current shot? Explain reasoning. conclusion say whether Kobe “hot hand”?","code":""},{"path":"lab5.html","id":"do-people-on-the-autism-spectrum-make-more-consistent-choices","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2 Do people on the autism spectrum make more consistent choices?","text":"kind hypothesis testing just fun case Kobe’s hot hand use answer serious research questions.Autism condition many facets. Individuals autism cognitive impairments often different cognitive “styles”. particular, thought people autism “detail-oriented”. can detriment trying find general pattern, might benefit situations many irrelevant distractions.potential benefit studied Farmer et al. (2017). experiment included group participants diagnosed autism spectrum (cognitive impairments) well group neuro-typical controls. looked participants’ choices pairs consumer products presented alongside third, less desirable “decoy” option. “rational” choice affected presence decoy, fact people often swayed irrelevant options. Might people autism make choices consistent—“rational”—ignore irrelevant decoy?rest lab, conduct hypothesis test address question. follow basic outline procedure followed address “hot hand” question, sure refer previous section guidance.","code":""},{"path":"lab5.html","id":"check-out-the-data-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.1 Check out the data","text":"relevant data stored R name asc_choice. ’s first rows look like.response variable Choice, either “Consistent” (participant’s choice affected decoy) “Inconsistent” (participant’s choice affected decoy).explanatory variable Group, either “ASC” (“Autism Spectrum Condition”) “NT” (“Neuro-Typical” control).","code":""},{"path":"lab5.html","id":"framing-the-hypotheses-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.2 Framing the hypotheses","text":"Exercise 5.4  research question , “proportion consistent choices higher participants ASC group NT group?” null hypothesis alternative hypothesis correspond research question?","code":""},{"path":"lab5.html","id":"summarize-the-data-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.3 Summarize the data","text":"Exercise 5.5  Find difference proportions consistent choices autism spectrum group (ASC) neuro-typical control group (NT). guidance, check Kobe example . sure note:name relevant dataset?names explanatory response variables?“Consistent” choice counts “success”.difference proportions, want look ASC minus NT.difference found consistent null hypothesis alternative hypothesis? Explain reasoning.","code":"___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))"},{"path":"lab5.html","id":"model-the-null-hypothesis","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.4 Model the null hypothesis","text":"null hypothesis true, participant’s choice shouldn’t depend group . Kobe example, modeled null hypothesis randomly permuting Kobe’s shot history, since explanatory variable. research scenario, group explanatory variable. randomly shuffling participants groups, can simulate data look null hypothesis true.Exercise 5.6  Fill blanks code simulate 1000 datasets assuming null hypothesis true, produce histogram (using visualize function) resulting simulated differences proportions. ’ll re-use lot last exercise!name relevant dataset?names explanatory response variables?“Consistent” choice counts “success”.want 1000 simulations.difference proportions, want look ASC minus NT.code working correctly, try running times.Explain words histogram get look time run code.Based just looking histogram (need calculations), describe aspects distribution seem stay seem differ time run code. Note things like shape distribution (number modes skewness), central tendency, variability.","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    visualize()"},{"path":"lab5.html","id":"find-the-p-value-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.5 Find the \\(p\\) value","text":"find \\(p\\) value, first need get observed difference proportions tell R remember label asc_obs_diff.Exercise 5.7  Fill blanks find observed difference proportions actual data tell R remember label asc_obs_diff. value used calculate \\(p\\) value. Hint 1: direction, remember correspond alternative hypothesis (“less”, “greater”, “two-sided”). Hint 2: sure reuse code previous exercises!\\(p\\) value got?","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_obs_diff <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    get_p_value(obs_stat = asc_obs_diff, direction = \"___\")"},{"path":"lab5.html","id":"form-a-conclusion-1","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.2.6 Form a conclusion","text":"Finally, position visualize observed difference proportions falls relative differences expected null hypothesis true. enable us form conclusion data tell us research question.Exercise 5.8  Fill blanks code visualize observed difference proportions falls distribution differences expected null hypothesis true (hint: look back previous exercises, ’ll able re-use lot!)Based analysis, reject null hypothesis? Explain reasoning, sure state reasonable significance level. conclusion say whether participants diagnosis autism made consistent choices neuro-typical participants?","code":"asc_null_distribution <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = ___, type = \"permute\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_obs_diff <- ___ %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nasc_null_distribution %>%\n    visualize() +\n    shade_p_value(obs_stat = asc_obs_diff, direction = \"___\")"},{"path":"lab5.html","id":"wrap-up-4","chapter":"Lab 5 Hypothesis testing with randomization","heading":"5.3 Wrap-up","text":"session, got practice using R perform hypothesis tests using randomization. Specifically, used type shuffling called permutation. Shuffling allows us simulate various ways particular dataset look like null hypothesis true. found \\(p\\) value visualized null distribution order get sense whether actual data unlikely null hypothesis true.","code":""},{"path":"lab6.html","id":"lab6","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"Lab 6 Confidence intervals with bootstrapping","text":"session, get practice using R find confidence intervals via bootstrapping. initial exposure hypothesis testing previous session, meant “first contact” basics techniques. first part, revisit Kobe data get view resampling works, since heart bootstrapping technique. second part, use bootstrapping create confidence intervals representing people’s tendencies making moral decisions.","code":""},{"path":"lab6.html","id":"required-packages-1","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.1 Required packages","text":"Like last time, require tidyverse infer packages session, make sure load R’s library line .","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab6.html","id":"what-is-kobes-field-goal-percentage","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2 What is Kobe’s field goal percentage?","text":"Last time, asked whether Kobe Bryant “hot hand” 2009 NBA finals. used randomization test null hypothesis Kobe’s chance making hit regardless whether previous shot hit .time just interested proportion Kobe’s shots actually make basket. traditional basketball term Kobe’s “field goal percentage”. constitutes population parameter label mathematically \\(\\pi_{\\text{Kobe}}\\). “population” shots Kobe ever attempted across career. sample shots Kobe attempted 2009 NBA finals. sample yields point estimate \\(\\hat{p}_{\\text{Kobe}}\\) population parameter \\(\\pi_{\\text{Kobe}}\\).","code":""},{"path":"lab6.html","id":"load-the-data-6","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.1 Load the data","text":"Run following line code download dataset consisting every shooting attempt Kobe made 2009 NBA finals. data looked last time:","code":"\nkobe <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/kobe.csv\")## Rows: 111 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (5): vs, quarter, description, shot, prev_shot\n## dbl  (1): game\n## time (1): time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab6.html","id":"what-is-the-point-estimate","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.2 What is the point estimate?","text":"interested time just proportion Kobe’s shots hits rather misses. today, don’t care outcome previous shot .get point estimate proportion Kobe’s shots hits rather misses, can use following chunk code:Exercise 6.1  chunk code used find observed difference hit proportions last time, function whether Kobe’s previous shot hit :Compare code last time code just used find proportion Kobe’s shots hits. Note similarities differences try describe differences might . Hint: consider last time explanatory response variable, now interested single (response) variable.","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.387\nkobe %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    calculate(stat = \"diff in props\", order = c(\"H\", \"M\"))"},{"path":"lab6.html","id":"model-the-randomness","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3 Model the randomness","text":"Although sample gives us point estimate (\\(\\hat{p}_{\\text{Kobe}}\\)) Kobe’s true field goal percentage (\\(\\pi_{\\text{Kobe}}\\)), know point estimate unlikely perfect estimate. know sample population subject sampling variability can treated effectively “random”. confidence interval based modeling sampling variability produced actual sample got, can know range values population parameter remain plausible.Bootstrapping models randomness using sample estimate population repeatedly resampling estimated population. Let’s see works.","code":""},{"path":"lab6.html","id":"a-single-resample","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3.1 A single resample","text":"get sense resampling works, let’s focus smaller number shots 2nd quarter 1st game. Like last time, filter function let’s us pull just shots:following chunk code resamples Kobe’s shots quarter produce new sample shots:Exercise 6.2  chunk code used last time generate single “shuffle” shots:Compare code last time code just used resampling. Note similarities differences. Hint: matter bootstrapping “null hypothesis”?final step bootstrapping calculate sample statistic—case, proportion hits—randomly generated re-sample. can adding calculate line end code used make resample:","code":"\nkobe %>%\n    filter(game == 1, quarter == 2)## # A tibble: 8 × 7\n##   vs     game quarter time   description                           shot  prev_…¹\n##   <chr> <dbl> <chr>   <time> <chr>                                 <chr> <chr>  \n## 1 ORL       1 2       05:58  Kobe Bryant makes 20-foot jumper      H     prev_H \n## 2 ORL       1 2       05:22  Kobe Bryant makes 14-foot jumper      H     prev_H \n## 3 ORL       1 2       04:37  Kobe Bryant misses driving layup      M     prev_H \n## 4 ORL       1 2       03:30  Kobe Bryant makes 9-foot two point s… H     prev_M \n## 5 ORL       1 2       02:55  Kobe Bryant makes 14-foot running ju… H     prev_H \n## 6 ORL       1 2       01:55  Kobe Bryant misses 19-foot jumper     M     prev_H \n## 7 ORL       1 2       00:38  Kobe Bryant misses 27-foot three poi… M     prev_M \n## 8 ORL       1 2       00:04  Kobe Bryant makes driving layup       H     prev_M \n## # … with abbreviated variable name ¹​prev_shot\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\")## Response: shot (factor)\n## # A tibble: 8 × 2\n## # Groups:   replicate [1]\n##   replicate shot \n##       <int> <fct>\n## 1         1 H    \n## 2         1 H    \n## 3         1 H    \n## 4         1 H    \n## 5         1 H    \n## 6         1 M    \n## 7         1 H    \n## 8         1 H\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(shot ~ prev_shot, success = \"H\") %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")\nkobe %>%\n    filter(game == 1, quarter == 2) %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1  0.75"},{"path":"lab6.html","id":"many-resamples","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.3.2 Many resamples","text":"can remove filter line generate single bootstrap resample proportion based full data.course, whole point bootstrapping get range plausible values need one resample! like last time, want get R remember proportions resample can use later; call boot_dist. following chunk code uses bootstrapping produce distribution sample proportions, variability mimics sampling variability work generating original observed sample:like last time, can use histogram get nice visual summary bootstrap distribution:","code":"\nkobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")## Response: shot (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.306\nboot_dist <- kobe %>%\n    specify(response = shot, success = \"H\") %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")\nboot_dist %>%\n    visualize()"},{"path":"lab6.html","id":"create-the-interval","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.4 Create the interval","text":"find confidence interval, need find “middle” bootstrap distribution. also need decide wide “middle” . example, create 95% confidence interval, need find middle 95% distribution .middle 95% two different quantiles bootstrap distribution. Specifically, 2.5% 97.5% quantiles define boundaries 95% confidence interval. values 2.5% simulated proportions interval 2.5% simulated proportions interval. , total, 5% values outside interval meaning remainder (95%) inside .can find relevant quantiles using faithful summarize function, now applied bootstrap distribution:list numbers probs gives probabilities , terms proportions rather percent (0.025 instead 2.5%, example).Exercise 6.3  Modify following code get boundaries 90% confidence interval:code use?boundaries found?many things, R shortcuts! following code also finds 95% confidence interval, use level set wide want interval. Note putting result label boot_ci can use help us visualize interval:Now can add 95% confidence interval saved boot_ci visualization:","code":"\nboot_dist %>%\n    summarize(CI = quantile(stat, probs = c(0.025, 0.975)))## # A tibble: 2 × 1\n##      CI\n##   <dbl>\n## 1 0.297\n## 2 0.478boot_dist %>%\n    summarize(CI = quantile(stat, probs = c(___, ___)))\nboot_ci <- boot_dist %>%\n    get_confidence_interval(level = 0.95)\nboot_dist %>%\n    visualize() +\n    shade_confidence_interval(endpoints = boot_ci)"},{"path":"lab6.html","id":"form-a-conclusion-2","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.2.5 Form a conclusion","text":"Now position form conclusion Kobe’s true field goal percentage based sample .Exercise 6.4  Answer following based 95% confidence interval just constructed Kobe’s field goal percentage.plausible Kobe makes half shots attempts? Explain reasoning.Kobe’s actual career field goal percentage 44.7%. words, true value population parameter \\(\\pi_{\\text{Kobe}} = 0.447\\). value contained within 95% confidence interval?","code":""},{"path":"lab6.html","id":"how-willing-are-people-to-sacrifice-one-life-to-save-many","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3 How willing are people to sacrifice one life to save many?","text":"well-known Vulcan dictum needs many outweigh needs (one). may true Vulcans necessarily morally acceptable humans. Moreover, even human endorses basic logic better save lives fewer lives may willing put logic practice. Fortunately, rarely put situations make choice.Philosophers psychologists studied moral decision making many ways. One way asking people hypothetical scenario. common scenario called “Trolley problem”:man blue standing railroad tracks notices empty trolley car rolling control. moving fast anyone hits die. Ahead main track five people. one person standing side track doesn’t rejoin main track. man blue nothing, trolley hit five people main track, one person side track. man blue flips switch next , divert trolley side track hit one person, hit five people main track.man blue ? sacrifice one person side track save five people main track? nothing, leaving five people main track fate? Awad et al. (2020) presented scenarios like large group people world (roughly 70,000), aim studying different cultural backgrounds might influence people’s moral judgments.presented different versions Trolley problem. One like description , man blue flip switch. different version went like :man blue standing footbridge railroad tracks notices empty trolley car rolling control. moving fast anyone hits die. Ahead track five people. large person standing near man blue footbridge, large person weighs enough trolley slow hit (man blue weigh enough slow trolley). man blue nothing, trolley hit five people track. man blue pushes one person, one person fall onto track, trolley hit one person, slow one person, hit five people farther track.second version, called “footbridge” version, man blue take much active role, literally throwing another person tracks save five. sense, though, amounts question: man blue sacrifice one person save five people?pictures, provided Awad et al. (2020), depict scenarios corresponding two different versions Trolley problem:section, look judgments Awad et al. (2020) obtained people see whether majority willing sacrifice one life save five scenarios.","code":""},{"path":"lab6.html","id":"load-the-data-7","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.1 Load the data","text":"Let’s load relevant data collected Awad et al. (2020) following line code:purposes, just data people living USA, still quite large sample! Now trolley data appeared RStudio’s environment pane (upper right), click explore structure data.","code":"\ntrolley <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/trolley.csv\")## Rows: 32163 Columns: 12\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (9): _id, Scenario, Sacrifice, UserID, Session_id, Template, lang, count...\n## dbl (1): Scenario_order\n## lgl (2): answerLeft, seenOther\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab6.html","id":"calc-prop","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.2 What is the point estimate?","text":"First, let’s get point estimate proportion people sacrifice someone. separately version Trolley problem, making use filter function like Kobe’s data .response variable Sacrifice either “Yes” “”. purposes, call “Yes” “success” (though course arguable whether either option Trolley problem considered “success”).Exercise 6.5  code provides starting point find point estimates proportion people scenario sacrifice someone save five others.proportion people sample “Switch” scenario chose sacrifice someone save five others?proportion people sample “Footbridge” scenario chose sacrifice someone save five others?","code":"trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    calculate(stat = \"___\")"},{"path":"lab6.html","id":"model-the-randomness-1","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.3 Model the randomness","text":"Now need model randomness involved sampling variability get bootstrap distributions proportions people recommend sacrificing scenario. Generate 1000 bootstrap resamples scenario.Exercise 6.6  code provides starting point use bootstrapping obtain distribution sample proportions gotten due sampling variability. Notice save two bootstrap distributions two different names, boot_dist_switch boot_dist_footbridge. goes well, appear RStudio environment pane, otherwise won’t see anything console.code use generate two bootstrap distributions?","code":"boot_dist_switch <- trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nboot_dist_footbridge <- trolley %>%\n    filter(Scenario == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = ___, type = \"___\") %>%\n    calculate(stat = \"___\")"},{"path":"lab6.html","id":"create-the-intervals","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.4 Create the intervals","text":"Now ’ve created bootstrap distributions, use following code starting point generate 95% confidence intervals . Hint: remember names gave bootstrap distributions previous exercise; also recall level number 0 1, rather percentage.Finally, can use following code outline visualize either two distributions filling blanks appropriate boot_dist_ boot_ci_.Exercise 6.7  –95% confidence interval proportion choose sacrifice “Switch” scenario?95% confidence interval proportion choose sacrifice “Footbridge” scenario?","code":"boot_ci_switch <- ___ %>%\n    get_confidence_interval(level = ___)\n\nboot_ci_footbridge <- ___ %>%\n    get_confidence_interval(level = ___)___ %>%\n    visualize() +\n    shade_confidence_interval(endpoints = ___)"},{"path":"lab6.html","id":"form-a-conclusion-3","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.3.5 Form a conclusion","text":"Now time interpret confidence intervals just found context particular research scenario.Exercise 6.8  confidence intervals just found tell us proportion people US population recommend sacrificing one life save five? proportion seem depend specifics scenario (.e., whether “Switch” “Footbridge” version)? version, say half population recommend sacrificing one person save five?","code":""},{"path":"lab6.html","id":"wrap-up-5","chapter":"Lab 6 Confidence intervals with bootstrapping","heading":"6.4 Wrap-up","text":"session, got practice using R construct confidence intervals using bootstrapping. used R generate many “resampled” dataset simulate kinds samples seen due sampling variability. allows us say values population parameter plausible given sample population.","code":""},{"path":"lab7.html","id":"lab7","chapter":"Lab 7 The normal distribution","heading":"Lab 7 The normal distribution","text":"venerable normal distribution “bell curve” almost like mascot statistics. Although variables real life distributed according normal distribution, real value describing sampling distributions. seen sampling distributions hypothesis testing confidence intervals: Sampling distributions represent variability point estimate like proportion mean due randomness involved selecting samples population. According central limit theorem, much time, sampling distributions approximately normal shape.session, first use normal distribution model population distributions, distribution values particular variable across whole population. Along way, get hang using R find proportions intervals based normal distribution. , second part, use normal distribution model sampling distributions. give us insight standard error relates things like sample size.","code":""},{"path":"lab7.html","id":"required-packages-tidyverse-and-infer","chapter":"Lab 7 The normal distribution","heading":"7.1 Required packages: tidyverse and infer","text":"require tidyverse infer packages session, make sure load R’s library:","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab7.html","id":"the-data-national-health-and-nutrition-examination-surveys-nhanes","chapter":"Lab 7 The normal distribution","heading":"7.2 The data: National Health and Nutrition Examination Surveys (NHANES)","text":"data using lab originally collected US National Center Heath Statistics 2009 2012. subset effectively simple random sample entire US population, though use observations many collected. Load data following line:","code":"\nnhanes <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/nhanes.csv\")## Rows: 4924 Columns: 76\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (31): SurveyYr, Gender, AgeDecade, Race1, Race3, Education, MaritalStatu...\n## dbl (41): ID, Age, AgeMonths, HHIncomeMid, Poverty, HomeRooms, Weight, Heigh...\n## lgl  (4): Length, HeadCirc, TVHrsDayChild, CompHrsDayChild\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-population-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3 The normal distribution as a model for a population distribution","text":"","code":""},{"path":"lab7.html","id":"hours-of-sleep-sleephrsnight","chapter":"Lab 7 The normal distribution","heading":"7.3.1 Hours of Sleep (SleepHrsNight)","text":"variable focus section number hours people report sleeping night, measured hours. recorded individual sample variable SleepHrsNight.","code":""},{"path":"lab7.html","id":"visualize-the-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.1 Visualize the distribution","text":"usual, begin visualizing actual data. reminder, assume time data constitute entire population interest. histogram shows population distribution nightly hours sleep:","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(binwidth = 1)"},{"path":"lab7.html","id":"is-the-normal-distribution-a-good-model","chapter":"Lab 7 The normal distribution","heading":"7.3.1.2 Is the normal distribution a good model?","text":"Next, want know whether normal distribution good model population distribution just visualized. , draw curve representing normal distribution top histogram just made.remember normal distribution just shape. center spread determined two population parameters: mean (\\(\\mu\\)) standard deviation (\\(\\sigma\\)). parameters ?Let’s first take wild guess say \\(\\mu = 8\\) \\(\\sigma = 1\\). can draw distribution using chunk code :Notice needed add aes(y = ..density..) histogram line, reasons clear next paragraph. final line lets us draw function graph. function called dnorm “density normal distribution”. say arguments function , list say mean sd normal distribution .whole “density” thing comes : histogram shows absolute counts. normal distribution specifies relative frequency different values. relative frequency called “density”. dnorm function gives us normal distribution. adding aes(y = ..density..) histogram line told R show density (relative frequency) rather absolute counts.Anyway, clear mean 8 hours standard deviation 1 hour make good fit. use instead actual mean SD data:Let’s give numbers label can use later:Exercise 7.1  Fill blanks code visualize fit normal distribution using values mean SD just found.normal distribution good job approximating shape population distribution?","code":"\nnhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = 8, sd = 1), color = 'darkred')\nnhanes %>%\n    summarize(mean(SleepHrsNight), sd(SleepHrsNight))## # A tibble: 1 × 2\n##   `mean(SleepHrsNight)` `sd(SleepHrsNight)`\n##                   <dbl>               <dbl>\n## 1                  6.86                1.32\nmean_sleep <- nhanes %>%\n    summarize(mean(SleepHrsNight)) %>%\n    pull()\n\nsd_sleep <- nhanes %>%\n    summarize(sd(SleepHrsNight)) %>%\n    pull()nhanes %>%\n    ggplot(aes(x = SleepHrsNight)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"proportions-from-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.3 Proportions from a normal distribution","text":"Using normal distribution, possible estimate proportion values number. example, following uses normal distribution estimate proportion people say get less eight hours sleep per night:Conversely, proportion get eight hours sleep per night:Notice difference whether lower.tail TRUE FALSE. lower.tail = TRUE, asking proportion value q, lower.tail = FALSE asking proportion value q.Exercise 7.2  Use pnorm function find following proportions:proportion people get less 6 hours sleep per night.proportion people get 10 hours sleep per night.proportion people get either less 6 hours sleep per night 10 hours sleep per night (Hint: can add proportions things overlap.)","code":"\npnorm(q = 8, mean = mean_sleep, sd = sd_sleep, lower.tail = TRUE)## [1] 0.8058069\npnorm(q = 8, mean = mean_sleep, sd = sd_sleep, lower.tail = FALSE)## [1] 0.1941931"},{"path":"lab7.html","id":"quantiles-from-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.3.1.4 Quantiles from a normal distribution","text":"saw last time, things like confidence intervals defined terms “quantiles”. Finding quantile opposite finding proportion. R, opposite relationship clear just swap p q. qnorm function find value q proportion p values either q.example, want know “25th percentile”, quantile q p = 0.25 less value:’s neat trick, though: 0.25 less value, means 0.75 value. flip lower.tail FALSE p 0.75, get answer:Exercise 7.3  Use qnorm function find following quantiles:quantile 0.025 data value?quantile 0.025 data value?two values middle 95% data fall? (Hint: much data fall middle 95%?)","code":"\nqnorm(p = 0.25, mean = mean_sleep, sd = sd_sleep, lower.tail = TRUE)## [1] 5.964337\nqnorm(p = 0.75, mean = mean_sleep, sd = sd_sleep, lower.tail = FALSE)## [1] 5.964337"},{"path":"lab7.html","id":"bad-days-daysmenthlthbad","chapter":"Lab 7 The normal distribution","heading":"7.3.2 Bad Days (DaysMentHlthBad)","text":"part survey, people asked many days last 30 consider mental health poor. recorded variable named DaysMentHlthBad.Just like number hours slept per night, find mean standard deviation DaysMentHlthBad see whether normal distribution seems provide good approximation population distribution.Exercise 7.4  Fill blanks chunk code find mean standard deviation DaysMentHlthBad plot data along approximating normal distribution.Describe shape distribution observed data shown histogram.normal distribution provide good fit data?Make reasoned guess population distribution might shape , taking note fact respondents give number days 30.","code":"mean_mental_health <- nhanes %>%\n    summarize(mean(___)) %>%\n    pull()\n\nsd_mental_health <- nhanes %>%\n    summarize(sd(___)) %>%\n    pull()\n\nnhanes %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    stat_function(fun = dnorm, args = list(mean = ___, sd = ___), color = 'darkred')"},{"path":"lab7.html","id":"the-normal-distribution-as-a-model-for-a-sampling-distribution","chapter":"Lab 7 The normal distribution","heading":"7.4 The normal distribution as a model for a sampling distribution","text":"previous section, got sense can use normal distribution approximate distribution observed values population:Find mean standard deviation.Check see well normal distribution mean standard deviation fits histogram observed data.now follow two steps, instead using normal distribution model population distribution, use model sampling distribution, , distribution summary statistics get across many samples population.section, assume nhanes dataset represents entire population interest. simulate drawing many different samples different sizes population calculate summary statistic sample. sampling distribution distribution summary statistics see well can approximate normal distribution.focus just one questions asked NHANES survey. question asks whether someone ever tried using marijuana. information resides variable Marijuana person’s response either “Yes” “”.Exercise 7.5  Without even looking data, make sense try use normal distribution approximate population distribution responses Marijuana question? ?","code":""},{"path":"lab7.html","id":"the-true-proportion","chapter":"Lab 7 The normal distribution","heading":"7.4.1 The “true” proportion","text":"assuming nhanes data represent entire population, can directly find “true” value population parameter \\(\\pi_{\\text{Marijuana}}\\), , proportion people ever tried marijuana. can use code :Actually, since interested proportion said “Yes”, can find proportion using compact code:“trick” previous chunk code : Marijuana == \"Yes\" checks see, person, whether response Marijuana question “Yes”. , gives person “score” 1, , gives person “score” 0. Recall mean sum set numbers divided total number numbers. find mean scores using mean(Marijuana == \"Yes\"), really adding number people “scored” 1 dividing total number people.","code":"\nnhanes %>%\n    group_by(Marijuana) %>%\n    summarize(n = n()) %>%\n    mutate(pi = n / sum(n))## # A tibble: 2 × 3\n##   Marijuana     n    pi\n##   <chr>     <int> <dbl>\n## 1 No         2045 0.415\n## 2 Yes        2879 0.585\nnhanes %>%\n    summarize(pi_marijuana = mean(Marijuana == \"Yes\"))## # A tibble: 1 × 1\n##   pi_marijuana\n##          <dbl>\n## 1        0.585"},{"path":"lab7.html","id":"simulating-many-samples","chapter":"Lab 7 The normal distribution","heading":"7.4.2 Simulating many samples","text":"Last time, used bootstrapping simulate happen collected many samples population using sample actually estimate whole population. Now, access population directly. can therefore draw many samples given size want!example, following chunk code randomly samples 5 people population gets R remember sample name sample_size5.Notice still people’s responses question NHANES survey. means can get summary statistic Marijuana variable sample, just like population.Exercise 7.6  Based code used find “true” proportion population, get point estimate based sample 5 people just drew (sample_size5).proportion people tried marijuana sample?expect get proportion another random sample population? ?’ve seen, great thing computers can boring repetitive things us quickly. Now, draw 1000 samples size 5 population get R remember name samples_size5.","code":"\nsample_size5 <- nhanes %>%\n    sample_n(size = 5)\n\nsample_size5## # A tibble: 5 × 76\n##      ID Surve…¹ Gender   Age AgeDe…² AgeMo…³ Race1 Race3 Educa…⁴ Marit…⁵ HHInc…⁶\n##   <dbl> <chr>   <chr>  <dbl> <chr>     <dbl> <chr> <chr> <chr>   <chr>   <chr>  \n## 1 65978 2011_12 male      57 50-59        NA White White Some C… NeverM… 25000-…\n## 2 53457 2009_10 female    30 30-39       364 Mexi… <NA>  8th Gr… Married <NA>   \n## 3 53965 2009_10 male      31 30-39       375 Mexi… <NA>  Some C… Married 65000-…\n## 4 54006 2009_10 male      20 20-29       240 Mexi… <NA>  Some C… NeverM… 75000-…\n## 5 63200 2011_12 female    59 50-59        NA White White Colleg… Married more 9…\n## # … with 65 more variables: HHIncomeMid <dbl>, Poverty <dbl>, HomeRooms <dbl>,\n## #   HomeOwn <chr>, Work <chr>, Weight <dbl>, Length <lgl>, HeadCirc <lgl>,\n## #   Height <dbl>, BMI <dbl>, BMICatUnder20yrs <chr>, BMI_WHO <chr>,\n## #   Pulse <dbl>, BPSysAve <dbl>, BPDiaAve <dbl>, BPSys1 <dbl>, BPDia1 <dbl>,\n## #   BPSys2 <dbl>, BPDia2 <dbl>, BPSys3 <dbl>, BPDia3 <dbl>, Testosterone <dbl>,\n## #   DirectChol <dbl>, TotChol <dbl>, UrineVol1 <dbl>, UrineFlow1 <dbl>,\n## #   UrineVol2 <dbl>, UrineFlow2 <dbl>, Diabetes <chr>, DiabetesAge <dbl>, …___ %>%\n    summarize(p_hat = ___)\nsamples_size5 <- nhanes %>%\n    rep_sample_n(size = 5, reps = 1000)\n\nsamples_size5## # A tibble: 5,000 × 77\n## # Groups:   replicate [1,000]\n##    replicate    ID SurveyYr Gender   Age AgeDecade AgeMonths Race1 Race3 Educa…¹\n##        <int> <dbl> <chr>    <chr>  <dbl> <chr>         <dbl> <chr> <chr> <chr>  \n##  1         1 54148 2009_10  male      34 30-39           418 Mexi… <NA>  9 - 11…\n##  2         1 60229 2009_10  male      50 50-59           605 Mexi… <NA>  8th Gr…\n##  3         1 68152 2011_12  male      56 50-59            NA Black Black High S…\n##  4         1 57109 2009_10  female    41 40-49           500 Other <NA>  Some C…\n##  5         1 71164 2011_12  male      48 40-49            NA White White Some C…\n##  6         2 57175 2009_10  male      49 40-49           590 White <NA>  Colleg…\n##  7         2 69456 2011_12  male      52 50-59            NA Black Black Colleg…\n##  8         2 66289 2011_12  male      41 40-49            NA White White Colleg…\n##  9         2 59541 2009_10  male      49 40-49           593 White <NA>  High S…\n## 10         2 67261 2011_12  male      57 50-59            NA White White 8th Gr…\n## # … with 4,990 more rows, 67 more variables: MaritalStatus <chr>,\n## #   HHIncome <chr>, HHIncomeMid <dbl>, Poverty <dbl>, HomeRooms <dbl>,\n## #   HomeOwn <chr>, Work <chr>, Weight <dbl>, Length <lgl>, HeadCirc <lgl>,\n## #   Height <dbl>, BMI <dbl>, BMICatUnder20yrs <chr>, BMI_WHO <chr>,\n## #   Pulse <dbl>, BPSysAve <dbl>, BPDiaAve <dbl>, BPSys1 <dbl>, BPDia1 <dbl>,\n## #   BPSys2 <dbl>, BPDia2 <dbl>, BPSys3 <dbl>, BPDia3 <dbl>, Testosterone <dbl>,\n## #   DirectChol <dbl>, TotChol <dbl>, UrineVol1 <dbl>, UrineFlow1 <dbl>, …"},{"path":"lab7.html","id":"summary-statistics-for-each-sample","chapter":"Lab 7 The normal distribution","heading":"7.4.3 Summary statistics for each sample","text":"Notice different samples labeled using variable replicate. can use get proportion marijuana triers sample using old group_by routine. tell R remember sample proportions name sample_props_mari_size5:Notice proportions calculated sample tend vary. can visualize using histogram:","code":"\nsample_props_mari_size5 <- samples_size5 %>%\n    group_by(replicate) %>%\n    summarize(p_hat = mean(Marijuana == \"Yes\"))\n\nsample_props_mari_size5## # A tibble: 1,000 × 2\n##    replicate p_hat\n##        <int> <dbl>\n##  1         1   0  \n##  2         2   0.8\n##  3         3   0.6\n##  4         4   0.8\n##  5         5   0.4\n##  6         6   0.8\n##  7         7   0.4\n##  8         8   0.8\n##  9         9   1  \n## 10        10   0.6\n## # … with 990 more rows\nsample_props_mari_size5 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2)"},{"path":"lab7.html","id":"approximating-with-a-normal-distribution","chapter":"Lab 7 The normal distribution","heading":"7.4.4 Approximating with a normal distribution","text":"Finally, see whether can approximate sampling distribution normal distribution, need find mean standard deviation sampling distribution. Recall, standard deviation sampling distribution special name: standard error (SE).tell R remember numbers names mean_props_mari_size5 se_props_mari_size5:Now draw normal curve top histogram made earlier:bad!Exercise 7.7  Use qnorm function help find 95% confidence interval proportion marijuana triers based samples size 5.may help start “skeleton” , put lower upper boundaries confidence interval labels ci_lower ci_upper respectively.95% confidence interval, according normal distribution?anything strange interval, given meant describe plausible values proportion? suggest whether normal distribution good approximation particular sampling distribution?","code":"\nmean_props_mari_size5 <- sample_props_mari_size5 %>%\n    summarize(mean(p_hat)) %>%\n    pull()\n\nse_props_mari_size5 <- sample_props_mari_size5 %>%\n    summarize(sd(p_hat)) %>%\n    pull()\nsample_props_mari_size5 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.2) +\n    stat_function(fun = dnorm, args = list(mean = mean_props_mari_size5, sd = se_props_mari_size5), color = 'darkred')ci_lower <- qnorm(p = ___, mean = ___, sd = ___, lower.tail = ___)\nci_upper <- qnorm(p = ___, mean = ___, sd = ___, lower.tail = ___)"},{"path":"lab7.html","id":"a-larger-sample","chapter":"Lab 7 The normal distribution","heading":"7.4.5 A larger sample","text":"following chunk code re-everything just , now sample size 80 instead 5:Exercise 7.8  –normal distribution provide good “fit” sampling distribution samples size 80?95% confidence interval based sample size 80, according normal distribution? (Hint: see previous exercise.)Compare standard error samples size 5 (se_props_mari_size5) samples size 80 (se_props_mari_size80). standard error change sample size?Compare mean sampling distributions samples size 5 (mean_props_mari_size5) samples size 80 (mean_props_mari_size80). mean change much sample size? close means “true” proportion population found earlier?Based relationships mean sampling distribution, standard error, sample size, conjecture reason might call standard deviation sampling distribution “standard error”. (Hint: consider typically trying use sample statistic imperfect estimate population parameter.)","code":"\nsamples_size80 <- nhanes %>%\n    rep_sample_n(size = 80, reps = 1000)\n\nsample_props_mari_size80 <- samples_size80 %>%\n    group_by(replicate) %>%\n    summarize(p_hat = mean(Marijuana == \"Yes\"))\n\nmean_props_mari_size80 <- sample_props_mari_size80 %>%\n    summarize(mean(p_hat)) %>%\n    pull()\n\nse_props_mari_size80 <- sample_props_mari_size80 %>%\n    summarize(sd(p_hat)) %>%\n    pull()\n\nsample_props_mari_size80 %>%\n    ggplot(aes(x = p_hat)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.0125) +\n    stat_function(fun = dnorm, args = list(mean = mean_props_mari_size80, sd = se_props_mari_size80), color = 'darkred')"},{"path":"lab7.html","id":"wrap-up-6","chapter":"Lab 7 The normal distribution","heading":"7.5 Wrap-up","text":"session, saw can use normal distribution model either population distribution sampling distribution. Sometimes normal distribution fits well, sometimes , important us check whether provides reasonable approximation . saw use normal distribution find intervals proportions. using normal distribution model sampling distribution, standard deviation (spread) called “standard error” close relationship sample size.","code":""},{"path":"lab8.html","id":"lab8","chapter":"Lab 8 Inference for proportions","heading":"Lab 8 Inference for proportions","text":"session, collect techniques encountered inferential statistics proportions:Hypothesis testing deciding whether two proportions significantly different one another.Confidence intervals representing values population proportion plausible based sample.Using normal distribution mathematical model sampling distribution.Many exercises activity require look back previous sessions hints guidance apply techniques new dataset addresses question whether algorithms can help making important medical diagnoses.","code":""},{"path":"lab8.html","id":"required-packages-2","chapter":"Lab 8 Inference for proportions","heading":"8.1 Required packages","text":"Make sure loaded tidyverse infer packages R’s library:","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab8.html","id":"get-to-know-the-data","chapter":"Lab 8 Inference for proportions","heading":"8.2 Get to know the data","text":"data working session study whether using algorithm can aid detection “adenomas” images colonoscopies (Wang et al., 2019). adenoma benign tumor can, time, become malignant. reason, important detect early can removed lead colon cancer. Adenomas detected looking particular structures images like one :Example detecting adenomas (outlined blue) medical image.can tell, looking structures always easy, even experts. Part problem images similar one another expert’s attention may drawn important parts image. words, even though application medical diagnosis, “problem” psychological. Perhaps computer algorithm help “solve” problem directing attention right parts image. hand, might algorithm gives bad suggestions. purpose study see whether algorithm effect , , whether helped hurt detection adenomas lead colon cancer.Run line download data R environment:Let’s take quick look data:data experiment. row represents specific image like one shown . image randomly assigned analyzed using different method: \"Routine\" analyses involved professional analyst looking image; \"Computer-aided\" analyses used automated algorithm identify potential adenomas double-checked human analyst. variable detected, either \"Yes\" \"\", indicates whether adenoma detected image.","code":"\ncolonoscopy <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/colonoscopy.csv\")## Rows: 1058 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): method, detected\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncolonoscopy## # A tibble: 1,058 × 2\n##    method         detected\n##    <chr>          <chr>   \n##  1 Routine        No      \n##  2 Computer-aided No      \n##  3 Computer-aided No      \n##  4 Routine        No      \n##  5 Routine        Yes     \n##  6 Routine        No      \n##  7 Computer-aided No      \n##  8 Computer-aided Yes     \n##  9 Routine        No      \n## 10 Routine        No      \n## # … with 1,048 more rows"},{"path":"lab8.html","id":"what-are-the-point-estimates","chapter":"Lab 8 Inference for proportions","heading":"8.3 What are the point estimates?","text":"start fancy inference, let’s first get point estimates, , proportions images identified containing adenoma using method. can figure hand using counts table :Exercise 8.1  Refer frequency table answer following questions.proportion images adenoma detected using “Routine” method?proportion images adenoma detected using “Computer-aided” method?Based point estimates, data suggest may benefit using algorithm?","code":"\ncolonoscopy %>%\n    group_by(method, detected) %>%\n    summarize(n = n())## `summarise()` has grouped output by 'method'. You can override using the\n## `.groups` argument.## # A tibble: 4 × 3\n## # Groups:   method [2]\n##   method         detected     n\n##   <chr>          <chr>    <int>\n## 1 Computer-aided No         370\n## 2 Computer-aided Yes        152\n## 3 Routine        No         427\n## 4 Routine        Yes        109"},{"path":"lab8.html","id":"confidence-intervals","chapter":"Lab 8 Inference for proportions","heading":"8.4 Confidence intervals","text":"consider question whether difference proportions methods large enough probably isn’t due chance, let’s take moment look detection rates using routine method. particular, based sample, construct confidence interval. interval reflect often routine analysis detects adenomas whole population. First, find interval using bootstrapping like . , use normal distribution.","code":""},{"path":"lab8.html","id":"using-bootstrapping","chapter":"Lab 8 Inference for proportions","heading":"8.4.1 Using bootstrapping","text":"Exercise 8.2  Fill blanks code generate bootstrap distribution proportion adenomas detected using \"Routine\" method find 95% confidence interval.code save bootstrap distribution name boot_dist_routine. confidence interval saved boot_ci_routine. end, code visualize result using histogram confidence interval overlaid top.guidance, sure refer previous activity.code use?95% confidence interval found?words, interpret interval means context scenario.Based histogram, think normal distribution good fit sampling distribution? ?","code":"boot_dist_routine <- colonoscopy %>%\n    filter(method == \"___\") %>%\n    specify(response = ___, success = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nboot_ci_routine <- boot_dist_routine %>%\n    get_confidence_interval(level = ___)\n\nboot_dist_routine %>%\n    visualize() +\n    shade_confidence_interval(endpoints = boot_ci_routine)"},{"path":"lab8.html","id":"using-the-normal-distribution","chapter":"Lab 8 Inference for proportions","heading":"8.4.2 Using the normal distribution","text":"Last time, saw , least time, use normal distribution approximate sampling distribution. advantage using normal distribution don’t need simulate distribution, can figure look like using math.particular, normal distribution says sampling distribution mean equal point estimate sample (\\(\\hat{p}\\)) standard error equal \\(\\sqrt{\\frac{\\hat{p} \\left(1 - \\hat{p} \\right)}{n}}\\) \\(n\\) sample size.Exercise 8.3  Fill blanks get R remember point estimate “Routine” method found earlier (\\(\\hat{p}\\)) well total number observations using “Routine” method (\\(n\\)). corresponding standard error calculated saved R name se_routine.standard error?Fill blanks following code use normal distribution find lower upper boundaries 95% confidence interval (ci_lower ci_upper, respectively). get lower upper boundaries confidence interval?interval just found part (b) compare 95% CI found earlier using bootstrapping? boundaries similar different?","code":"p_hat_routine <- ___\nn_routine <- ___\nse_routine <- sqrt((p_hat_routine * (1 - p_hat_routine)) / n_routine)\n\nse_routineci_lower <- qnorm(p = 0.025, mean = ___, sd = ___, lower.tail = TRUE)\nci_upper <- qnorm(p = 0.025, mean = ___, sd = ___, lower.tail = FALSE)"},{"path":"lab8.html","id":"hypothesis-test","chapter":"Lab 8 Inference for proportions","heading":"8.5 Hypothesis test","text":"Now handle well routine analysis can detect adenomas, using algorithm effect ? words, research question : difference proportion detected adenomas using routine method vs. computer-aided method? address question using hypothesis test, first randomization normal distribution.Note interested whether effect using computer-aided method , noted , possible algorithm help directing attention important parts image, also hurt directing attention uninformative parts image. want make sure test sensitive possible outcomes.Exercise 8.4  –explanatory variable response variable?null alternative hypotheses?Many techniques use used earlier activity, just applied current colonoscopy data instead. sure refer earlier activity hints guidance.","code":""},{"path":"lab8.html","id":"the-point-estimate-of-the-difference","chapter":"Lab 8 Inference for proportions","heading":"8.5.1 The point estimate of the difference","text":"Exercise 8.5  Fill blanks code find point estimate difference proportions routine computer-aided methods.specify line, remember first part goes [name response variable] ~ [name explanatory variable].code used?","code":"obs_diff <- colonoscopy %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"diff in props\", order = c(\"Computer-aided\", \"Routine\"))\n\nobs_diff"},{"path":"lab8.html","id":"using-randomization","chapter":"Lab 8 Inference for proportions","heading":"8.5.2 Using randomization","text":"First, conduct hypothesis test using randomization simulate data looked like null hypothesis true.Exercise 8.6  Fill blanks code use random permutation simulate data looked like null hypothesis true. resulting sampling distribution saved null_dist_random visualized using histogram.Hint: last blank, three “legal” options: can put either direction = \"less\", direction = \"greater\", direction = \"two-sided\" depending alternative hypothesis .code used?Based visualization just made, observed difference proportions plausible null hypothesis true?","code":"null_dist_random <- colonoscopy %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"diff in props\", order = c(\"Computer-aided\", \"Routine\"))\n\nnull_dist_random %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = \"___\")"},{"path":"lab8.html","id":"using-the-normal-distribution-1","chapter":"Lab 8 Inference for proportions","heading":"8.5.3 Using the normal distribution","text":"see hypothesis test go used normal distribution, actually don’t need much code. major difference stat calculate called \"z\", , code directly calculate \\(Z\\) value. know, \\(Z\\) value given \\[\n\\begin{align}\nZ & = \\frac{\\text{Point estimate} - \\text{Null mean}}{\\text{Standard error}} \\\\\nZ & = \\frac{(\\hat{p}_1 - \\hat{p}_2) - 0}{SE}\n\\end{align}\n\\]\\((\\hat{p}_1 - \\hat{p}_2)\\) observed difference proportions. result, \\(Z\\) value represents unusual observed difference relative distribution differences expected null hypothesis true.Exercise 8.7  –Fill blanks code find \\(Z\\) value observed difference. \\(Z\\) value found?Run following code visualize observed difference falls normal distribution. Compare code code used previous exercise; similar different?Use following code find \\(p\\) value, , proportion differences least extreme one observed, null hypothesis true. \\(p\\) value got?Assuming adopt significance level 0.05, reject null hypothesis? can conclude relationship using computer-aided analysis accuracy adenoma detection? (Recall experiment.)ended making different decision using randomization procedure previous section?","code":"obs_z <- colonoscopy %>%\n    specify(___ ~ ___, success = \"___\") %>%\n    calculate(stat = \"z\", order = c(\"Computer-aided\", \"Routine\"))\n\nobs_z\nnull_dist_normal <- colonoscopy %>%\n    specify(detected ~ method, success = \"Yes\") %>%\n    assume(\"z\")\n\nnull_dist_normal %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_z, direction = \"two-sided\")\nnull_dist_normal %>%\n    get_p_value(obs_stat = obs_z, direction = \"two-sided\")"},{"path":"lab8.html","id":"wrap-up-7","chapter":"Lab 8 Inference for proportions","heading":"8.6 Wrap-up","text":"session, revisited many core concepts statistical inference. used simulation normal distribution find confidence intervals individual proportions conduct hypothesis tests comparing proportions.","code":""},{"path":"lab9.html","id":"lab9","chapter":"Lab 9 Inference for a single mean","heading":"Lab 9 Inference for a single mean","text":"session, learn inference response variable numerical rather categorical. Specifically, focus can use data draw inferences population average might , based sample. similar drawing inferences population proportion. shall see, simulation methods like bootstrapping work just numerical response variable categorical response variable. differ mathematical model proportions normal distribution whereas mathematical model means T distribution.","code":""},{"path":"lab9.html","id":"required-packages-3","chapter":"Lab 9 Inference for a single mean","heading":"9.1 Required packages","text":"Make sure loaded tidyverse infer packages R’s library:","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab9.html","id":"inference-with-nhanes","chapter":"Lab 9 Inference for a single mean","heading":"9.2 Inference with NHANES","text":"begin, take another look two variables NHANES dataset already looked previous lab. Load now:Remember variable DaysMentHlthBad represents number days, past 30, person says mental health poor. saw already distribution observed values variable well described normal curve, can see histogram :Nonetheless, know central limit theorem , even though distribution original data normal, distribution sample means tend look something like normal distribution.","code":"\nnhanes <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/nhanes.csv\")## Rows: 4924 Columns: 76\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (31): SurveyYr, Gender, AgeDecade, Race1, Race3, Education, MaritalStatu...\n## dbl (41): ID, Age, AgeMonths, HHIncomeMid, Poverty, HomeRooms, Weight, Heigh...\n## lgl  (4): Length, HeadCirc, TVHrsDayChild, CompHrsDayChild\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nnhanes %>%\n    ggplot(aes(x = DaysMentHlthBad)) +\n    geom_histogram(binwidth = 1)"},{"path":"lab9.html","id":"bootstrapping","chapter":"Lab 9 Inference for a single mean","heading":"9.2.1 Bootstrapping","text":"can construct distribution sample means using bootstrapping. Run following code build sampling distribution save name boot_dist_mental_health.Exercise 9.1  following chunk code use bootstrapping construct sampling distribution proportion people tried marijuana:Compare chunk code first four lines code just prior exercise used build sampling distribution mean number bad mental health days. four lines, similar different? difference, describe think difference.Let’s visualize sampling distribution mean number poor mental health days:","code":"\nboot_dist_mental_health <- nhanes %>%\n    specify(response = DaysMentHlthBad) %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"mean\")\n\nboot_dist_mental_health## Response: DaysMentHlthBad (numeric)\n## # A tibble: 1,000 × 2\n##    replicate  stat\n##        <int> <dbl>\n##  1         1  4.49\n##  2         2  4.45\n##  3         3  4.42\n##  4         4  4.69\n##  5         5  4.33\n##  6         6  4.38\n##  7         7  4.38\n##  8         8  4.35\n##  9         9  4.33\n## 10        10  4.62\n## # … with 990 more rows\nboot_dist_marijuana <- nhanes %>%\n    specify(response = Marijuana, success = \"Yes\") %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"prop\")\nboot_dist_mental_health %>%\n    visualize()"},{"path":"lab9.html","id":"standard-error-of-the-mean","chapter":"Lab 9 Inference for a single mean","heading":"9.2.2 Standard error of the mean","text":"find standard deviation sampling distribution, get estimate standard error mean:can also estimate standard error mean using mathematical model approach based central limit theorem. According approach, standard error \\[\nSE = \\frac{s}{\\sqrt{n}}\n\\]\\(s\\) sample standard deviation \\(n\\) sample size.Exercise 9.2  Fill blanks code use R calculate standard error , based mathematical model. (Hint: blanks names numbers calculated earlier chunk.)get standard_error?Compare value part () found just exercise calculating standard deviation bootstrap distribution. values similar different? Describe expect two values similar different.","code":"\nboot_dist_mental_health %>%\n    summarize(standard_error = sd(stat))## # A tibble: 1 × 1\n##   standard_error\n##            <dbl>\n## 1          0.115sample_sd <- nhanes %>%\n    summarize(sd(DaysMentHlthBad)) %>%\n    pull()\n\nsample_size <- nhanes %>%\n    summarize(n()) %>%\n    pull()\n\nstandard_error <- ___ / sqrt(___)\n\nstandard_error"},{"path":"lab9.html","id":"confidence-intervals-1","chapter":"Lab 9 Inference for a single mean","heading":"9.2.3 Confidence intervals","text":"Suppose want construct 95% confidence interval population mean, based sample. ’ve already seen bootstrap distribution:using mathematical model, use formula ’ve seen :\\[\n\\bar{x} \\pm t^{\\star}_{df} \\times SE\n\\]already found standard error (\\(SE\\)). Next, need find \\(t^{\\star}_{df}\\), “critical value” T distribution divides middle 95% distribution upper lower tails. R’s qt function help us , ’ll recall, T distribution different shape depending number degrees freedom (\\(df\\)). working just single sample, degrees freedom one less sample size:\\[\ndf = n - 1\n\\]Luckily, already told R remember sample size previous exercise, can find number degrees freedom:Now, can use qt distribution find quantile T distribution represents upper tail confidence interval. set p = 0.975 want find point 2.5% (0.025) distribution quantile, 97.5% . resulting quantile critical t value:Finally, last ingredient need confidence interval sample mean (\\(\\bar{x}\\)):Exercise 9.3  Fill blanks find 95% confidence interval according T distribution. Hint: best bet fill blanks names numbers calculated running code chunks exercise. Consider formula confidence interval given . Also, remember * R multiplication.code use?interval got?resulting confidence interval similar different one found earlier using bootstrap distribution?Interpret confidence interval context research scenario.","code":"\nboot_ci_mental_health <- boot_dist_mental_health %>%\n    get_confidence_interval(level = 0.95)\n\nboot_ci_mental_health## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1     4.26     4.71\ndegrees_of_freedom <- sample_size - 1\n\ndegrees_of_freedom## [1] 4923\ncritical_t <- qt(p = 0.975, df = degrees_of_freedom)\n\ncritical_t## [1] 1.960446\nsample_mean <- nhanes %>%\n    summarize(mean(DaysMentHlthBad)) %>%\n    pull()\n\nsample_mean## [1] 4.48355ci_lower <- ___ - ___ * ___\nci_upper <- ___ + ___ * ___\n\nc(ci_lower, ci_upper)"},{"path":"lab9.html","id":"hypothesis-testing","chapter":"Lab 9 Inference for a single mean","heading":"9.2.4 Hypothesis testing","text":"Consider , course past 30 days, 4 days poor mental health equivalent roughly one bad mental health day per week. don’t know people’s responses question whether bad days occurred together spread , reasonable ask research question, “Americans average four bad mental health days per month?”question can address via hypothesis test. null hypothesis average number bad mental health days per month less equal 4. alternative hypothesis average number bad mental health days per month 4.","code":""},{"path":"lab9.html","id":"by-simulation","chapter":"Lab 9 Inference for a single mean","heading":"9.2.4.1 By simulation","text":"One way conduct hypothesis test use bootstrapping simulate data look like null hypothesis true. construct sampling distribution assumption true average population 4 bad days per month. see mean data falls distribution find \\(p\\) value decide whether reject null hypothesis.code constructs sampling distribution assumption true average population 4 bad days per month:Exercise 9.4  chunk code build sampling distribution confidence interval (used ) wasCompare code just ran build sampling distribution hypothesis test:similar different two chunks code ? difference find, describe think difference .second chunk code used line hypothesize(null = \"point\", mu = 4) tell R hypothesizing true average population 4. change line wanted hypothesize true average population 7?Now built sampling distribution (assuming null hypothesis true), let’s see observed sample mean falls distribution. Note set direction = \"greater\" code , alternative hypothesis average greater 4 days:looking good null hypothesis! None () simulations least extreme observed sample mean.","code":"\nnull_dist_mental_health <- nhanes %>%\n    specify(response = DaysMentHlthBad) %>%\n    hypothesize(null = \"point\", mu = 4) %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"mean\")\n\nnull_dist_mental_health## Response: DaysMentHlthBad (numeric)\n## Null Hypothesis: point\n## # A tibble: 1,000 × 2\n##    replicate  stat\n##        <int> <dbl>\n##  1         1  3.93\n##  2         2  4.16\n##  3         3  4.14\n##  4         4  4.02\n##  5         5  3.91\n##  6         6  3.93\n##  7         7  4.23\n##  8         8  4.13\n##  9         9  4.07\n## 10        10  3.95\n## # … with 990 more rows\nboot_dist_mental_health <- nhanes %>%\n    specify(response = DaysMentHlthBad) %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"mean\")\nnull_dist_mental_health <- nhanes %>%\n    specify(response = DaysMentHlthBad) %>%\n    hypothesize(null = \"point\", mu = 4) %>%\n    generate(reps = 1000, type = \"bootstrap\") %>%\n    calculate(stat = \"mean\")\nnull_dist_mental_health %>%\n    visualize() +\n    shade_p_value(obs_stat = sample_mean, direction = \"greater\")"},{"path":"lab9.html","id":"using-the-t-distribution","chapter":"Lab 9 Inference for a single mean","heading":"9.2.4.2 Using the T distribution","text":"sample large, can also use mathematical model find \\(p\\) value need hypothesis test. , use R calculate T score find proportion T distribution larger T score.Recall , just like Z score, T score defined difference point estimate hypothesized value, relative standard error:\\[\n\\begin{align*}\nT & = \\frac{\\text{point estimate} - \\text{null value}}{\\text{standard error}} \\\\\n& = \\frac{\\bar{x} - \\mu_0}{SE}\n\\end{align*}\n\\]Exercise 9.5  Fill blanks code find T score corresponding \\(p\\) value using T distribution. Hint: Try fill least blanks names values already calculated earlier session.code use?Assuming significance level 0.05, reject null hypothesis? ?Give one sentence summary outcome hypothesis test context scenario.","code":"t_score <- (___ - ___) / ___\n\np_value <- pt(q = t_score, df = ___, lower.tail = FALSE)"},{"path":"lab9.html","id":"how-much-can-people-keep-track-of","chapter":"Lab 9 Inference for a single mean","heading":"9.3 How much can people keep track of?","text":"get experience seeing inference mean numerical variable important psychology, consider case “memory span”. clear, total amount stuff can remember whole life! Rather, much can remember situation multiple different things . sense, related idea “attention span”: many distinct things can keep track one time?One way memory span measured showing people sequence objects asking recall objects sequence. longest sequence can recall without making errors “memory span”. example, two sequences someone might shown, row pictures sequence:Two example sequences random objects, top one 3 objects bottom one 4 objects.Say participant sees first sequence correctly recalls small black square, large black triangle, small purple triangle. see second sequence recall large blue cross, small blue spiral, large red spiral. made error forgetting small red spiral. result, memory span measured 3 objects recall without error.Measurements memory span recently collected Mathy et al. (2018) using random sample college students.","code":""},{"path":"lab9.html","id":"load-the-data-8","chapter":"Lab 9 Inference for a single mean","heading":"9.3.1 Load the data","text":"Download data, look two variables: participant ID number measured memory span participant. Note reported spans always integers allowed partial credit sequences perfectly recalled.","code":"\nspan_data <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/simple_span.csv\")## Rows: 94 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (2): id, span\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"lab9.html","id":"check-out-the-data-2","chapter":"Lab 9 Inference for a single mean","heading":"9.3.2 Check out the data","text":"First, let’s get sense raw data making histogram measured memory spans, recorded variable named span.let’s also get sample mean, sample standard deviation, sample size:Exercise 9.6  Consider histogram summary statistics span variable just made.Describe distribution memory span, sure note number modes, skewness, whether outliers.seem like conditions satisfied use mathematical model sampling distribution mean?","code":"\nspan_data %>%\n    ggplot(aes(x = span)) +\n    geom_histogram(binwidth = 1)\nspan_data %>%\n    summarize(sample_mean = mean(span), sample_sd = sd(span), sample_size = n())## # A tibble: 1 × 3\n##   sample_mean sample_sd sample_size\n##         <dbl>     <dbl>       <int>\n## 1        4.05      1.06          94"},{"path":"lab9.html","id":"build-a-confidence-interval","chapter":"Lab 9 Inference for a single mean","heading":"9.3.3 Build a confidence interval","text":"Exercise 9.7  Use bootstrapping construct sampling distribution mean memory span, based data sample. Use distribution find 95% confidence interval mean memory span. , fill blanks chunk code (Hint: refer code earlier session remember name response variable .):code use?Report confidence interval found (can find R’s environment boot_ci_span) interpret interval context research scenario.","code":"boot_dist_span <- span_data %>%\n    specify(response = ___) %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nboot_ci_span <- boot_dist_span %>%\n    get_confidence_interval(level = ___)\n\nboot_dist_span %>%\n    visualize() +\n    shade_confidence_interval(boot_ci_span)\n\nboot_ci_span"},{"path":"lab9.html","id":"test-a-hypothesis","chapter":"Lab 9 Inference for a single mean","heading":"9.3.4 Test a hypothesis","text":"Based many prior studies memory span, seems people average can keep track 4 unique objects time. therefore reasonable ask research question, “data provide evidence population sampled memory span different 4 objects?”Exercise 9.8  –null alternative hypotheses corresponding research question?Fill blanks code build sampling distribution assumes null hypothesis true. end, get histogram sampling distribution showing observed sample mean falls. code use? (Hint: remember last blank three options, less, greater, two-sided, appropriate depends null/alternative hypotheses.)Based visualization made part (b), expect T score large small magnitude? positive negative?Use code find \\(p\\) value (making sure use direction part b). Based believe reasonable significance level, reject null hypothesis?Use code find \\(p\\) value (making sure use direction part b). Based believe reasonable significance level, reject null hypothesis?one sentence, summarize result hypothesis test context problem.one sentence, summarize result hypothesis test context problem.anticipated outcome hypothesis test based confidence interval found previous exercise? ?anticipated outcome hypothesis test based confidence interval found previous exercise? ?","code":"null_dist_span <- span_data %>%\n    specify(response = ___) %>%\n    hypothesize(null = \"___\", mu = ___) %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nsample_mean_span <- span_data %>%\n    summarize(mean(span)) %>%\n    pull()\n\nnull_dist_span %>%\n    visualize() +\n    shade_p_value(obs_stat = sample_mean_span, direction = \"___\")\nnull_dist_span %>%\n    get_p_value(obs_stat = sample_mean_span, direction = \"___\")"},{"path":"lab9.html","id":"wrap-up-8","chapter":"Lab 9 Inference for a single mean","heading":"9.4 Wrap-up","text":"session, saw can use kinds computational techniques applied proportions—particularly bootstrapping—construct confidence intervals perform hypothesis tests means. also saw use T distribution find confidence intervals conduct hypothesis tests.","code":""},{"path":"lab10.html","id":"lab10","chapter":"Lab 10 Inference for comparing two independent means","heading":"Lab 10 Inference for comparing two independent means","text":"session, see conduct inferences whether two groups differ average. situations, two separate independent samples two (potentially) different populations. want know observed differences sample means reflect difference means two populations reasonably attributed chance alone. Just like inferences differences proportions, can use simulation mathematical methods address types questions.","code":""},{"path":"lab10.html","id":"required-packages-4","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.1 Required packages","text":"’ve practiced quite bit now, first load tidyverse infer packages.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab10.html","id":"are-recruiters-more-impressed-by-a-spoken-vs.-written-hiring-pitch","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2 Are recruiters more impressed by a spoken vs. written hiring pitch?","text":"Imagine trying convince recruiter right person job. come better writing can give pitch person? question addressed series experiments Schroeder & Epley (2015). focus one experiments (Experiment 4 original paper).experiment, MBA students University Chicago business school prepared short “elevator pitches” describing hired firm choice. students recorded giving pitch. Finally, sample professional recruiters selected randomly assigned one two groups: One group hear audio recording student’s pitch; another group read transcript audio recording. recruiters evaluated potential applicants number scales, focus one response variable: scale 1–10, recruiters rated likelihood hire student based (spoken written) pitch.research question address data , “difference average hiring ratings recruiter hears reads pitch?”Exercise 10.1  null alternative hypotheses corresponding research question?","code":""},{"path":"lab10.html","id":"check-out-the-data-3","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.1 Check out the data","text":"First, let’s load data RStudio environment:row represents responses particular recruiter. Whether recruiter got spoken written pitch recorded Condition variable, either “audio” (hearing spoken pitch) “transcript” (reading transcript pitch). recruiter’s hiring rating recorded hire variable, number 1 10.Exercise 10.2  –names response variable explanatory variable hiring_study1 dataset?Fill blanks code corresponding variable names (part [] exercise) visualize distribution ratings two groups. Describe shape distributions whether seems like may relationship type pitch hiring ratings.Fill blanks code corresponding variable names (part [] exercise) simulate version data look null hypothesis true (using random permutation). Describe , , distributions differ looked original data part (b).Exercise 10.3  Fill blanks code using appropriate variable names obtain numerical summaries group’s hiring ratings.Use resulting table answer following questions.two groups similar variability? (sure refer “rule thumb” checking whether two samples similar variability.)sample means suggest advantage either spoken (“audio”) written (“transcript”) hiring pitches?","code":"\nhiring_study <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/hiring_study.csv\")## Rows: 39 Columns: 27\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr   (2): test, Condition\n## dbl  (23): compt, thought, intell, like, pos, neg, hire, age, gender, cond, ...\n## dttm  (2): start_time, end_time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.hiring_study %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(\"___\", ncol = 1)null_simulation <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1, type = \"permute\")\n\nnull_simulation %>%\n    ggplot(aes(x = ___)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(\"___\", ncol=1)hiring_study %>%\n    group_by(___) %>%\n    summarize(sample_mean = mean(___), sample_sd = sd(___), sample_size = n())"},{"path":"lab10.html","id":"hypothesis-testing-via-simulation","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.2 Hypothesis testing via simulation","text":"’ve seen, can use random permutation model data look null hypothesis true. simulated datasets, compute difference mean hiring ratings audio transcript conditions. many times, build sampling distribution difference means. simulated datasets produce difference least extreme actually observed, can reject null hypothesis.Exercise 10.4  –Fill blanks code (using appropriate variable names) find observed difference mean hiring ratings conditions. observed difference means?Fill blanks code simulate visualize 1000 differences means according null hypothesis. final blank, direction can either less, greater, two-sided; pick one corresponding alternative hypothesis. Based visualization, observed difference unusual null hypothesis true?Fill blank (using direction gave part b) get proportion simulated datasets least extreme observed. assume significance level 0.05, reject null hypothesis? ?","code":"obs_diff <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"diff in means\", order = c(\"audio\", \"transcript\"))\n\nobs_diffnull_dist_simulation <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"independence\") %>%\n    generate(reps = 1000, type = \"permute\") %>%\n    calculate(stat = \"diff in means\", order = c(\"audio\", \"transcript\"))\n\nnull_dist_simulation %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = \"___\")\nnull_dist_simulation %>%\n    get_p_value(obs_stat = obs_diff, direction = \"___\")"},{"path":"lab10.html","id":"hypothesis-testing-using-the-t-distribution","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.2.3 Hypothesis testing using the T distribution","text":"section, relate simulation approach just used mathematical model approach using T distribution.Exercise 10.5  Fill blanks code (using appropriate variable names) calculate T score associated sample:T score got?T score indicate observed difference relatively near far null hypothesis predict? T score tells near/far null?, used code line calculate(stat = \"diff means\", order = c(\"audio\", \"transcript\")) get observed difference sample means. line differ calculate line code used Exercise find T score? Describe words stat option seems .Exercise 10.6  Fill blanks code visualize sample falls corresponding T distribution. , sure fill last blank according alternative hypothesis (either two-sided, less, greater).Compare visualization just made using mathematical model visualization made earlier using simulation. Relative null distribution, observed data (red vertical line) similar place? similar proportion null distribution extreme observed?Compare visualization just made using mathematical model visualization made earlier using simulation. Relative null distribution, observed data (red vertical line) similar place? similar proportion null distribution extreme observed?Fill blanks code get \\(p\\) value according mathematical model (, direction correspond alternative hypothesis). \\(p\\) value got? assuming significance level 0.05, reject null hypothesis? come different conclusion using simulation?Fill blanks code get \\(p\\) value according mathematical model (, direction correspond alternative hypothesis). \\(p\\) value got? assuming significance level 0.05, reject null hypothesis? come different conclusion using simulation?Based hypothesis test (either simulation mathematical approach ), give one sentence summary conclude effectiveness spoken vs. written pitches hiring ratings.Speculate results turned way . expect result generalize hiring decisions real life? ?","code":"obs_t <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"t\", order = c(\"audio\", \"transcript\"))\n\nobs_tnull_dist_math <- hiring_study %>%\n    specify(___ ~ ___) %>%\n    assume(\"t\")\n\nnull_dist_math %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_t, direction = \"___\")\nnull_dist_math %>%\n    get_p_value(obs_stat = obs_t, direction = \"___\")"},{"path":"lab10.html","id":"wrap-up-9","chapter":"Lab 10 Inference for comparing two independent means","heading":"10.3 Wrap-up","text":"session, got practice hypothesis tests compare means two independent groups. carried test using simulation mathematical models. Along way, saw whether evidence spoken written job pitches likely lead hiring ratings, illustrating methods used applied research settings.","code":""},{"path":"lab11.html","id":"lab11","chapter":"Lab 11 Inference for paired data","heading":"Lab 11 Inference for paired data","text":"session, use R conduct inference paired data. Like previous session, interested comparing two sets numbers. observation one set paired observation set, techniques inference amount ones learned dealing just single set numbers. treat difference within pair object study.","code":""},{"path":"lab11.html","id":"required-packages-5","chapter":"Lab 11 Inference for paired data","heading":"11.1 Required packages","text":"session, going need load standard tidyverse package well infer package R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab11.html","id":"do-infants-prefer-people-who-sing-familiar-songs","chapter":"Lab 11 Inference for paired data","heading":"11.2 Do infants prefer people who sing familiar songs?","text":"Sam Mehr, Lee Ann Song (aptly named), Liz Spelke (Mehr et al., 2016) interested infants use music social cue. infant sees someone first time, likely attracted person sing song infant already knows?Mehr et al. (2016) conducted experiment child’s parents taught new melody instructed sing child home course 1–2 weeks. exposure period, parents brought infant back lab. infant seated front screen showing videos two adults infant never seen . first, two adults just smiled silence. recorded proportion time infant looked stranger “” phase. , one people sang melody parents singing 1–2 weeks, sang totally new song. Finally, “” phase, infant saw videos person silently smiling researchers recorded proportion time spent looking person sang familiar song. infants tend look stranger sang familiar song?","code":""},{"path":"lab11.html","id":"check-out-the-data-4","chapter":"Lab 11 Inference for paired data","heading":"11.2.1 Check out the data","text":"First, let’s get data R:look data clicking lullaby RStudio’s “Environment” pane. row data specific infant. three variables dataset:“id”: number identifying infant study.“”: phase infants heard anyone sing, proportion time look person eventually sing familiar melody?“”: phase infants heard two people sing, proportion time look person sang familiar melody?Exercise 11.1  –infant showed preference either phase, proportion time phase spend looking person sang familiar melody?infant change looked hearing person sing, difference looking proportions phase?","code":"\nlullaby <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/lullaby_wide.csv\")## Rows: 32 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): id, Before, After\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nlullaby## # A tibble: 32 × 3\n##       id Before After\n##    <dbl>  <dbl> <dbl>\n##  1   101  0.437 0.603\n##  2   102  0.413 0.683\n##  3   103  0.754 0.724\n##  4   104  0.439 0.282\n##  5   105  0.475 0.499\n##  6   106  0.871 0.951\n##  7   107  0.237 0.418\n##  8   108  0.759 0.938\n##  9   109  0.416 0.5  \n## 10   110  0.800 0.586\n## # … with 22 more rows"},{"path":"lab11.html","id":"examine-the-distribution-of-differences","chapter":"Lab 11 Inference for paired data","heading":"11.2.2 Examine the distribution of differences","text":"research question whether hearing two people sing affected infant’s viewing preferences. result, interested difference . find infant, use R’s mutate function:Exercise 11.2  Look result ran chunk code just prior exercise.words, describe line code mutate(diff = - ) .Refer description study given . positive value diff say infant’s looking preferences changed? negative value diff say preferences changed?nice thing mutate line can add code uses diff variable created line. example, can make histogram differences looking time :Exercise 11.3  Refer histogram just made differences looking times hearing people sing.Describe shape distribution (sure note number modes, skewness, whether may outliers).Based histogram, data appear satisfy conditions required using mathematical model (specifically, T distribution)? ?Based histogram, seem like average preference may changed hearing people sing? , direction?","code":"\nlullaby %>%\n    mutate(diff = After - Before)## # A tibble: 32 × 4\n##       id Before After    diff\n##    <dbl>  <dbl> <dbl>   <dbl>\n##  1   101  0.437 0.603  0.166 \n##  2   102  0.413 0.683  0.270 \n##  3   103  0.754 0.724 -0.0304\n##  4   104  0.439 0.282 -0.157 \n##  5   105  0.475 0.499  0.0239\n##  6   106  0.871 0.951  0.0800\n##  7   107  0.237 0.418  0.181 \n##  8   108  0.759 0.938  0.179 \n##  9   109  0.416 0.5    0.0837\n## 10   110  0.800 0.586 -0.213 \n## # … with 22 more rows\nlullaby %>%\n    mutate(diff = After - Before) %>%\n    ggplot(aes(x = diff)) +\n    geom_histogram(binwidth = 0.1)"},{"path":"lab11.html","id":"confidence-interval-by-bootstrapping","chapter":"Lab 11 Inference for paired data","heading":"11.2.3 Confidence interval by bootstrapping","text":"begin, let’s use bootstrapping construct confidence interval difference looking proportion . confidence interval describe much hearing someone sing familiar melody changes infants’ looking preferences.Exercise 11.4  Fill blanks code construct 95% confidence interval mean difference looking proportion . sure refer code earlier session, well previous labs, guidance. (Hint: ’re looking name response variable, remember inference difference.)code use?Based confidence interval found, evidence significance difference looking preferences hearing people sing? Explain reasoning.","code":"boot_dist <- lullaby %>%\n    ___(diff = After - Before) %>%\n    specify(response = ___) %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"___\")\n\nboot_ci <- boot_dist %>%\n    get_confidence_interval(level = ___)\n\nboot_dist %>%\n    visualize() +\n    shade_confidence_interval(boot_ci)"},{"path":"lab11.html","id":"hypothesis-test-via-mathematical-model","chapter":"Lab 11 Inference for paired data","heading":"11.2.4 Hypothesis test via mathematical model","text":"Next, use mathematical model conduct hypothesis test. hypothesis test address research question, “difference infants’ average looking behavior hearing two strangers sing?”Exercise 11.5  –null alternative hypotheses corresponding research question?Fill blanks code calculate T score observed data. code use? (Hint: mu, remember null hypothesis.)Fill blanks code visualize observed T score falls null distribution. Based visualization made, observed data likely unlikely null hypothesis true? (Hint: last blank, check filled previous labs depending alternative hypothesis .)Fill blanks code get \\(p\\) value. get \\(p\\) value?adopt significance level 0.05, reject null hypothesis ? ?Give one-sentence summary outcome hypothesis test tells us infant looking behavior affected hearing someone sing familiar melody.","code":"obs_t_diff <- lullaby %>%\n    ___(diff = After - Before) %>%\n    specify(response = ___) %>%\n    hypothesize(null = \"point\", mu = ___) %>%\n    calculate(stat = \"t\")null_dist <- lullaby %>%\n    ___(diff = After - Before) %>%\n    specify(response = ___) %>%\n    hypothesize(null = \"point\", mu = ___) %>%\n    assume(\"t\")\n\nnull_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_t_diff, direction = \"___\")\nnull_dist %>%\n    get_p_value(obs_stat = obs_t_diff, direction = \"___\")"},{"path":"lab11.html","id":"wrap-up-10","chapter":"Lab 11 Inference for paired data","heading":"11.3 Wrap-up","text":"session, used R inference paired data, typically interested whether difference within pairs observations average. used R’s mutate function compute differences within participant. used bootstrapping construct confidence interval mathematical model conduct hypothesis test. end, able address question whether infants respond favorably (looking ) stranger sings familiar song, giving us insight potential social importance music.","code":""},{"path":"lab12.html","id":"lab12","chapter":"Lab 12 Inference for multiple independent groups","heading":"Lab 12 Inference for multiple independent groups","text":"session, use R inference comparing averages multiple independent groups. Specifically, interested hypothesis tests address questions whether multiple groups differ average, called Analysis Variance (ANOVA). see simulation techniques learned comparing proportions means two groups also apply multiple groups. mathematical model ANOVA different, however; “F distribution”. use techniques analyse experiment designed compare different ways minimize intrusions traumatic memories.","code":""},{"path":"lab12.html","id":"required-packages-6","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.1 Required packages","text":"session, going need load standard tidyverse package well infer package R’s library.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(infer)"},{"path":"lab12.html","id":"can-we-reduce-intrusions-of-traumatic-memories","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2 Can we reduce intrusions of traumatic memories?","text":"One hallmarks post-traumatic stress disorder memories traumatic event “intrude” everyday life, popping wanted. remember previous event, memory said “active”. According reconsolidation theory, active memories can also changed. example, tell story memory, telling story might change memory tell differently next time. Reconsolidation theory suggests one way reduce intrusions traumatic memories “activate” change less likely intrude everyday life.One way modify intrusive memories studied James et al. (2015). thought playing video game, namely Tetris, memory active help make memory less traumatic. participants first view traumatic film (including scenes real car accidents). next week, participants recorded often memory traumatic film intruded everyday life. week, participants came back lab randomly assigned one four treatment conditions:Reactivation-plus-Tetris: group, participants shown still images traumatic film “reactivate” memories film. played Tetris 12 minutes.Reactivation : group, participants shown still images traumatic film “reactivate” memories film, sat silently 12 minutes play Tetris.Tetris : group, participants shown images film simply played Tetris 12 minutes.task (control): group, participants simply sat quietly 12 minutes.According reconsolidation theory, people reactivation-plus-Tetris condition experience fewer intrusive memories playing Tetris change memories less disruptive. predicted happen group either original traumatic memories reactivated (control Tetris-) changed reactivation (control reactivation-).","code":""},{"path":"lab12.html","id":"load-the-data-9","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.1 Load the data","text":"Load relevant data R now:row shows data single participant. 3 variables dataset:condition: four treatment conditions participant .intrusions_pre: number times participant reported intrusive memory traumatic film prior treatment.intrusions_post: number times participant reported intrusive memory traumatic film treatment.Exercise 12.1  Consider design study address following questions.possible conclude treatment condition plays role causing changes might observe number intrusive memories? ?Fill blanks code use mutate create new variable called effect represents difference number intrusions treatment number intrusions treatment. Write code negative value effect means reduction number intrusive memories treatment. code use?","code":"\ntetris_data <- read_csv(\"https://raw.githubusercontent.com/gregcox7/StatLabs/main/data/tetris.csv\")## Rows: 72 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): condition\n## dbl (2): intrusions_pre, intrusions_post\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntetris_data## # A tibble: 72 × 3\n##    condition         intrusions_pre intrusions_post\n##    <chr>                      <dbl>           <dbl>\n##  1 No task (control)              2               4\n##  2 No task (control)              2               3\n##  3 No task (control)              5               6\n##  4 No task (control)              0               2\n##  5 No task (control)              5               3\n##  6 No task (control)              4               4\n##  7 No task (control)              0               0\n##  8 No task (control)              4               4\n##  9 No task (control)              3               2\n## 10 No task (control)              5              11\n## # … with 62 more rowstetris_data %>%\n    mutate(effect = ___ - ___)"},{"path":"lab12.html","id":"visualize-the-data","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.2 Visualize the data","text":"Exercise 12.2  Fill blanks code make boxplot compares distribution effect group (defined treatment condition):research question , “difference average effectiveness treatment conditions?” null alternative hypotheses corresponding research question?names explanatory variable response variable?Based boxplot just made exercise, data seem consistent null alternative hypothesis? Explain reasoning.","code":"tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    ggplot(aes(x = ___, y = effect)) +\n    geom_boxplot()"},{"path":"lab12.html","id":"hypothesis-testing-by-randomization","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.3 Hypothesis testing by randomization","text":"use data conduct hypothesis test help us address research question, “difference average effectiveness treatment conditions?” ’ve seen, test statistic need F statistic, represents much variability groups, relative amount variability within groups.","code":""},{"path":"lab12.html","id":"the-observed-f-statistic","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.3.1 The observed F statistic","text":"Exercise 12.3  Fill blanks code find F statistic observed data. Hint: specify line, remember put name response variable left ~ name explanatory variable right.F statistic found?F statistic found indicate -group variability greater smaller within-group variability? Explain reasoning.","code":"obs_f <- tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    specify(___ ~ ___) %>%\n    calculate(stat = \"F\")\n\nobs_f"},{"path":"lab12.html","id":"simulating-the-null-hypothesis","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.3.2 Simulating the null hypothesis","text":"Just like comparing proportions comparing means independent samples, can simulate data look null hypothesis true. Specifically, simulate single dataset, randomly re-assign (permute) values explanatory variable across observations. calculate F statistic simulated dataset. repeat process many times build sampling distribution F statistic. Finally, see whether F statistic actual data large enough reject idea came sampling error alone.Exercise 12.4  Fill blanks code use random permutation conduct hypothesis test. final result histogram simulated F statistics along line indicating observed F statistic (obs_f last exercise) falls distribution. Hint: blanks hypothesize generate lines, consider simulated null hypothesis comparing proportions means independent samples.Based histogram just produced, consider observed F statistic unusually large null hypothesis true?Run chunk code overlay mathematical model null hypothesis—“F distribution”—histogram simulated F statistics. Describe shape F distribution (skewness, number modes) well whether F distribution (smooth curve) make good “fit” histogram simulated F statistics.","code":"null_dist <- tetris_data %>%\n    mutate(effect = ___ - ___) %>%\n    specify(___ ~ ___) %>%\n    hypothesize(null = \"___\") %>%\n    generate(reps = 1000, type = \"___\") %>%\n    calculate(stat = \"F\")\n\nnull_dist %>%\n    visualize() +\n    shade_p_value(obs_stat = obs_f, direction = \"greater\")\nnull_dist %>%\n    visualize(method = \"both\") +\n    shade_p_value(obs_stat = obs_f, direction = \"greater\")"},{"path":"lab12.html","id":"hypothesis-testing-by-mathematical-model","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.2.4 Hypothesis testing by mathematical model","text":"practice, ANOVA often done using mathematical model null hypothesis instead using simulation. relevant calculations carried computer, results often presented table makes easier see sausage made (format used class book).Exercise 12.5  Fill blanks code use R produce ANOVA table. Hint: first two lines just convenience; tell R store version data already effect variable. sure use mutate line ’ve using previous exercises. lm line, recall squiggly ~ used specify response explanatory variables.Find mathematically computed \\(p\\) value table just produced (column headings helpful, may also compare format ANOVA tables class book). \\(p\\) value?Using significance level 0.05, reject null hypothesis ? ?Summarize results hypothesis test tell us whether treatment conditions equally effective reducing intrusive memories.reconsolidation theory predicts reactivation-plus-Tetris significantly effective three treatment conditions. Describe follow-analyses use test prediction well whether follow-analyses require special adjustments.","code":"tetris_data_effect <- tetris_data %>%\n    mutate(effect = ___ - ___)\n\nlm(___ ~ ___, data = tetris_data_effect) %>%\n    anova()"},{"path":"lab12.html","id":"wrap-up-11","chapter":"Lab 12 Inference for multiple independent groups","heading":"12.3 Wrap-up","text":"final technique ’ve learned, Analysis Variance, used address questions whether multiple independent groups differ one another average. F statistic used summarize much variability groups versus amount variability within groups. used random permutation simulate kinds data occur null hypothesis true. used simulation well mathematical model determine whether observed F statistic large enough reject null hypothesis.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
